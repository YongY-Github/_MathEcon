\section{Basic Differentiation}

Before we study the differentiation of single-variable functions, we briefly review several foundational mathematical concepts.

\subsection{Functions}

A function $f$ from a set $X$ to a set $Y$, written $f: X \to Y$, is a rule that assigns \textbf{exactly one} element of $Y$ to each element of $X$.

\begin{itemize}
\item $X$ is called the \textbf{domain}
\item The \textbf{range} is the set of values in $Y$ that are actually attained
\end{itemize}

Using $x \in X$ and $y \in Y$, a function is written as
$y = f(x)$,
where $x$ is the independent variable and $y$ the dependent variable.

\subsection{Graphs}

If $X$ and $Y$ are sets of real numbers, the \textbf{graph} of a function $f$ is the set of points
$(x, y)$ such that $y = f(x)$.

\begin{quote}
\textbf{Economic convention.}\newline
Economists often draw demand curves with quantity on the horizontal axis and price on the vertical axis, even when the function is written as $q = f(p)$.
\end{quote}

\subsection{Slope}

The slope of a line through points $(x, y)$ and $(x', y')$ is

\begin{equation*}
m = \frac{y' - y}{x' - x}.
\end{equation*}

Differentiation is the method of finding the slope of a function and is denoted by $f'(x)$.

\subsection{Limits}

We say that a function $f$ has \textbf{limit} $L$ as $x \to a$ if, for any $\varepsilon > 0$, there exists a $\delta > 0$ such that\footnote{A function need not be defined at the point $a$ in order to have a limit as $x \to a$.
For example,

\begin{equation*}
f(x) = \frac{x^2 - 1}{x - 1}
\end{equation*}

is not defined at $x = 1$, but

\begin{equation*}
\lim_{x \to 1} f(x) = 2.
\end{equation*}}

\begin{equation*}
|f(x) - L| < \varepsilon
\quad \text{whenever} \quad
0 < |x - a| < \delta.
\end{equation*}

When this condition holds, we write

\begin{equation*}
\lim_{x \to a} f(x) = L.
\end{equation*}

\subsection{Continuity}

A function $f$ is \textbf{continuous at $a$} if:\footnote{The two functions discussed above are not continuous.
The first is not continuous because $f(a)$ is not defined.
The second is not continuous because $f$ does not converge to a limit as $x \to a$.
For example, if

\begin{equation*}
f(x) =
\begin{cases}
-1, & x < 0, \\
1, & x > 0,
\end{cases}
\end{equation*}

then $f$ has no limit as $x \to 0$, since the right-hand limit equals 1 while the left-hand limit equals -1.}

\begin{enumerate}
\item $f(a)$ is defined
\item $\lim_{x \to a} f(x)$ exists
\item $\lim_{x \to a} f(x) = f(a)$
\end{enumerate}

\subsection{Derivative at a Point}

Let $y = f(x)$. When $x$ changes by $\Delta x$, the change in $y$ is

\begin{equation*}
\frac{\Delta y}{\Delta x}
=
\frac{f(x+\Delta x) - f(x)}{\Delta x}.
\end{equation*}

\subsection{Derivative as a Function}

The \textbf{derivative of $f$ at $x$} is defined as

\begin{equation*}
f'(x)
=
\lim_{\Delta x \to 0}
\frac{f(x+\Delta x) - f(x)}{\Delta x}.
\end{equation*}

If the derivative exists for every $x$ in the domain of $f$, then the derivative itself defines a new function, denoted $f'(x)$.

Geometrically, $f'(x)$ is the slope of the tangent line to the graph of $f$ at $(x, f(x))$.

Common notations include:

\begin{itemize}
\item $f'(x)$
\item $\dfrac{dy}{dx}$
\item $Df(x)$
\end{itemize}

Note that what we usually think of as a variable $x$ is held constant while $\Delta x$ varies and converges to zero. It is useful to keep in mind that the derivative of a function $f$ at $x$ is the slope of a line tangent to the graph of the function $f$ at the point $(x, f(x))$. It is crucial to understand the implications of the existence of the derivative at a point $x$. The function must be smooth---meaning it is both continuous and differentiable---at the point $x$. The tangent line provides a high-quality linear approximation to the graph of the function near $x$. In general, if we know that the function $f$ is differentiable at $a$, then the tangent line approximation to $f$ at $a$ is:

\begin{equation*}
y = f(a) + f'(a)(x - a)
\end{equation*}

where $a, f(a), \text{ and } f'(a)$ are constants, $x$ is the independent variable, and $y$ is the dependent variable. We will see this point again with Taylor series expansions. Many important concepts in economics---such as marginal cost or marginal utility---are based on this derivative function.

\subsection{Second Derivative}

The \textbf{second derivative} is the derivative of the derivative and is written as

\begin{equation*}
f''(x)
=
\frac{d^2 f(x)}{dx^2}.
\end{equation*}

\begin{quote}
\textbf{Economic interpretation.}\newline
If $\ln p(t)$ describes log prices over time, then:

\begin{itemize}
\item the first derivative is inflation
\item the second derivative is the change in inflation
\end{itemize}
\end{quote}

\subsection{Basic Rules of Differentiation}

Let $y = f(x)$.

\subsubsection{Constant-function Rule}

The derivative of a contsant function $y=f(x)=k$ is zero, for all values of x-it has zero slope!

\begin{equation*}
\frac{d}{dx}(k) = 0.
\end{equation*}

\subsubsection{Power-function Rule}

The derivative of a power function $f(x) = x^n$ is:

\begin{equation*}
\frac{d}{dx}(x^n) = n x^{n-1}.
\end{equation*}

\subsubsection{Generalized Power-function Rule}

When a multiplicaytive constant $k$ appears in the power fuction, so that $f(x) = kx^n$, then:

\begin{equation*}
\frac{d}{dx}(k x^n) = k n x^{n-1}.
\end{equation*}

\subsubsection{Logarithmic Rule}

The derivatice of the log-function $f(x) = lnx$ is:

\begin{equation*}
\frac{d}{dx}(\ln x) = \frac{1}{x}.
\end{equation*}

\subsubsection{Exponential Rule}

For some exponential function $f(x) = a^x$, where $a$ is some constant, then:

\begin{equation*}
\frac{d}{dx}(a^x) = a^x \ln a.
\end{equation*}

Note that a particular case of the above is

\begin{equation*}
\frac{d}{d x} e^x = e^x
\end{equation*}

While

\begin{equation*}
\frac{d}{d x} \ln x = \frac{1}{x}
\end{equation*}

Now, let's consider some further useful rules of differentiation involving two or more functions of the same variable. Specifically, suppose $f(x)$ and $g(x)$ are two different functions of $x$ and that $f'(x)$ and $g'(x)$ exist. That is, let $f(x)$ and $g(x)$ be differentiable, then:

\subsubsection{Sum-difference Rules}

The derivative of a sum (difference) of two functions is the sum (difference) of the derivatives of the two functions.

\begin{equation*}
\frac{d}{dx}[f(x) \pm g(x)] = f'(x) \pm g'(x).
\end{equation*}

\subsubsection{Product Rule}

The derivative of the product of two (differentiable) functions is equal to the first function times the derivative of the second function plus the second function times the derivative of the first function.

\begin{equation*}
\frac{d}{dx}[f(x)g(x)]
=
f'(x)g(x) + f(x)g'(x).
\end{equation*}

\subsubsection{Quotient Rule}

The derivative of the quotient of two (differentiable) functions, $f(x)/g(x)$, is

\begin{equation*}
\frac{d}{dx} \left[ \frac{f(x)}{g(x)} \right] = \frac{g(x)f'(x) - f(x)g'(x)}{[g(x)]^2}
\end{equation*}

provided that $g(x) \neq 0$. Note that $[g(x)]^2 = g^2(x)$.

\subsubsection{Chain Rule}

If $z = f(y)$ and $y = g(x)$, then

\begin{equation*}
\frac{dz}{dx}
=
\frac{dz}{dy}\frac{dy}{dx}.
\end{equation*}

The chain rule provides a convenient way to study how one variable (say, $x$) affects another variable ($z$) through its influence on some intermediate variable ($y$).

Sometimes, we can write for a composite function $y = f(g(x))$:

\begin{equation*}
\frac{dy}{dx} = f'(g(x)) \cdot g'(x)
\end{equation*}

\subsubsection{Chain Rule for Exponential and Logarithmic Functions}

\paragraph{The general exponential function rule}

\begin{equation*}
\frac{d}{d x} e^{g(x)} = e^{g(x)} g'(x)
\end{equation*}

For example:

\begin{equation*}
\frac{d}{d x} e^{ax} = \frac{d}{d (ax)} e^{ax} \frac{d}{d x} (ax) = e^{ax} a = ae^{ax}
\end{equation*}

If we are using a base other than $e$:

\begin{equation*}
\frac {d}{d x}(a^{g(x)}) = a^{g(x)} g'(x) \ln a, \text{ where } a > 0, a \neq 0
\end{equation*}

\paragraph{The general natural logarithmic function rule}

\begin{equation*}
\frac{d}{d x} \ln(g(x)) = \frac{g'(x)}{g(x)}
\end{equation*}

Interestingly:

\begin{equation*}
\frac{d}{d x} \ln(ax) = \frac{d}{d(ax)} \ln(ax) \frac{d}{d x}(ax) = \frac{1}{ax} a = 1/x
\end{equation*}

while

\begin{equation*}
\frac{d}{d x} \ln(x^2) = \frac{d}{d(x^2)} \ln(x^2) \frac{d}{d x}(x^2) = \frac{1}{x^2} 2x = 2/x
\end{equation*}

Note also when considered base other than $e$. Because

\begin{equation*}
\log_b(x) = \frac{\ln(x)}{\ln(b)}
\end{equation*}

we have

\begin{equation*}
\frac{d}{d x} \log_b(x) = \frac{1}{x} \frac{1}{\ln(b)}
\end{equation*}

Or more generally:

\begin{equation*}
\begin{aligned} 
\frac{d}{d x} \log_b g(x) &= \frac{g'(x)}{g(x)} \frac{1}{\ln b}, \text{ where } b > 0, b \neq 1 \\ 
&= \frac{g'(x)}{g(x)} \log_b e 
\end{aligned}
\end{equation*}

Note that $\log_b e = \displaystyle \frac{1}{\ln b}$.


\bigskip
\centerline{\rule{13cm}{0.4pt}}
\bigskip

\subsection{The Differential}

Define $dx$ as an arbitrary change in $x$ from its initial value $x_0$ and $dy$ as the resulting change in $y$ \textbf{along the tangent line} from the initial value of the function $y_0 = f(x_0)$.

The \textbf{differential} of $y=f(x_0)$ evaluated at $x_0$ is

\begin{equation*}
dy = f'(x_0)\, dx.
\end{equation*}

This represents the change in $y$ along the tangent line at $x_0$. Graphically, this is shown in Figure~\ref{fig-differential}.

\begin{figure}[!htbp]
\centering
\includegraphics[width=0.7\linewidth]{files/differential-82c869fec1b7d0de23208baaa6e7e41a.png}
\caption[]{Differential}
\label{fig-differential}
\end{figure}

\subsection{Taylor Series}

A smooth complex function $z(x)$ can be approximated around $x=a$ by

\begin{equation*}
f(x)
=
z(a)
+ z'(a)(x-a)
+ \frac{1}{2}z''(a)(x-a)^2
+ \frac{1}{6}f'''(a)(x-a)^3
+ \cdots
\end{equation*}

This idea underlies many approximation methods in economics.

\begin{figure}[!htbp]
\centering
\includegraphics[width=0.7\linewidth]{files/TaylorSeries-72060cf4e6dc5bf03c49514fc32cc69e.png}
\caption[]{Taylor expansion of a smooth function around a point.}
\label{fig-taylor-series}
\end{figure}

As shown in Figure~\ref{fig-taylor-series} a function $z(x)$ being approximated by three different Taylor polynomials (or Taylor series expansions) centered around the point $x=a$.

The simplest approximation perhaps would simply be $g(x) = a$. This constant-valued function does not work well, especially if we move away from the point $a$.

A better approximation would be a linear function of the form $h(x) = z(a) + b(x -a)$, where $b$ is some slope. But what would be a good value of $b$? We saw above that the differential is an equation for the tangent line (or slope) at the point $x = a$. So, we could argue that the best \textbf{linear approximation} to the function around this point would be

\begin{equation*}
h(x) = z(a) + z'(a)(x-a)
\end{equation*}

where $z'(a)$ is the derivative of the function evaluated at $x=a$.

But why stop here? We could improve on this. A better approximation could allow for some curvature. The general form would then be, say,  $f(x) = z(a) + z'(a) .(x -a) + c.(x -a)^2$. Again, we ask, ``What would be the best value for $c$?'' The rate of change of the slope of the quadratic approximation should be equal to the rate of change of change of the function at the $a$. And since the second derivative of $z(x)$ is $2c$, then for $f''(x)$ to equal $z''(x)$ at $x = a$, we need $c = 1/2 z''(a)$. Hence the \textbf{quadratic approximation} to the function aound $x = a$ is:

\begin{equation*}
f(x) = z(a) + z'(a)(x-a) + \frac{1}{2}f''(a)(x-a)^2
\end{equation*}

Exteding the above argument for cubic and higher-degree approximations, we could find the $n$th-degree approximation to the function $z(x)$, which we could call $m(x)$, around the point $x = a$ is

\begin{equation*}
m(x)
=
\frac{z(a)}{0!}
+ \frac{z'(a)}{1!}(x-a)
+ \frac{z''(a)}{2!}(x-a)^2
+ \cdots
+ \frac{f^{(n)}(a)}{n!}(x-a)^n
\end{equation*}

where $f^{(n)}(a)$ is the $n$ the derivative of $z(x)$ evaluated at $x = a$. The function $m(x)$ above is called the \textbf{$n$-th degree Taylor expansion series} of $z(x)$ evaluated at $x=a$.

To sum, $z(x)$ is the original function being approximated (the solid curve). $g(x)$ represents a constant function. $h(x)$ represents the first-order Taylor polynomial, i.e. a straight line that has the same value and slope as $z(x)$ at $x=a$ (or a tangent to $z(x)$ at $x=a$). The formula for $f(x)$ is $f(x) = z(a) + z^{\prime }(a)(x -a) + \frac{1}{2} z''(a)(x -a)^2$ representing a second-order (quadratic) polynomial-the dashed curve, which matches better the function's value, slope, and concavity (curvature) at $x=a$. It is a better approximation of $z(x)$ near $x=a$ than the linear approximation $h(x)$ and of course the constant function $g(a)$.~The graph demonstrates that as more terms are included in the Taylor polynomial, the approximation of the original function becomes more accurate over a larger range around the center point $x=a$.

\begin{framed}
\textbf{Example}\\
For example, consider the function

\begin{equation*}
y = e^{x/2} - e^{-x/2},
\end{equation*}

expanded around the point $x = 2$.


\bigskip
\centerline{\rule{13cm}{0.4pt}}
\bigskip

\textbf{Linear approximation}

The linear approximation to this function is

\begin{equation*}
h(x)
= \bigl(e^{1} - e^{-1}\bigr)
+ \left(\tfrac12\bigl(e^{1} + e^{-1}\bigr)\right)(x - 2).
\end{equation*}


\bigskip
\centerline{\rule{13cm}{0.4pt}}
\bigskip

\textbf{Quadratic approximation}

The quadratic approximation is

\begin{equation*}
j(x)
= \bigl(e^{1} - e^{-1}\bigr)
+ \left(\tfrac12\bigl(e^{1} + e^{-1}\bigr)\right)(x - 2)
+ \left(\tfrac18\bigl(e^{1} - e^{-1}\bigr)\right)(x - 2)^2.
\end{equation*}
\end{framed}

\subsection{Implicit Differentiation}

Let's consider a very simple function,

\begin{equation*}
xy = 7.
\end{equation*}

Here, possible solutions include $(x,y)=(1,7)$, $(7,1)$, and so on.\newline
If we want to find the slope of this function, we can differentiate it.

\textbf{Finding $y'$}

To find $y'$, we proceed as follows.

\textbf{(a)} We make the main assumption that $y$ is a function of $x$, i.e. $y=f(x)$.\newline
We then differentiate both sides of the equation with respect to $x$.

Hence, we obtain

\begin{equation*}
\frac{d}{dx}[x f(x)] = 0.
\end{equation*}

Using the product rule, this gives

\begin{equation*}
1\cdot f(x) + x f'(x) = 0.
\end{equation*}

Equivalently,

\begin{equation*}
y + x y' = 0.
\end{equation*}

\textbf{(b)} Solving the resulting equation for $y'$ gives

\begin{equation*}
y' = -\frac{y}{x}.
\end{equation*}

So, if we substitute, for example, $x=1$ and $y=5$, we obtain the slope of the function at that point:

\begin{equation*}
y' = -5.
\end{equation*}

\subsection{Inverse Function Rule for Implicit Functions}

We can show that

\begin{equation*}
\frac{dy}{dx} = -\frac{f_x}{f_y}.
\end{equation*}

That is, if we have an implicit function written as

\begin{equation*}
f(x,y) = 0,
\end{equation*}

then the derivative of $y$ with respect to $x$ can be obtained by:

\begin{enumerate}
\item differentiating $f$ with respect to $x$ to obtain $f_x$,
\item differentiating $f$ with respect to $y$ to obtain $f_y$,
\item taking the ratio $-\dfrac{f_x}{f_y}$.
\end{enumerate}

This gives the derivative of the implicit function $y$ with respect to $x$.

It often feels like magic --- but it is simply a consequence of the chain rule.

\begin{framed}
\textbf{Why this works}\\
Since $f(x,y)=0$ holds along the curve, differentiating both sides with respect to $x$
and solving for $y'$ naturally leads to the ratio $-\dfrac{f_x}{f_y}$.
\end{framed}

\subsection{Some Uses of Differentiation in Economics}

Some common applications of differentiation include:

\begin{itemize}
\item Increasing and decreasing functions
\item Relative extrema (maximum or minimum)
\item Inflection points
\item Optimization of functions
\end{itemize}

etc.


\bigskip
\centerline{\rule{13cm}{0.4pt}}
\bigskip

\subsubsection{A CES production function example}

Given the CES production function

\begin{equation*}
Q = A\bigl[\alpha K^{-\beta} + (1-\alpha)L^{-\beta}\bigr]^{-1/\beta},
\end{equation*}

we can show that the elasticity of substitution is constant, as follows.

\textbf{First-order conditions}

The first-order conditions require that

\begin{equation*}
\frac{\partial Q / \partial L}{\partial Q / \partial K}
=
\frac{P_L}{P_K}.
\end{equation*}

Using the generalized power function rule, we take the first-order partial derivatives.

For labor,

\begin{equation*}
\frac{\partial Q}{\partial L}
=
-\frac{1}{\beta}
A\bigl[\alpha K^{-\beta} + (1-\alpha)L^{-\beta}\bigr]^{-(1/\beta+1)}
(-\beta)(1-\alpha)L^{-\beta-1}.
\end{equation*}

Canceling the $-\beta$ terms, rearranging $(1 -\alpha)$, and adding the exponents
$-(1/\beta) -1$, we obtain

\begin{equation*}
\frac{\partial Q}{\partial L}
=
(1-\alpha)A
\bigl[\alpha K^{-\beta} + (1-\alpha)L^{-\beta}\bigr]^{-(1+\beta)/\beta}
L^{-(1+\beta)}.
\end{equation*}

Substituting $A^{1+\beta}/A^{\beta}=A$, we can write

\begin{equation*}
\frac{\partial Q}{\partial L}
=
(1-\alpha)\frac{A^{1+\beta}}{A^\beta}
\bigl[\alpha K^{-\beta} + (1-\alpha)L^{-\beta}\bigr]^{-(1+\beta)/\beta}
L^{-(1+\beta)}.
\end{equation*}

From the CES production function,

\begin{equation*}
A^{1+\beta}
\bigl[\alpha K^{-\beta} + (1-\alpha)L^{-\beta}\bigr]^{-(1+\beta)/\beta}
=
Q^{1+\beta},
\end{equation*}

and

\begin{equation*}
L^{-(1+\beta)} = \frac{1}{L^{1+\beta}}.
\end{equation*}

Thus,

\begin{equation*}
\frac{\partial Q}{\partial L}
=
\frac{1-\alpha}{A^\beta}
\left(\frac{Q}{L}\right)^{1+\beta}.
\end{equation*}

\textbf{The marginal product of capital}

Similarly,

\begin{equation*}
\frac{\partial Q}{\partial K}
=
\frac{\alpha}{A^\beta}
\left(\frac{Q}{K}\right)^{1+\beta}.
\end{equation*}

Dividing the two equations and equating the result to $P_L/P_K$ (from the FOC)
leads to the cancellation of $A^\beta$ and $Q$:

\begin{equation*}
\frac{1-\alpha}{\alpha}
\left(\frac{K}{L}\right)^{1+\beta}
=
\frac{P_L}{P_K}.
\end{equation*}

Rearranging,

\begin{equation*}
\left(\frac{K}{L}\right)^{1+\beta}
=
\frac{\alpha}{1-\alpha}
\frac{P_L}{P_K},
\end{equation*}

and therefore,

\begin{equation*}
\frac{K}{L}
=
\left(\frac{\alpha}{1-\alpha}\right)^{1/(1+\beta)}
\left(\frac{P_L}{P_K}\right)^{1/(1+\beta)}.
\end{equation*}

\textbf{Elasticity of substitution}

Since $\alpha$ and $\beta$ are constants, we can treat $K/L$ as a function of
$P_L/P_K$.

Let

\begin{equation*}
h = \left(\frac{\alpha}{1-\alpha}\right)^{1/(1+\beta)}.
\end{equation*}

Then

\begin{equation*}
\frac{K}{L} = h\left(\frac{P_L}{P_K}\right)^{1/(1+\beta)}.
\end{equation*}

The marginal function is

\begin{equation*}
\frac{d(K/L)}{d(P_L/P_K)}
=
\frac{h}{1+\beta}
\left(\frac{P_L}{P_K}\right)^{1/(1+\beta)-1}.
\end{equation*}

The average function is

\begin{equation*}
\frac{K/L}{P_L/P_K}
=
h\left(\frac{P_L}{P_K}\right)^{1/(1+\beta)-1}.
\end{equation*}

Dividing the marginal function by the average function, we obtain the elasticity
of substitution:

\begin{equation*}
\text{MRS}
=
\frac{d(K/L)}{d(P_L/P_K)} \Big/ \frac{K/L}{P_L/P_K}
=
\frac{1}{1+\beta}.
\end{equation*}

This is constant, hence the CES production function exhibits \textbf{constant elasticity of substitution}.


\bigskip
\centerline{\rule{13cm}{0.4pt}}
\bigskip

\textbf{Interpretation}

\begin{itemize}
\item If $-1 < \beta < 0$, then MRS $> 1$.
\item If $\beta = 0$, then MRS $= 1$ (Cobb--Douglas case).
\item If $0 < \beta < \infty$, then MRS $< 1$.
\end{itemize}

\begin{framed}
\textbf{Intuition: elasticity of substitution}\\
The elasticity of substitution measures how easily a firm can substitute labor for capital when their relative prices change. In a CES production function, this elasticity is constant: it does not depend on the levels of $K$, $L$, or output. When the elasticity is high, firms can adjust input combinations easily in response to wage or rental-rate changes; when it is low, substitution is difficult and input proportions are relatively rigid. The parameter $\beta$ governs this flexibility: values of $\beta$ close to zero imply unit elasticity (the Cobb--Douglas case), while larger values of $\beta$ imply more limited substitution.
\end{framed}
{"version":"1","records":[{"hierarchy":{"lvl1":"Math Notes"},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":"Math Notes"},"content":"These notes build intuition for core tools used in economics, data science, and optimization.\n\nTopics (in progress)\n\nLinear Algebra\n\nCalculus\n\nDifference & Differential Equations\n\nNote\n\nThis is a living document. I will revise and expand as the course evolves.","type":"content","url":"/","position":1},{"hierarchy":{"lvl1":"Linear Algebra: Basics"},"type":"lvl1","url":"/linear-algebra-basic","position":0},{"hierarchy":{"lvl1":"Linear Algebra: Basics"},"content":"","type":"content","url":"/linear-algebra-basic","position":1},{"hierarchy":{"lvl1":"Linear Algebra: Basics","lvl2":"Vector"},"type":"lvl2","url":"/linear-algebra-basic#vector","position":2},{"hierarchy":{"lvl1":"Linear Algebra: Basics","lvl2":"Vector"},"content":"A vector is a mathematical quantity that has both magnitude (or size) and direction.\n\nGeometrically, a vector is represented as a directed line segment, like an arrow, where the length signifies the magnitude and the arrowhead indicates the direction.\n\nMore conveniently, you may write a vector as \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} or simply \\vec{v}. Can you tell what the magnitude (or length or norm) of the above vector is?\n\nAnother way to represent a vector is by using basis vectors, i.e. \\hat{\\imath} and \\hat{\\jmath}, where \\hat{\\imath} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} and \\hat{\\jmath} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}. Hence the vector \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} can be written as \\vec{v} = 1 \\hat{\\imath} + 2 \\hat{\\jmath}.\n\nMake sure you know how to add, subtract vectors, and also multiply/scale vectors by some scalar.\n\nTry these ^^\n\n(1) Express the above two vectors as column vectors and in \\hat{\\imath}, \\hat{\\jmath} notation.\n\n(2) Find also:\n\n(a) \\vec{v} + \\vec{w}    (b) \\vec{v} - \\vec{w}   \n(c) \\vec{v} + 2\\vec{w}    (d) \\vec{v} - 2\\vec{w}\n\n(3) Consider the following vectors:\\vec{u} = \\begin{bmatrix} -1 \\\\ 2 \\\\ 3 \\end{bmatrix}, \\quad\n\\vec{v} = \\begin{bmatrix} -1 \\\\ 2 \\\\ 3 \\end{bmatrix}, \\quad\n\\vec{w} = \\begin{bmatrix} 4 \\\\ 5 \\\\ -2 \\end{bmatrix}\n\nFind:\n\n(a) \\vec{u} + \\vec{v}     (b) -\\vec{u} + 2\\vec{v} - \\vec{w}(c) \\vec{u} \\cdot \\vec{v} (i.e. the dot product)    (d) \\vec{u}^T \\vec{v}(e) Which of the above vectors are orthogonal?(f) Express each of the vectors in \\hat{\\imath}, \\hat{\\jmath}, \\hat{k} notation.","type":"content","url":"/linear-algebra-basic#vector","position":3},{"hierarchy":{"lvl1":"Linear Algebra: Basics","lvl2":"Matrix Basics"},"type":"lvl2","url":"/linear-algebra-basic#matrix-basics","position":4},{"hierarchy":{"lvl1":"Linear Algebra: Basics","lvl2":"Matrix Basics"},"content":"We have already introduced the basis vectors \\hat{\\imath} and \\hat{\\jmath}, which we can put into a matrix. That is, the \\hat{\\imath} and \\hat{\\jmath} vectors placed side-by-side in a 2 \\times 2 matrix:\\begin{bmatrix} \n1 & 0 \\\\ \n0 & 1 \n\\end{bmatrix}\n\nNow suppose we want to rotate \\vec{v} = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} anticlockwise by 90°. Where will it go?\n\nWe can rotate the 2-D space, such that \\hat{\\imath} will go to \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} and \\hat{\\jmath} will go to \\begin{bmatrix} -1 \\\\ 0 \\end{bmatrix}. Combining these into a matrix gives:\\begin{bmatrix} 0 & -1 \\\\ 1 & 0 \\end{bmatrix}\n\nThis can easily be done by pre-multiplying the vector by the transformation matrix, as follows:\\begin{bmatrix} 0 & -1 \\\\ 1 & 0 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} = \\begin{bmatrix} -2 \\\\ 1 \\end{bmatrix}\n\nBasically, a matrix can be viewed as a way to transform/change a vector!\n\nMatrix multiplication\n\nThe proper way to multiply a matrix and a vector is:1 \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} + 2 \\begin{bmatrix} -1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} -2 \\\\ 1 \\end{bmatrix}\n\nMatrix addition and subtraction: If \\mathbf{A} = (a_{ij}) and \\mathbf{B} = (b_{ij}) are two m \\times n matrices of same dimension, then \\mathbf{A} + \\mathbf{B} is defined as (a_{ij} + b_{ij}). That is, we add element by element the two matrices.Clearly \\mathbf{A} + \\mathbf{B} = \\mathbf{B} + \\mathbf{A}. The rule applies to matrix subtraction.\n\nTranspose of a matrix: If \\mathbf{A} is an m \\times n matrix, then \\mathbf{A}' is the n \\times m matrix whose rows are the columns of \\mathbf{A}. So \\mathbf{A}' = (a_{ji}). For example:\\begin{bmatrix}\na & b \\\\\nc & d\n\\end{bmatrix} = \\begin{bmatrix}\na & c \\\\\nb & d\n\\end{bmatrix}\n\nNote: (\\mathbf{A} + \\mathbf{B})' = \\mathbf{A}' + \\mathbf{B}', however (\\mathbf{A}\\mathbf{B})' = \\mathbf{B}'\\mathbf{A}'.\n\nNull matrix: Has all elements as 0. Clearly \\mathbf{A} + \\mathbf{0}_{m,n} = \\mathbf{0}_{m,n} + \\mathbf{A} = \\mathbf{A} for all m \\times n matrices.\n\nScalar multiplication: If \\mathbf{A} = (a_{ij}) then for any constant k, define k\\mathbf{A} = (k a_{ij}). That is, multiply each element in \\mathbf{A} by k.\n\nMatrix multiplication: Say \\mathbf{A} is m \\times n, and \\mathbf{B} is n \\times p, then the m \\times p matrix \\mathbf{A}\\mathbf{B} is the product of \\mathbf{A} and \\mathbf{B}. For example:\\begin{bmatrix}\na & b \\\\\nc & d\n\\end{bmatrix}\n\\begin{bmatrix}\ne & f \\\\\ng & h\n\\end{bmatrix}\n= \\begin{bmatrix}\nae + bg & af + bh \\\\\nce + dg & cf + dh\n\\end{bmatrix}\n\nand the matrices \\mathbf{A} and \\mathbf{B} are conformable.\n\nFro example, given the matrices:A = \\begin{bmatrix} 1 & 2 \\\\ 3 & -4 \\end{bmatrix}, \\quad B = \\begin{bmatrix} 5 & 0 \\\\ -6 & 7 \\end{bmatrix}\n\nTo multiply A and B, we apply the row-by-column rule:\\begin{align*}\n\\begin{bmatrix} 1 & 2 \\\\ 3 & -4 \\end{bmatrix}\n\\begin{bmatrix} 5 & 0 \\\\ -6 & 7 \\end{bmatrix}\n&= \\begin{bmatrix}\n(1 \\times 5) + (2 \\times -6) & (1 \\times 0) + (2 \\times 7) \\\\\n(3 \\times 5) + (-4 \\times -6) & (3 \\times 0) + (-4 \\times 7)\n\\end{bmatrix} \\\\\n\n&= \\begin{bmatrix}\n5 - 12 & 0 + 14 \\\\\n15 + 24 & 0 - 28\n\\end{bmatrix} = \\begin{bmatrix}\n-7 & 14 \\\\\n39 & -28\n\\end{bmatrix}\n\\end{align*}\n\nLong story!!\n\nMatrix Multiplication: The Column-Wise (Basis Vector) Method\n\nInstead of dot products, we can view AB as taking the columns of A (the transformed \\hat{\\imath} and \\hat{\\jmath}) and scaling them by the components in each column of B.\n\nFrom the sam eexample above, let the columns of A be:\\vec{a}_1 = \\begin{bmatrix} 1 \\\\ 3 \\end{bmatrix}, \\quad \\vec{a}_2 = \\begin{bmatrix} 2 \\\\ -4 \\end{bmatrix}\n\nFinding Column 1 of the Result:\n\nWe use the first column of B \\begin{bmatrix} 5 \\\\ -6 \\end{bmatrix} as scalars:5\\begin{bmatrix} 1 \\\\ 3 \\end{bmatrix} + (-6)\\begin{bmatrix} 2 \\\\ -4 \\end{bmatrix} = \\begin{bmatrix} 5 \\\\ 15 \\end{bmatrix} + \\begin{bmatrix} -12 \\\\ 24 \\end{bmatrix} = \\begin{bmatrix} -7 \\\\ 39 \\end{bmatrix}\n\nFinding Column 2 of the Result:\n\nWe use the second column of B \\begin{bmatrix} 0 \\\\ 7 \\end{bmatrix} as scalars:0\\begin{bmatrix} 1 \\\\ 3 \\end{bmatrix} + 7\\begin{bmatrix} 2 \\\\ -4 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} + \\begin{bmatrix} 14 \\\\ -28 \\end{bmatrix} = \\begin{bmatrix} 14 \\\\ -28 \\end{bmatrix}\n\nFinal Result:AB = \\begin{bmatrix} -7 & 14 \\\\ 39 & -28 \\end{bmatrix}\n\nas before!\n\nTry these ^^\n\n(1) For the following matrices:A = \\begin{bmatrix} 1 & 2 \\\\ 3 & -4 \\end{bmatrix}, \\quad\nB = \\begin{bmatrix} 5 & 0 \\\\ -6 & 7 \\end{bmatrix},C = \\begin{bmatrix} 1 & -3 & 4 \\\\ 2 & 6 & -5 \\end{bmatrix}, \\quad\nD = \\begin{bmatrix} 3 & 7 & -1 \\\\ 4 & -8 & 9 \\end{bmatrix}.\n\nFind:\n\n(a) 5A - 2B    (b)  2A + 3B    (c)  2C - 3D(d)  AB and (AB)C    (e) BC and A(BC) [Note that (AB)C = A(BC)](f) A^2 and A^3    (g) AD and BD(h) C^T D [Note that we cannot get CD. Why?](i) A^T    (j) B^T    (k) A^T B^T    (l) (AB)^T [Note that A^T B^T \\neq (AB)^T]\n\nDiagonal matrix: A square n \\times n matrix \\mathbf{A} is diagonal if all entries off the ‘main diagonal’ are zero, i.e.\\mathbf{A} = \\begin{bmatrix}\na_{11} & 0 & \\cdots & 0 \\\\\n0 & a_{22} & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & a_{nn}\n\\end{bmatrix}\n\nNote: The elements in the diagonal of \\mathbf{A} are the same as those in \\mathbf{A}'.\n\nTrace of a square matrix: If \\mathbf{A} is a square matrix, the trace of \\mathbf{A}, denoted \\operatorname{tr}(\\mathbf{A}), is the sum of the elements on the main/leading diagonal of \\mathbf{A}.\n\nIdentity matrix: Denoted by \\mathbf{I}_n, the n \\times n diagonal matrix with a_{ii} = 1 for all i. That is:\\mathbf{I}_n = \\begin{bmatrix}\n1 & 0 & \\cdots & 0 \\\\\n0 & 1 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & 1\n\\end{bmatrix}\n\nSymmetric matrix: A square matrix \\mathbf{A} is symmetric if \\mathbf{A} = \\mathbf{A}'. The identity matrix and the square null matrix are symmetric.\n\nIdempotent matrix: A square matrix is said to be idempotent if \\mathbf{A}^n = \\cdots = \\mathbf{A}^2 = \\mathbf{A}. Below is an example (try squaring the matrix and see what you get).\\begin{bmatrix}\n1 & 0 \\\\\n0 & 0\n\\end{bmatrix}\n\nSingular Matrix: A matrix that is linearly dependent (in the columns or rows) is singular and has determinant of zero. Note that any two (or three) vectors \\mathbf{x} and \\mathbf{y} (and \\mathbf{z}) are said to be linearly dependent iff one can be written as a scalar multiple of the other. Two vectors \\mathbf{x} \\neq \\mathbf{0} and \\mathbf{y} \\neq \\mathbf{0} are said to be linearly independent iff the ONLY solution to a\\mathbf{x} + b\\mathbf{y} = \\mathbf{0} is a = 0 AND b = 0. And \\mathbf{x} and \\mathbf{y} are said to be orthogonal.\n\nTry these ^^\n\nFind:(a) \\quad \n\n\\begin{bmatrix} 1 & 1 \\\\ 0 & 1 \\end{bmatrix}^2 \\quad \\text{and} \\quad \\begin{bmatrix} 1 & 1 \\\\ 0 & 1 \\end{bmatrix}^3(b) \\quad \n\n\\begin{bmatrix} 0 & 0 \\\\ 0 & 1 \\end{bmatrix}^2 \\quad \\text{and} \\quad \\begin{bmatrix} 0 & 0 \\\\ 0 & 1 \\end{bmatrix}^3","type":"content","url":"/linear-algebra-basic#matrix-basics","position":5},{"hierarchy":{"lvl1":"Linear Algebra: Basics","lvl3":"Some Notes on Matrices","lvl2":"Matrix Basics"},"type":"lvl3","url":"/linear-algebra-basic#some-notes-on-matrices","position":6},{"hierarchy":{"lvl1":"Linear Algebra: Basics","lvl3":"Some Notes on Matrices","lvl2":"Matrix Basics"},"content":"Usually \\mathbf{A}\\mathbf{B} \\neq \\mathbf{B}\\mathbf{A}. For example, try \\mathbf{A} = \\begin{bmatrix}2 & 0 \\\\ 3 & 1\\end{bmatrix} and \\mathbf{B} = \\begin{bmatrix}0 & 1 \\\\ 0 & 0\\end{bmatrix}.\n\nWe can have \\mathbf{A}\\mathbf{B} = \\mathbf{0} even if \\mathbf{A} or \\mathbf{B} are not zero. For example, try \\mathbf{A} = \\begin{bmatrix}6 & -12 \\\\ -3 & 6\\end{bmatrix} and \\mathbf{B} = \\begin{bmatrix}12 & 6 \\\\ 6 & 3\\end{bmatrix}.\n\nSay, \\mathbf{A} = \\begin{bmatrix}4 & 8 \\\\ 1 & 2\\end{bmatrix}, \\mathbf{B} = \\begin{bmatrix}2 & 1 \\\\ 2 & 2\\end{bmatrix}, and \\mathbf{C} = \\begin{bmatrix}-2 & 1 \\\\ 4 & 2\\end{bmatrix}. We find that \\mathbf{A}\\mathbf{B} = \\mathbf{A}\\mathbf{C} even though \\mathbf{B} \\neq \\mathbf{C}.\n\nSay \\mathbf{A} and \\mathbf{B} are singular matrices. Then \\mathbf{A}\\mathbf{B} will not be zero, although the product will be a singular matrix.","type":"content","url":"/linear-algebra-basic#some-notes-on-matrices","position":7},{"hierarchy":{"lvl1":"Linear Algebra: Basics","lvl2":"Systems of Equations in Matrix Form"},"type":"lvl2","url":"/linear-algebra-basic#systems-of-equations-in-matrix-form","position":8},{"hierarchy":{"lvl1":"Linear Algebra: Basics","lvl2":"Systems of Equations in Matrix Form"},"content":"One of the uses of linear algebra in economics is to represent and then solve systems of equations. For instance, consider the system of two equations:2x_1 + x_2 = 5 \\\\\n-x_1 + x_2 = 2\n\nHere, we can define:\\mathbf{A} = \\begin{bmatrix} 2 & 1 \\\\ -1 & 1 \\end{bmatrix}, \\quad \\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}, \\quad \\mathbf{b} = \\begin{bmatrix} 5 \\\\ 2 \\end{bmatrix}\n\nThen we see that:\\mathbf{A}\\mathbf{x} = \\begin{bmatrix} 2 & 1 \\\\ -1 & 1 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} = \\begin{bmatrix} 2x_1 + x_2 \\\\ -x_1 + x_2 \\end{bmatrix}\n\nThe original system is in fact equivalent to the matrix form:\\mathbf{A}\\mathbf{x} = \\mathbf{b}","type":"content","url":"/linear-algebra-basic#systems-of-equations-in-matrix-form","position":9},{"hierarchy":{"lvl1":"Linear Algebra: Basics","lvl2":"Matrix Row Operations"},"type":"lvl2","url":"/linear-algebra-basic#matrix-row-operations","position":10},{"hierarchy":{"lvl1":"Linear Algebra: Basics","lvl2":"Matrix Row Operations"},"content":"Matrix row operations, also known as elementary row operations, are three basic actions performed on a matrix: swapping two rows, multiplying a row by a non-zero constant, and adding a multiple of one row to another row.\n\nInterchanging two rows (Row Swapping): You can swap the positions of any two rows in a matrix. As we will see later, this operation is useful for changing the order of equations in a system without altering the solution.\n\nMultiplying a row by a non-zero constant (Scalar Multiplication): You can multiply every element in a specific row by any non-zero number. This is equivalent to multiplying both sides of an equation by a constant.\n\nAdding a multiple of one row to another row (Row Addition): You can multiply one row by a constant and then add the result to another row. The original row and the row being multiplied remain unchanged. This operation is often the most powerful for simplifying systems, as it corresponds to adding a modified version of one equation to another.\n\nWe can solve system of equation by row operations.\n\nTo solve for x_1 and x_2, we can use an augmented matrix and perform elementary row operations—swapping rows, scalar multiplication, and row addition—to reach Reduced Row Echelon Form (RREF).\n\n1. Set up the augmented matrix:\\begin{bmatrix} 2 & 1 & | & 5 \\\\ -1 & 1 & | & 2 \\end{bmatrix}\n\n2. Create a leading one in Row 1:\n\nWe can swap R_1 and R_2:\\begin{bmatrix} -1 & 1 & | & 2 \\\\ 2 & 1 & | & 5 \\end{bmatrix}\n\nThen multiply R_1 by -1 to get a leading one (R_1 \\leftarrow -1 \\cdot R_1):\\begin{bmatrix} 1 & -1 & | & -2 \\\\ 2 & 1 & | & 5 \\end{bmatrix}\n\n3. Create a zero below the leading one:\n\nAdd -2 times R_1 to R_2 (R_2 \\leftarrow R_2 - 2R_1):\\begin{bmatrix} 1 & -1 & | & -2 \\\\ 0 & 3 & | & 9 \\end{bmatrix}\n\n4. Create a leading one in Row 2:\n\nMultiply R_2 by 1/3 (R_2 \\leftarrow \\frac{1}{3}R_2):\\begin{bmatrix} 1 & -1 & | & -2 \\\\ 0 & 1 & | & 3 \\end{bmatrix}\n\n5. Create a zero above the leading one (RREF):\n\nTo reach reduced row echelon form, every column with a leading one must have zeros elsewhere. Add R_2 to R_1 (R_1 \\leftarrow R_1 + R_2):\\begin{bmatrix} 1 & 0 & | & 1 \\\\ 0 & 1 & | & 3 \\end{bmatrix}\n\nBasically, a matrix can be viewed as a way to transform or change a vector to solve these systems!\n\nFinal Result\n\nThe system provides the unique solution:x_1 = 1, \\quad x_2 = 3\n\nBasically, a matrix can be viewed as a way to transform or change a vector to solve these systems!\n\nNote: We need not reach RREF, and could stop at Step 4 (or even Step 3). At Step 4, the last row tells us that x_2 = 3. We can then use back-substitution into the first row (x_1 - x_2 = -2):x_1 - 3 = -2 \\implies x_1 = 1\n\nThis confirms the same result without performing the final row addition.\n\nDefinition: REF (Row Echelon Form)\n\nA matrix is in Row Echelon Form if...\n\nEvery non-zero row begins with a leading one.\n\nA leading one in a lower row is further to the right.\n\nZero rows are at the bottom of the matrix.\n\nNote: In some books, leading by one is not required.\n\nDefinition: RREF Reduced Row Echelon Form\n\nA matrix is in Reduced Row Echelon Form if...\n\nEvery non-zero row begins with a leading one.\n\nA leading one in a lower row is further to the right.\n\nZero rows are at the bottom of the matrix.\n\nEvery column with a leading one has zeros elsewhere.\n\nWhile there can exist several row echelon forms for a matrix, there is only one (a unique) reduced row echelon form.\n\nTry these ^^\n\nFor the following matrices:A = \\begin{bmatrix} 2 & 0 & 0 \\\\ 0 & 3 & 0 \\\\ 0 & 0 & 5 \\end{bmatrix}, \\quad \nB = \\begin{bmatrix} 7 & 0 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & -4 \\end{bmatrix},C = \\begin{bmatrix} 2 & 1 & 2 \\\\ 0 & 3 & 0 \\\\ 0 & 0 & 5 \\end{bmatrix}, \\quad \nD = \\begin{bmatrix} 7 & 0 & 0 \\\\ -2 & 0 & 0 \\\\ 3 & 1 & -4 \\end{bmatrix}.\n\nFind:\n\n(a) AB, A^2, B^2    (b) CD, C^2, D^2    (c) AC and BD(d) Determinant of A, B, C and D    (e) Inverse of A and C(f) Is the matrix A in REF or RREF? What are its pivots?(g) How about matrix B and C? Explain.(h) Change all matrices A to D to RREF (if they aren’t in RREF yet) and determine their rank.","type":"content","url":"/linear-algebra-basic#matrix-row-operations","position":11},{"hierarchy":{"lvl1":"Linear Algebra: Basics","lvl2":"The Determinant of a Matrix"},"type":"lvl2","url":"/linear-algebra-basic#the-determinant-of-a-matrix","position":12},{"hierarchy":{"lvl1":"Linear Algebra: Basics","lvl2":"The Determinant of a Matrix"},"content":"The determinant is a scalar value uniquely associated with a square non-singular matrix, and is usually denoted as |\\mathbf{A}|. The question of whether or not a matrix is nonsingular and therefore invertible is linked to the value of its determinant.\n\nWe can write a generic 2 \\times 2 matrix as:\\mathbf{A} = \\begin{bmatrix} a_{11} & a_{12} \\\\ a_{21} & a_{22} \\end{bmatrix}\n\nand its determinant is defined as:|\\mathbf{A}| = a_{11}a_{22} - a_{12}a_{21}\n\nFor a 3 \\times 3 matrix, say:\\mathbf{A} = \\begin{bmatrix} a & b & c \\\\ d & e & f \\\\ g & h & i \\end{bmatrix}\n\nits determinant is:|\\mathbf{A}| = a(ei - fh) - b(di - fg) + c(dh - ef)\n\nNote the sign in front of b is begative, because we imppose (multiply by) the sign matrix \\begin{bmatrix} + & - & + \\\\ - & + & - \\\\ + & - & + \\end{bmatrix}.\n\nVisually:\n\nThe ‘yellow’ bits called minors are ‘smaller’ determinants, now 2 by 2 and easier to handle. Note that the elements of the minor come from remaining elements after deleting the rows and columns of the corresponding elements in the selected row (or column).\n\nLet’s take a numerical example.\n\nTo find the determinant of matrix A using Cofactor Expansion along the first row (technically we could pick any row to work with) involves multiplying each element of the first row by its corresponding 2 \\times 2 minor, following the sign pattern: (+ , - , +) for the first row.\n\nGiven:A = \\begin{bmatrix} 2 & -3 & 1 \\\\ 1 & -1 & 2 \\\\ 3 & 1 & -1 \\end{bmatrix}\n\nThe formula for expansion along the first row is:\\det(A) = a(M_{11}) - b(M_{12}) + c(M_{13})\n\n1. First Element (a = 2):+2 \\cdot \\begin{vmatrix} -1 & 2 \\\\ 1 & -1 \\end{vmatrix} = 2 \\cdot [(-1)(-1) - (2)(1)] = 2(1 - 2) = -2\n\n2. Second Element (b = -3):\nNote the negative sign from the checkerboard pattern:-(-3) \\cdot \\begin{vmatrix} 1 & 2 \\\\ 3 & -1 \\end{vmatrix} = 3 \\cdot [(1)(-1) - (2)(3)] = 3(-1 - 6) = -21\n\n3. Third Element (c = 1):+1 \\cdot \\begin{vmatrix} 1 & -1 \\\\ 3 & 1 \\end{vmatrix} = 1 \\cdot [(1)(1) - (-1)(3)] = 1(1 + 3) = 4\n\nFinal Result\n\nSumming the results from each step gives \\det(A) = -2 - 21 + 4 = -19.","type":"content","url":"/linear-algebra-basic#the-determinant-of-a-matrix","position":13},{"hierarchy":{"lvl1":"Linear Algebra: Basics","lvl2":"Properties of Determinants"},"type":"lvl2","url":"/linear-algebra-basic#properties-of-determinants","position":14},{"hierarchy":{"lvl1":"Linear Algebra: Basics","lvl2":"Properties of Determinants"},"content":"Property 1: |\\mathbf{A}| = |\\mathbf{A}'| and |\\mathbf{A}| \\cdot |\\mathbf{B}| = |\\mathbf{B}| \\cdot |\\mathbf{A}|.\n\nProperty 2: Interchanging any two rows or columns will affect the sign of the determinant, but not its absolute value.\n\nProperty 3: Multiplying a single row or column by a scalar will cause the value of the determinant to be multiplied by the scalar.\n\nProperty 4: Addition or subtraction of a nonzero multiple of any row or column to or from another row or column does not change the value of the determinant.\n\nProperty 5: The determinant of a triangular matrix is equal to the product of the elements along the principal diagonal.\n\nProperty 6: If any of the rows or columns equal zero, the determinant is also zero.\n\nProperty 7: If two rows or columns are identical or proportional, i.e. linearly dependent, then the determinant is zero.\n\nNote: Properties 2,3 and 4  are standard row operations already discussed above.","type":"content","url":"/linear-algebra-basic#properties-of-determinants","position":15},{"hierarchy":{"lvl1":"Linear Algebra: Basics","lvl2":"Matrix Inversion"},"type":"lvl2","url":"/linear-algebra-basic#matrix-inversion","position":16},{"hierarchy":{"lvl1":"Linear Algebra: Basics","lvl2":"Matrix Inversion"},"content":"Consider,the following.\\mathbf{A} = \\begin{bmatrix} 1 & -2 \\\\ 3 & 4 \\end{bmatrix}\n\nThen pre-multiplying by:\\begin{bmatrix} 4 & 2 \\\\ -3 & 1 \\end{bmatrix}\n\nGives:\\begin{bmatrix} 4 & 2 \\\\ -3 & 1 \\end{bmatrix} \\begin{bmatrix} 1 & -2 \\\\ 3 & 4 \\end{bmatrix} = \\begin{bmatrix} 10 & 0 \\\\ 0 & 10 \\end{bmatrix} = 10 \\mathbf{I}\n\nWhere does the 10 in the matrix after the = sign come from? It is the determinant of \\mathbf{A}.\n\nThen, (pre-)multiplying both sides of the equation by \\frac{1}{10} gives:\\frac{1}{10} \\begin{bmatrix} 4 & 2 \\\\ -3 & 1 \\end{bmatrix} \\begin{bmatrix} 1 & -2 \\\\ 3 & 4 \\end{bmatrix} = \\mathbf{I}\n\nThe matrix \\frac{1}{10} \\begin{bmatrix} 4 & 2 \\\\ -3 & 1 \\end{bmatrix} is in fact the inverse of \\mathbf{A}.\n\nHence we have the relationship \\mathbf{A}^{-1} \\mathbf{A} = \\mathbf{I}. In fact, \\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{I}.","type":"content","url":"/linear-algebra-basic#matrix-inversion","position":17},{"hierarchy":{"lvl1":"Linear Algebra: Basics","lvl2":"Finding the Inverse of a Matrix by Row Operations"},"type":"lvl2","url":"/linear-algebra-basic#finding-the-inverse-of-a-matrix-by-row-operations","position":18},{"hierarchy":{"lvl1":"Linear Algebra: Basics","lvl2":"Finding the Inverse of a Matrix by Row Operations"},"content":"We can find the inverse of a matrix using row operations or Gauss-Jordan Elimination method. The objective is to transform the matrix A into the Identity matrix I. By applying the same sequence of elementary row operations to an Identity matrix simultaneously, that Identity matrix transforms into A^{-1}.\n\nGiven the matrix A from the example aboveA = \\begin{bmatrix} 1 & -2 \\\\ 3 & 4 \\end{bmatrix}\n\nWe set up an augmented matrix [A | I] by placing the 2 \\times 2 Identity matrix to the right of A:[A | I] = \\begin{bmatrix} 1 & -2 & | & 1 & 0 \\\\ 3 & 4 & | & 0 & 1 \\end{bmatrix}\n\nStep-by-Step Row Operations\n\n1. Create a zero below the first leading one:\n\nSubtract 3 times the first row from the second row (R_2 \\leftarrow R_2 - 3R_1):\\begin{bmatrix} 1 & -2 & | & 1 & 0 \\\\ 0 & 10 & | & -3 & 1 \\end{bmatrix}\n\n2. Create a leading one in the second row:\n\nMultiply the second row by 1/10 (R_2 \\leftarrow \\frac{1}{10}R_2):\\begin{bmatrix} 1 & -2 & | & 1 & 0 \\\\ 0 & 1 & | & -3/10 & 1/10 \\end{bmatrix}\n\n3. Create a zero above the second leading one:\n\nTo reach Reduced Row Echelon Form (RREF), we add 2 times the second row to the first row (R_1 \\leftarrow R_1 + 2R_2):\\begin{bmatrix} 1 & 0 & | & 1 + 2(-3/10) & 0 + 2(1/10) \\\\ 0 & 1 & | & -3/10 & 1/10 \\end{bmatrix}\n\nSimplifying the arithmetic in the first row:\\begin{bmatrix} 1 & 0 & | & 4/10 & 2/10 \\\\ 0 & 1 & | & -3/10 & 1/10 \\end{bmatrix}\n\nFinal Result\n\nThe left side of the augmented matrix has been transformed into the Identity matrix. Therefore, the right side is now the inverse, A^{-1}:A^{-1} = \\begin{bmatrix} 0.4 & 0.2 \\\\ -0.3 & 0.1 \\end{bmatrix}\n\nVerification\n\nA matrix multiplied by its inverse must result in the Identity matrix:\\begin{bmatrix} 1 & -2 \\\\ 3 & 4 \\end{bmatrix}\n\\begin{bmatrix} 0.4 & 0.2 \\\\ -0.3 & 0.1 \\end{bmatrix}\n= \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}","type":"content","url":"/linear-algebra-basic#finding-the-inverse-of-a-matrix-by-row-operations","position":19},{"hierarchy":{"lvl1":"Linear Algebra: Basics","lvl2":"Properties of Inverse"},"type":"lvl2","url":"/linear-algebra-basic#properties-of-inverse","position":20},{"hierarchy":{"lvl1":"Linear Algebra: Basics","lvl2":"Properties of Inverse"},"content":"Here are some properties of the inverse of a matrix worth knowing:\n\nProperty 1: For any nonsingular matrix \\mathbf{A}, (\\mathbf{A}^{-1})^{-1} = \\mathbf{A}.\n\nProperty 2: The determinant of the inverse of a matrix is equal to the reciprocal of the determinant of the matrix. That is |\\mathbf{A}^{-1}| = \\frac{1}{|\\mathbf{A}|}.\n\nProperty 3: The inverse of matrix \\mathbf{A} is unique.\n\nProperty 4: For any nonsingular matrix \\mathbf{A}, the inverse of the transpose of a matrix is equal to the transpose of the inverse of the matrix. (\\mathbf{A}')^{-1} = (\\mathbf{A}^{-1})'.\n\nProperty 5: If \\mathbf{A} and \\mathbf{B} are nonsingular and of the same dimension, then \\mathbf{A}\\mathbf{B} is also nonsingular.\n\nProperty 6: The inverse of the product of two matrices is equal to the product of their inverses in reverse order. (\\mathbf{A}\\mathbf{B})^{-1} = \\mathbf{B}^{-1}\\mathbf{A}^{-1}.","type":"content","url":"/linear-algebra-basic#properties-of-inverse","position":21},{"hierarchy":{"lvl1":"Linear Algebra: Basics","lvl2":"Finding the Inverse of a Matrix (Standard Approach)"},"type":"lvl2","url":"/linear-algebra-basic#finding-the-inverse-of-a-matrix-standard-approach","position":22},{"hierarchy":{"lvl1":"Linear Algebra: Basics","lvl2":"Finding the Inverse of a Matrix (Standard Approach)"},"content":"The inverse of a matrix is defined as:\\mathbf{A}^{-1} = \\frac{1}{|\\mathbf{A}|} \\operatorname{adj}(\\mathbf{A})\n\nLet’s say we have the following linear system:2x_1 - 3x_2 + x_3 = -1 \\\\\nx_1 - x_2 + 2x_3 = -3 \\\\\n3x_1 + x_2 - x_3 = 9\n\nWriting the above system in the form \\mathbf{A}\\mathbf{x} = \\mathbf{b}, where:\\mathbf{A} = \\begin{bmatrix} 2 & -3 & 1 \\\\ 1 & -1 & 2 \\\\ 3 & 1 & -1 \\end{bmatrix}, \\quad \\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix}, \\quad \\mathbf{b} = \\begin{bmatrix} -1 \\\\ -3 \\\\ 9 \\end{bmatrix}\n\nWe can therefore solve the system by \\mathbf{x} = \\mathbf{A}^{-1}\\mathbf{b}, which means that we need the inverse of \\mathbf{A}.\n\nStep 1. The minor |M_{ij}| is the determinant of the submatrix formed by deleting the i-th row and j-th column of the original matrix. So we have:\\begin{bmatrix}\n-1 & -7 & 4 \\\\\n2 & -5 & 11 \\\\\n-5 & 3 & 1\n\\end{bmatrix}\n\nStep 2: The cofactor C_{ij} is a minor with the prescribed sign that follows the rule:C_{ij} = (-1)^{i+j} |M_{ij}|\n\nHence, we have:\\begin{bmatrix}\n-1 & 7 & 4 \\\\\n-2 & -5 & -11 \\\\\n-5 & -3 & 1\n\\end{bmatrix}\n\nStep 3: An adjoint matrix is simply the transpose of a cofactor matrix. Continuing the example above, we have:\\begin{bmatrix}\n-1 & -2 & -5 \\\\\n7 & -5 & -3 \\\\\n4 & -11 & 1\n\\end{bmatrix}\n\nSince |\\mathbf{A}| = -19, we then have:\\mathbf{A}^{-1} = - \\frac{1}{19} \\begin{bmatrix}\n-1 & -2 & -5 \\\\\n7 & -5 & -3 \\\\\n4 & -11 & 1\n\\end{bmatrix}\n\nand\\mathbf{x} = \\mathbf{A}^{-1}\\mathbf{b} = - \\frac{1}{19} \\begin{bmatrix}\n-1 & -2 & -5 \\\\\n7 & -5 & -3 \\\\\n4 & -11 & 1\n\\end{bmatrix} \\begin{bmatrix} -1 \\\\ -3 \\\\ 9 \\end{bmatrix}\n\nwhich gives x_1 = 2, x_2 = 1 and x_3 = -2.","type":"content","url":"/linear-algebra-basic#finding-the-inverse-of-a-matrix-standard-approach","position":23},{"hierarchy":{"lvl1":"Linear Algebra: Basics","lvl2":"Cramer’s Rule"},"type":"lvl2","url":"/linear-algebra-basic#cramers-rule","position":24},{"hierarchy":{"lvl1":"Linear Algebra: Basics","lvl2":"Cramer’s Rule"},"content":"Cramer came up with a simplified method for solving a system of linear equations through the use of determinants.\n\nBasically, given \\mathbf{A}\\mathbf{x} = \\mathbf{b}, we have:\\mathbf{A} = \\begin{bmatrix} a_{11} & a_{12} & a_{13} \\\\ a_{21} & a_{22} & a_{23} \\\\ a_{31} & a_{32} & a_{33} \\end{bmatrix}, \\quad \\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix}, \\quad \\mathbf{b} = \\begin{bmatrix} b_1 \\\\ b_2 \\\\ b_3 \\end{bmatrix}\n\nDefine:\\Delta = |\\mathbf{A}|\n\nand\\Delta_1 = \\begin{vmatrix} b_1 & a_{12} & a_{13} \\\\ b_2 & a_{22} & a_{23} \\\\ b_3 & a_{32} & a_{33} \\end{vmatrix}, \\quad \\Delta_2 = \\begin{vmatrix} a_{11} & b_1 & a_{13} \\\\ a_{21} & b_2 & a_{23} \\\\ a_{31} & b_3 & a_{33} \\end{vmatrix}, \\quad \\Delta_3 = \\begin{vmatrix} a_{11} & a_{12} & b_1 \\\\ a_{21} & a_{22} & b_2 \\\\ a_{31} & a_{32} & b_3 \\end{vmatrix}\n\nwhere the columns in the \\mathbf{A} matrix are replaced one by one by the \\mathbf{b} vector as shown above.\n\nThen:x_i = \\frac{\\Delta_i}{\\Delta}\n\nSo, for the previous example:\\mathbf{A} = \\begin{bmatrix} 2 & -3 & 1 \\\\ 1 & -1 & 2 \\\\ 3 & 1 & -1 \\end{bmatrix} \\quad \\text{and} \\quad \\det(A) = -19, \\quad \\mathbf{b} = \\begin{bmatrix} -1 \\\\ -3 \\\\ 9 \\end{bmatrix}\n\nIt follows that\\Delta_1 = \\begin{vmatrix} -1 & -3 & 1 \\\\ -3 & -1 & 2 \\\\ 9 & 1 & -1 \\end{vmatrix}, \\quad \\Delta_2 = \\begin{vmatrix} 2 & -1 & 1 \\\\ 1 & -3 & 2 \\\\ 3 & 9 & -1 \\end{vmatrix}, \\quad \\Delta_3 = \\begin{vmatrix} 2 & -3 & -1 \\\\ 1 & -1 & -3 \\\\ 3 & 1 & 9 \\end{vmatrix}\n\nwhere \\Delta_1 = -38, \\Delta_2 = -19, and \\Delta_3 = 38.\n\nSo dividing through, you can easily verify that you get the soluitons \\{2, 1, -2\\} as before!","type":"content","url":"/linear-algebra-basic#cramers-rule","position":25}]}
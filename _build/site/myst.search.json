{"version":"1","records":[{"hierarchy":{"lvl1":"Basic Differentiation"},"type":"lvl1","url":"/basic-differentiation","position":0},{"hierarchy":{"lvl1":"Basic Differentiation"},"content":"Before we study the differentiation of single-variable functions, we briefly review several foundational mathematical concepts.","type":"content","url":"/basic-differentiation","position":1},{"hierarchy":{"lvl1":"Basic Differentiation","lvl2":"Functions"},"type":"lvl2","url":"/basic-differentiation#functions","position":2},{"hierarchy":{"lvl1":"Basic Differentiation","lvl2":"Functions"},"content":"A function  f  from a set  X  to a set  Y , written  f: X \\to Y , is a rule that assigns exactly one element of  Y  to each element of  X .\n\nX is called the domain\n\nThe range is the set of values in Y that are actually attained\n\nUsing x \\in X and y \\in Y, a function is written as\n y = f(x) ,\nwhere x is the independent variable and y the dependent variable.","type":"content","url":"/basic-differentiation#functions","position":3},{"hierarchy":{"lvl1":"Basic Differentiation","lvl2":"Graphs"},"type":"lvl2","url":"/basic-differentiation#graphs","position":4},{"hierarchy":{"lvl1":"Basic Differentiation","lvl2":"Graphs"},"content":"If X and Y are sets of real numbers, the graph of a function f is the set of points\n(x, y) such that y = f(x).\n\nEconomic convention.Economists often draw demand curves with quantity on the horizontal axis and price on the vertical axis, even when the function is written as  q = f(p) .","type":"content","url":"/basic-differentiation#graphs","position":5},{"hierarchy":{"lvl1":"Basic Differentiation","lvl2":"Slope"},"type":"lvl2","url":"/basic-differentiation#slope","position":6},{"hierarchy":{"lvl1":"Basic Differentiation","lvl2":"Slope"},"content":"The slope of a line through points (x, y) and (x', y') ism = \\frac{y' - y}{x' - x}.\n\nDifferentiation is the method of finding the slope of a function and is denoted by  f'(x) .","type":"content","url":"/basic-differentiation#slope","position":7},{"hierarchy":{"lvl1":"Basic Differentiation","lvl2":"Limits"},"type":"lvl2","url":"/basic-differentiation#limits","position":8},{"hierarchy":{"lvl1":"Basic Differentiation","lvl2":"Limits"},"content":"We say that a function f has limit L as x \\to a if, for any \\varepsilon > 0, there exists a \\delta > 0 such that|f(x) - L| < \\varepsilon\n\\quad \\text{whenever} \\quad\n0 < |x - a| < \\delta.\n\nWhen this condition holds, we write\\lim_{x \\to a} f(x) = L.","type":"content","url":"/basic-differentiation#limits","position":9},{"hierarchy":{"lvl1":"Basic Differentiation","lvl2":"Continuity"},"type":"lvl2","url":"/basic-differentiation#continuity","position":10},{"hierarchy":{"lvl1":"Basic Differentiation","lvl2":"Continuity"},"content":"A function f is continuous at a if:\n\nf(a) is defined\n\n\\lim_{x \\to a} f(x) exists\n\n\\lim_{x \\to a} f(x) = f(a)","type":"content","url":"/basic-differentiation#continuity","position":11},{"hierarchy":{"lvl1":"Basic Differentiation","lvl2":"Derivative at a Point"},"type":"lvl2","url":"/basic-differentiation#derivative-at-a-point","position":12},{"hierarchy":{"lvl1":"Basic Differentiation","lvl2":"Derivative at a Point"},"content":"Let y = f(x). When x changes by \\Delta x, the change in y is\\frac{\\Delta y}{\\Delta x}\n=\n\\frac{f(x+\\Delta x) - f(x)}{\\Delta x}.","type":"content","url":"/basic-differentiation#derivative-at-a-point","position":13},{"hierarchy":{"lvl1":"Basic Differentiation","lvl2":"Derivative as a Function"},"type":"lvl2","url":"/basic-differentiation#derivative-as-a-function","position":14},{"hierarchy":{"lvl1":"Basic Differentiation","lvl2":"Derivative as a Function"},"content":"The derivative of f at x is defined asf'(x)\n=\n\\lim_{\\Delta x \\to 0}\n\\frac{f(x+\\Delta x) - f(x)}{\\Delta x}.\n\nIf the derivative exists for every x in the domain of f, then the derivative itself defines a new function, denoted f'(x).\n\nGeometrically, f'(x) is the slope of the tangent line to the graph of f at (x, f(x)).\n\nCommon notations include:\n\n f'(x) \n\n \\dfrac{dy}{dx} \n\n Df(x) \n\nNote that what we usually think of as a variable x is held constant while \\Delta x varies and converges to zero. It is useful to keep in mind that the derivative of a function f at x is the slope of a line tangent to the graph of the function f at the point (x, f(x)). It is crucial to understand the implications of the existence of the derivative at a point x. The function must be smooth—meaning it is both continuous and differentiable—at the point x. The tangent line provides a high-quality linear approximation to the graph of the function near x. In general, if we know that the function f is differentiable at a, then the tangent line approximation to f at a is:y = f(a) + f'(a)(x - a)\n\nwhere a, f(a), \\text{ and } f'(a) are constants, x is the independent variable, and y is the dependent variable. We will see this point again with Taylor series expansions. Many important concepts in economics—such as marginal cost or marginal utility—are based on this derivative function.","type":"content","url":"/basic-differentiation#derivative-as-a-function","position":15},{"hierarchy":{"lvl1":"Basic Differentiation","lvl2":"Second Derivative"},"type":"lvl2","url":"/basic-differentiation#second-derivative","position":16},{"hierarchy":{"lvl1":"Basic Differentiation","lvl2":"Second Derivative"},"content":"The second derivative is the derivative of the derivative and is written asf''(x)\n=\n\\frac{d^2 f(x)}{dx^2}.\n\nEconomic interpretation.If  \\ln p(t)  describes log prices over time, then:\n\nthe first derivative is inflation\n\nthe second derivative is the change in inflation","type":"content","url":"/basic-differentiation#second-derivative","position":17},{"hierarchy":{"lvl1":"Basic Differentiation","lvl2":"Basic Rules of Differentiation"},"type":"lvl2","url":"/basic-differentiation#basic-rules-of-differentiation","position":18},{"hierarchy":{"lvl1":"Basic Differentiation","lvl2":"Basic Rules of Differentiation"},"content":"Let y = f(x).","type":"content","url":"/basic-differentiation#basic-rules-of-differentiation","position":19},{"hierarchy":{"lvl1":"Basic Differentiation","lvl3":"Constant-function Rule","lvl2":"Basic Rules of Differentiation"},"type":"lvl3","url":"/basic-differentiation#constant-function-rule","position":20},{"hierarchy":{"lvl1":"Basic Differentiation","lvl3":"Constant-function Rule","lvl2":"Basic Rules of Differentiation"},"content":"The derivative of a contsant function y=f(x)=k is zero, for all values of x-it has zero slope!\\frac{d}{dx}(k) = 0.","type":"content","url":"/basic-differentiation#constant-function-rule","position":21},{"hierarchy":{"lvl1":"Basic Differentiation","lvl3":"Power-function Rule","lvl2":"Basic Rules of Differentiation"},"type":"lvl3","url":"/basic-differentiation#power-function-rule","position":22},{"hierarchy":{"lvl1":"Basic Differentiation","lvl3":"Power-function Rule","lvl2":"Basic Rules of Differentiation"},"content":"The derivative of a power function f(x) = x^n is:\\frac{d}{dx}(x^n) = n x^{n-1}.","type":"content","url":"/basic-differentiation#power-function-rule","position":23},{"hierarchy":{"lvl1":"Basic Differentiation","lvl3":"Generalized Power-function Rule","lvl2":"Basic Rules of Differentiation"},"type":"lvl3","url":"/basic-differentiation#generalized-power-function-rule","position":24},{"hierarchy":{"lvl1":"Basic Differentiation","lvl3":"Generalized Power-function Rule","lvl2":"Basic Rules of Differentiation"},"content":"When a multiplicaytive constant k appears in the power fuction, so that f(x) = kx^n, then:\\frac{d}{dx}(k x^n) = k n x^{n-1}.","type":"content","url":"/basic-differentiation#generalized-power-function-rule","position":25},{"hierarchy":{"lvl1":"Basic Differentiation","lvl3":"Logarithmic Rule","lvl2":"Basic Rules of Differentiation"},"type":"lvl3","url":"/basic-differentiation#logarithmic-rule","position":26},{"hierarchy":{"lvl1":"Basic Differentiation","lvl3":"Logarithmic Rule","lvl2":"Basic Rules of Differentiation"},"content":"The derivatice of the log-function f(x) = lnx is:\\frac{d}{dx}(\\ln x) = \\frac{1}{x}.","type":"content","url":"/basic-differentiation#logarithmic-rule","position":27},{"hierarchy":{"lvl1":"Basic Differentiation","lvl3":"Exponential Rule","lvl2":"Basic Rules of Differentiation"},"type":"lvl3","url":"/basic-differentiation#exponential-rule","position":28},{"hierarchy":{"lvl1":"Basic Differentiation","lvl3":"Exponential Rule","lvl2":"Basic Rules of Differentiation"},"content":"For some exponential function f(x) = a^x, where a is some constant, then:\\frac{d}{dx}(a^x) = a^x \\ln a.\n\nNote that a particular case of the above is\\frac{d}{d x} e^x = e^x\n\nWhile\\frac{d}{d x} \\ln x = \\frac{1}{x}\n\nNow, let’s consider some further useful rules of differentiation involving two or more functions of the same variable. Specifically, suppose f(x) and g(x) are two different functions of x and that f'(x) and g'(x) exist. That is, let f(x) and g(x) be differentiable, then:","type":"content","url":"/basic-differentiation#exponential-rule","position":29},{"hierarchy":{"lvl1":"Basic Differentiation","lvl3":"Sum-difference Rules","lvl2":"Basic Rules of Differentiation"},"type":"lvl3","url":"/basic-differentiation#sum-difference-rules","position":30},{"hierarchy":{"lvl1":"Basic Differentiation","lvl3":"Sum-difference Rules","lvl2":"Basic Rules of Differentiation"},"content":"The derivative of a sum (difference) of two functions is the sum (difference) of the derivatives of the two functions.\\frac{d}{dx}[f(x) \\pm g(x)] = f'(x) \\pm g'(x).\n\nTry these ^^\n\nDetermine the derivative of each of the functions below.\n\ny = 30x + 10\n\ny = 8x^2 - 6x + 12\n\ny = 6\n\ny = \\sqrt{3} - 2x^2\n\nAnswers\n\n1.\\; f'(x) = 30 \\quad\n2.\\; f'(x) = 16x - 6 \\quad\n3.\\; f'(x) = 0 \\quad\n4.\\; f'(x) = -4x","type":"content","url":"/basic-differentiation#sum-difference-rules","position":31},{"hierarchy":{"lvl1":"Basic Differentiation","lvl3":"Product Rule","lvl2":"Basic Rules of Differentiation"},"type":"lvl3","url":"/basic-differentiation#product-rule","position":32},{"hierarchy":{"lvl1":"Basic Differentiation","lvl3":"Product Rule","lvl2":"Basic Rules of Differentiation"},"content":"The derivative of the product of two (differentiable) functions is equal to the first function times the derivative of the second function plus the second function times the derivative of the first function.\\frac{d}{dx}[f(x)g(x)]\n=\nf'(x)g(x) + f(x)g'(x).","type":"content","url":"/basic-differentiation#product-rule","position":33},{"hierarchy":{"lvl1":"Basic Differentiation","lvl3":"Quotient Rule","lvl2":"Basic Rules of Differentiation"},"type":"lvl3","url":"/basic-differentiation#quotient-rule","position":34},{"hierarchy":{"lvl1":"Basic Differentiation","lvl3":"Quotient Rule","lvl2":"Basic Rules of Differentiation"},"content":"The derivative of the quotient of two (differentiable) functions, f(x)/g(x), is\\frac{d}{dx} \\left[ \\frac{f(x)}{g(x)} \\right] = \\frac{g(x)f'(x) - f(x)g'(x)}{[g(x)]^2}\n\nprovided that g(x) \\neq 0. Note that [g(x)]^2 = g^2(x).\n\nTry these ^^\n\nDifferentiate the following functions with respect to x.\n\ny=\\dfrac{3x(2x-1)}{5x-2}\n\ny=3x(4x-5)^2\n\ny=(5x-1)(3x+4)^3\n\ny=(3x-4)\\dfrac{5x+1}{2x+7}\n\ny=\\dfrac{(8x-5)^3}{7x+4}\n\ny=\\left(\\dfrac{3x+4}{2x+5}\\right)^2\n\nAnswers\n\n1.\\; y'=\\frac{30x^2-24x+6}{(5x-2)^2} \\quad\n2.\\; y'=144x^2-240x+75 \\quad\n3.\\;  y'=(45x-9)(3x+4)^2+5(3x+4)^3 \\quad\n4.\\;  y'=\\frac{30x^2+210x-111}{(2x+7)^2} \\quad\n5.\\;  y'=\\frac{(168x+96)(8x-5)^2-7(8x-5)^3}{(7x+4)^2} \\quad\n6.\\;  y'=\\frac{42x+56}{(2x+5)^3}\n\nUse the quotient rule to differentiate each function.\n\nf(x)=\\dfrac{2x+7}{x^2-1}\n\nf(x)=\\dfrac{bx^3+cx^2+x-4}{x}\n\nf(x)=\\dfrac{e^{2x}}{x^2}\n\nf(x)=\\dfrac{(3x+2)^2}{x}\n\nAnswers\n\n1.\\; f'(x)=\\frac{-2x^2-14x-2}{(x^2-1)^2} \\quad\n2.\\; f'(x)=\\frac{2bx^3+cx^2+4}{x^2} \\\\[6pt]\n3.\\; f'(x)=\\frac{2xe^{2x}-2e^{2x}}{x^3} \\quad\n4.\\; f'(x)=\\frac{9x^2-4}{x^2}\n\nSecond Derivatives\n\nFind the second derivative of each function.\n\ny = 9 - 3x + 7x^2 - x^3\n\ny = \\dfrac{4x+5}{x}\n\ny = \\ln(4x)\n\ny = x^2 e^x\n\ny = (x-6)^4\n\nAnswers\n\n1.\\; y'' = 14 - 6x \\quad\n2.\\; y'' = 10x^{-3} \\quad\n3.\\; y'' = -x^{-2} \\quad\n4.\\; y'' = 2e^x + 4xe^x + x^2 e^x \\quad\n5.\\; y'' = 12(x-6)^2","type":"content","url":"/basic-differentiation#quotient-rule","position":35},{"hierarchy":{"lvl1":"Basic Differentiation","lvl3":"Chain Rule","lvl2":"Basic Rules of Differentiation"},"type":"lvl3","url":"/basic-differentiation#chain-rule","position":36},{"hierarchy":{"lvl1":"Basic Differentiation","lvl3":"Chain Rule","lvl2":"Basic Rules of Differentiation"},"content":"If z = f(y) and y = g(x), then\\frac{dz}{dx}\n=\n\\frac{dz}{dy}\\frac{dy}{dx}.\n\nThe chain rule provides a convenient way to study how one variable (say, x) affects another variable (z) through its influence on some intermediate variable (y).\n\nSometimes, we can write for a composite function y = f(g(x)):\\frac{dy}{dx} = f'(g(x)) \\cdot g'(x)","type":"content","url":"/basic-differentiation#chain-rule","position":37},{"hierarchy":{"lvl1":"Basic Differentiation","lvl3":"Chain Rule for Exponential and Logarithmic Functions","lvl2":"Basic Rules of Differentiation"},"type":"lvl3","url":"/basic-differentiation#chain-rule-for-exponential-and-logarithmic-functions","position":38},{"hierarchy":{"lvl1":"Basic Differentiation","lvl3":"Chain Rule for Exponential and Logarithmic Functions","lvl2":"Basic Rules of Differentiation"},"content":"","type":"content","url":"/basic-differentiation#chain-rule-for-exponential-and-logarithmic-functions","position":39},{"hierarchy":{"lvl1":"Basic Differentiation","lvl4":"The general exponential function rule","lvl3":"Chain Rule for Exponential and Logarithmic Functions","lvl2":"Basic Rules of Differentiation"},"type":"lvl4","url":"/basic-differentiation#the-general-exponential-function-rule","position":40},{"hierarchy":{"lvl1":"Basic Differentiation","lvl4":"The general exponential function rule","lvl3":"Chain Rule for Exponential and Logarithmic Functions","lvl2":"Basic Rules of Differentiation"},"content":"\\frac{d}{d x} e^{g(x)} = e^{g(x)} g'(x)\n\nFor example:\\frac{d}{d x} e^{ax} = \\frac{d}{d (ax)} e^{ax} \\frac{d}{d x} (ax) = e^{ax} a = ae^{ax}\n\nIf we are using a base other than e:\\frac {d}{d x}(a^{g(x)}) = a^{g(x)} g'(x) \\ln a, \\text{ where } a > 0, a \\neq 0","type":"content","url":"/basic-differentiation#the-general-exponential-function-rule","position":41},{"hierarchy":{"lvl1":"Basic Differentiation","lvl4":"The general natural logarithmic function rule","lvl3":"Chain Rule for Exponential and Logarithmic Functions","lvl2":"Basic Rules of Differentiation"},"type":"lvl4","url":"/basic-differentiation#the-general-natural-logarithmic-function-rule","position":42},{"hierarchy":{"lvl1":"Basic Differentiation","lvl4":"The general natural logarithmic function rule","lvl3":"Chain Rule for Exponential and Logarithmic Functions","lvl2":"Basic Rules of Differentiation"},"content":"\\frac{d}{d x} \\ln(g(x)) = \\frac{g'(x)}{g(x)}\n\nInterestingly:\\frac{d}{d x} \\ln(ax) = \\frac{d}{d(ax)} \\ln(ax) \\frac{d}{d x}(ax) = \\frac{1}{ax} a = 1/x\n\nwhile\\frac{d}{d x} \\ln(x^2) = \\frac{d}{d(x^2)} \\ln(x^2) \\frac{d}{d x}(x^2) = \\frac{1}{x^2} 2x = 2/x\n\nTry these ^^\n\nExponential Functions\n\nUse the rules of differentiating exponential functions to find the derivative with respect to x of each of the following functions.\n\ny = x^2 e^{5x}\n\ny = \\dfrac{e^{5x}-1}{e^{5x}+1}\n\ny = a^{2x}\n\ny = a^{5x^2}\n\ny = 4^{2x+7}\n\ny = x^3 2^x\n\nAnswers\n\n1.\\; y' = x e^{5x}(5x+2) \\quad\n2.\\; y' = \\frac{10e^{5x}}{(e^{5x}+1)^2} \\quad\n3.\\; y' = 2a^{2x}\\ln a \\quad\n4.\\; y' = 10x\\,a^{5x^2}\\ln a \\quad\n5.\\; y' = 2\\ln(4)\\,4^{2x+7} \\quad\n6.\\; y' = x^2 2^x(x\\ln 2 + 3)\n\nLogarithmic Functions\n\nUse the rules of differentiating logarithms to find the derivative of each function.\n\nf(x)=x^{-4}+\\ln(ax)\n\nf(x)=4x^3\\ln x^2\n\nf(x)=\\ln x-\\ln(1+x)\n\nf(x)=\\ln\\!\\left(\\dfrac{2x^2}{5x}\\right)\n\nf(x)=\\log_2(6x)\n\nf(x)=\\log_4(9x^3)\n\nAnswers\n\n1.\\; f'(x)=-4x^{-5}+x^{-1} \\quad\n2.\\; f'(x)=8x^2+24x^2\\ln x \\quad\n3.\\; f'(x)=\\frac{1}{x(1+x)} \\quad\n4.\\; f'(x)=\\frac{1}{x} \\quad\n5.\\; f'(x)=\\frac{1}{x\\ln 2} \\quad\n6.\\; f'(x)=\\frac{3}{x\\ln 4}\n\nChain Rule\n\nUse the chain rule to find the derivative, f'(x), of the following:\n\nf(x) = (x + 1)^3 + (x^2 - 2x)^2 - 5\n\nf(x) = (2x + 4)^{99}\n\nf(x) = (5x^2 + 10x + 3)^{20}\n\nf(x) = (e^x)^{ab}\n\nf(x) = (e^{x^a})^{b}\n\nf(x) = (e^{a + bx + cx^2})^{10}\n\nAnswers\n\n1.\\; f'(x)=4x^3-9x^2+14x+3 \\quad\n2.\\; f'(x)=198(2x+4)^{98} \\quad\n3.\\; f'(x)=(200x+200)(5x^2+10x+3)^{19} \\\\[6pt]\n4.\\; f'(x)=ab\\,e^{abx} \\quad\n5.\\; f'(x)=ab\\,x^{a-1}e^{bx^a} \\quad\n6.\\; f'(x)=10(b+2cx)(e^{a+bx+cx^2})^{10}\n\nDerivatives of Exponential and Logarithmic Functions\n\n1. Exponential Rule with Base a\n\nProblem: Find the derivative of f(x) = 5^{x^3 + 2x}.\n\nSolution:\nUsing the rule \\frac{d}{d x}(a^{g(x)}) = a^{g(x)} g'(x) \\ln a:\n\nLet a = 5\n\nLet g(x) = x^3 + 2x, so g'(x) = 3x^2 + 2f'(x) = 5^{x^3 + 2x} \\cdot (3x^2 + 2) \\cdot \\ln 5\n\nGeneral Natural Logarithmic Rule\n\nProblem: Find the derivative of f(x) = \\ln(\\sin(x)).\n\nSolution:\nUsing the rule \\frac{d}{d x} \\ln(g(x)) = \\frac{g'(x)}{g(x)}:\n\nLet g(x) = \\sin(x), so g'(x) = \\cos(x)f'(x) = \\frac{\\cos(x)}{\\sin(x)} = \\cot(x)\n\nLogarithm with Base b\n\nProblem: Find the derivative of f(x) = \\log_{10}(x^2 + 1).\n\nSolution:\nUsing the rule \\frac{d}{d x} \\log_b g(x) = \\frac{g'(x)}{g(x) \\ln b}:\n\nLet b = 10\n\nLet g(x) = x^2 + 1, so g'(x) = 2xf'(x) = \\frac{2x}{(x^2 + 1) \\ln(10)}\n\nComparison of \\ln(ax) vs \\ln(x^n)\n\nProblem: Differentiate y = \\ln(7x) and y = \\ln(x^7) to see the difference.\n\nCase A: For \\ln(7x), the constant a=7 cancels out:\\frac{dy}{dx} = \\frac{7}{7x} = \\frac{1}{x}\n\nCase B: For \\ln(x^7), the power n=7 remains in the numerator:\\frac{dy}{dx} = \\frac{7x^6}{x^7} = \\frac{7}{x}\n\nNote also when considered base other than e. Because\\log_b(x) = \\frac{\\ln(x)}{\\ln(b)}\n\nwe have\\frac{d}{d x} \\log_b(x) = \\frac{1}{x} \\frac{1}{\\ln(b)}\n\nOr more generally:\\begin{aligned} \n\\frac{d}{d x} \\log_b g(x) &= \\frac{g'(x)}{g(x)} \\frac{1}{\\ln b}, \\text{ where } b > 0, b \\neq 1 \\\\ \n&= \\frac{g'(x)}{g(x)} \\log_b e \n\\end{aligned}\n\nNote that \\log_b e = \\displaystyle \\frac{1}{\\ln b}.","type":"content","url":"/basic-differentiation#the-general-natural-logarithmic-function-rule","position":43},{"hierarchy":{"lvl1":"Basic Differentiation","lvl2":"The Differential"},"type":"lvl2","url":"/basic-differentiation#the-differential","position":44},{"hierarchy":{"lvl1":"Basic Differentiation","lvl2":"The Differential"},"content":"Define dx as an arbitrary change in xfro its initial value x_0 and dy as teh resulting change in y along the tangent line from the initial value of the function y_0 = f(x_0).\n\nThe differential of y=f(x_0) evaluated at x_0 isdy = f'(x_0)\\, dx.\n\nThis represents the change in y along the tangent line at x_0. Graphically, we have:\n\nTry these ^^\n\nDifferentials\n\nFind the differential dy for a given change in x, dx, for each function.\n\ny = 7x^2 - 3x + 5\n\ny = 10x - \\dfrac14 x^2\n\ny = -x^2\n\ny = x^3 + 3x - 6\n\nAnswers\n\n1.\\; dy = (14x - 3)\\,dx \\quad\n2.\\; dy = \\left(10 - \\tfrac12 x\\right)dx \\quad\n3.\\; dy = (-2x)\\,dx \\quad\n4.\\; dy = (3x^2 + 3)\\,dx","type":"content","url":"/basic-differentiation#the-differential","position":45},{"hierarchy":{"lvl1":"Basic Differentiation","lvl2":"Taylor Series (Preview)"},"type":"lvl2","url":"/basic-differentiation#taylor-series-preview","position":46},{"hierarchy":{"lvl1":"Basic Differentiation","lvl2":"Taylor Series (Preview)"},"content":"A smooth complex function z(x) can be approximated around x=a byf(x)\n=\nz(a)\n+ z'(a)(x-a)\n+ \\frac{1}{2}z''(a)(x-a)^2\n+ \\frac{1}{6}f'''(a)(x-a)^3\n+ \\cdots\n\nThis idea underlies many approximation methods in economics.\n\nThe image provided illustrates a function z(x) being approximated by three different Taylor polynomials (or Taylor series expansions) centered around the point x=a.\n\nThe simplest approximation perhaps would simply be g(x) = a. This constant-valued function does not work well, especially if we move away from the point a.\n\nA better approximation would be a linear function of the form h(x) = z(a) + b(x-a), where b is some slope. But what would be a good value of b? We saw above that the differential is an equation for the tangent line (or slope) at the point x = a. So, we could argue that the best linear approximation to the function around this point would beh(x) = z(a) + z'(a)(x-a)\n\nwhere z'(a) is the derivative of the function evaluated at x=a.\n\nBut why stop here? We could improve on this. A better approximation could allow for some curvature. The general form would then be, say,  f(x) = z(a) + z'(a) .(x-a) + c.(x-a)^2. Again, we ask, “What would be the best value for c?” The rate of change of the slope of the quadratic approximation should be equal to the rate of change of change of the function at the a. And since the second derivative of z(x) is 2c, then for f''(x) to equal z''(x) at x = a, we need c = 1/2 z''(a). Hence the quadratic approximation to the function aound x = a is:f(x) = z(a) + z'(a)(x-a) + \\frac{1}{2}f''(a)(x-a)^2\n\nExteding the above argument for cubic and higher-degree approximations, we could find the nth-degree approximation to the function z(x), which we could call m(x), around the point x = a ism(x)\n=\n\\frac{z(a)}{0!}\n+ \\frac{z'(a)}{1!}(x-a)\n+ \\frac{z''(a)}{2!}(x-a)^2\n+ \\cdots\n+ \\frac{f^{(n)}(a)}{n!}(x-a)^n\n\nwhere f^{(n)}(a) is the n the derivative of z(x) evaluated at x = a. The function m(x) above is called the n-th degree Taylor expansion series of z(x) evaluated at x=a.\n\nTo sum, z(x) is the original function being approximated (the solid curve). g(x) represents a constant function. h(x) represents the first-order Taylor polynomial, i.e. a straight line that has the same value and slope as z(x) at x=a (or a tangent to z(x) at x=a). The formula for f(x) is f(x) = z(a) + z^{\\prime }(a)(x-a) + \\frac{1}{2} z''(a)(x-a)^2 representing a second-order (quadratic) polynomial-the dashed curve, which matches better the function’s value, slope, and concavity (curvature) at x=a. It is a better approximation of z(x) near x=a than the linear approximation h(x) and of course the constant function g(a). The graph demonstrates that as more terms are included in the Taylor polynomial, the approximation of the original function becomes more accurate over a larger range around the center point x=a.\n\nExample\n\nFor example, consider the functiony = e^{x/2} - e^{-x/2},\n\nexpanded around the point x = 2.\n\nLinear approximation\n\nThe linear approximation to this function ish(x)\n= \\bigl(e^{1} - e^{-1}\\bigr)\n+ \\left(\\tfrac12\\bigl(e^{1} + e^{-1}\\bigr)\\right)(x - 2).\n\nQuadratic approximation\n\nThe quadratic approximation isj(x)\n= \\bigl(e^{1} - e^{-1}\\bigr)\n+ \\left(\\tfrac12\\bigl(e^{1} + e^{-1}\\bigr)\\right)(x - 2)\n+ \\left(\\tfrac18\\bigl(e^{1} - e^{-1}\\bigr)\\right)(x - 2)^2.","type":"content","url":"/basic-differentiation#taylor-series-preview","position":47},{"hierarchy":{"lvl1":"Basic Differentiation","lvl2":"Implicit Differentiation"},"type":"lvl2","url":"/basic-differentiation#implicit-differentiation","position":48},{"hierarchy":{"lvl1":"Basic Differentiation","lvl2":"Implicit Differentiation"},"content":"Let’s consider a very simple function,xy = 7.\n\nHere, possible solutions include (x,y)=(1,7), (7,1), and so on.If we want to find the slope of this function, we can differentiate it.\n\nFinding y'\n\nTo find y', we proceed as follows.\n\n(a) We make the main assumption that y is a function of x, i.e. y=f(x).We then differentiate both sides of the equation with respect to x.\n\nHence, we obtain\\frac{d}{dx}[x f(x)] = 0.\n\nUsing the product rule, this gives1\\cdot f(x) + x f'(x) = 0.\n\nEquivalently,y + x y' = 0.\n\n(b) Solving the resulting equation for y' givesy' = -\\frac{y}{x}.\n\nSo, if we substitute, for example, x=1 and y=5, we obtain the slope of the function at that point:y' = -5.","type":"content","url":"/basic-differentiation#implicit-differentiation","position":49},{"hierarchy":{"lvl1":"Basic Differentiation","lvl2":"Inverse Function Rule for Implicit Functions"},"type":"lvl2","url":"/basic-differentiation#inverse-function-rule-for-implicit-functions","position":50},{"hierarchy":{"lvl1":"Basic Differentiation","lvl2":"Inverse Function Rule for Implicit Functions"},"content":"We can show that\\frac{dy}{dx} = -\\frac{f_x}{f_y}.\n\nThat is, if we have an implicit function written asf(x,y) = 0,\n\nthen the derivative of y with respect to x can be obtained by:\n\ndifferentiating f with respect to x to obtain f_x,\n\ndifferentiating f with respect to y to obtain f_y,\n\ntaking the ratio -\\dfrac{f_x}{f_y}.\n\nThis gives the derivative of the implicit function y with respect to x.\n\nIt often feels like magic — but it is simply a consequence of the chain rule.\n\nWhy this works\n\nSince f(x,y)=0 holds along the curve, differentiating both sides with respect to x\nand solving for y' naturally leads to the ratio -\\dfrac{f_x}{f_y}.","type":"content","url":"/basic-differentiation#inverse-function-rule-for-implicit-functions","position":51},{"hierarchy":{"lvl1":"Basic Differentiation","lvl2":"Some Uses of Differentiation in Economics"},"type":"lvl2","url":"/basic-differentiation#some-uses-of-differentiation-in-economics","position":52},{"hierarchy":{"lvl1":"Basic Differentiation","lvl2":"Some Uses of Differentiation in Economics"},"content":"Some common applications of differentiation include:\n\nIncreasing and decreasing functions\n\nRelative extrema (maximum or minimum)\n\nInflection points\n\nOptimization of functions\n\netc.","type":"content","url":"/basic-differentiation#some-uses-of-differentiation-in-economics","position":53},{"hierarchy":{"lvl1":"Basic Differentiation","lvl3":"A CES production function example","lvl2":"Some Uses of Differentiation in Economics"},"type":"lvl3","url":"/basic-differentiation#a-ces-production-function-example","position":54},{"hierarchy":{"lvl1":"Basic Differentiation","lvl3":"A CES production function example","lvl2":"Some Uses of Differentiation in Economics"},"content":"Given the CES production functionQ = A\\bigl[\\alpha K^{-\\beta} + (1-\\alpha)L^{-\\beta}\\bigr]^{-1/\\beta},\n\nwe can show that the elasticity of substitution is constant, as follows.\n\nFirst-order conditions\n\nThe first-order conditions require that\\frac{\\partial Q / \\partial L}{\\partial Q / \\partial K}\n=\n\\frac{P_L}{P_K}.\n\nUsing the generalized power function rule, we take the first-order partial derivatives.\n\nFor labor,\\frac{\\partial Q}{\\partial L}\n=\n-\\frac{1}{\\beta}\nA\\bigl[\\alpha K^{-\\beta} + (1-\\alpha)L^{-\\beta}\\bigr]^{-(1/\\beta+1)}\n(-\\beta)(1-\\alpha)L^{-\\beta-1}.\n:::{math}\n:enumerated: false\n\nCanceling the $-\\beta$ terms, rearranging $(1-\\alpha)$, and adding the exponents\n$-(1/\\beta)-1$, we obtain\n\n:::{math}\n:enumerated: false\n\\frac{\\partial Q}{\\partial L}\n=\n(1-\\alpha)A\n\\bigl[\\alpha K^{-\\beta} + (1-\\alpha)L^{-\\beta}\\bigr]^{-(1+\\beta)/\\beta}\nL^{-(1+\\beta)}.\n\nSubstituting A^{1+\\beta}/A^{\\beta}=A, we can write\\frac{\\partial Q}{\\partial L}\n=\n(1-\\alpha)\\frac{A^{1+\\beta}}{A^\\beta}\n\\bigl[\\alpha K^{-\\beta} + (1-\\alpha)L^{-\\beta}\\bigr]^{-(1+\\beta)/\\beta}\nL^{-(1+\\beta)}.\n\nFrom the CES production function,A^{1+\\beta}\n\\bigl[\\alpha K^{-\\beta} + (1-\\alpha)L^{-\\beta}\\bigr]^{-(1+\\beta)/\\beta}\n=\nQ^{1+\\beta},\n\nandL^{-(1+\\beta)} = \\frac{1}{L^{1+\\beta}}.\n\nThus,\\frac{\\partial Q}{\\partial L}\n=\n\\frac{1-\\alpha}{A^\\beta}\n\\left(\\frac{Q}{L}\\right)^{1+\\beta}.\n\nThe marginal product of capital*\n\nSimilarly,\\frac{\\partial Q}{\\partial K}\n=\n\\frac{\\alpha}{A^\\beta}\n\\left(\\frac{Q}{K}\\right)^{1+\\beta}.\n\nDividing the two equations and equating the result to P_L/P_K (from the FOC)\nleads to the cancellation of A^\\beta and Q:\\frac{1-\\alpha}{\\alpha}\n\\left(\\frac{K}{L}\\right)^{1+\\beta}\n=\n\\frac{P_L}{P_K}.\n\nRearranging,\\left(\\frac{K}{L}\\right)^{1+\\beta}\n=\n\\frac{\\alpha}{1-\\alpha}\n\\frac{P_L}{P_K},\n\nand therefore,\\frac{K}{L}\n=\n\\left(\\frac{\\alpha}{1-\\alpha}\\right)^{1/(1+\\beta)}\n\\left(\\frac{P_L}{P_K}\\right)^{1/(1+\\beta)}.\n\nElasticity of substitution\n\nSince \\alpha and \\beta are constants, we can treat K/L as a function of\nP_L/P_K.\n\nLeth = \\left(\\frac{\\alpha}{1-\\alpha}\\right)^{1/(1+\\beta)}.\n\nThen\\frac{K}{L} = h\\left(\\frac{P_L}{P_K}\\right)^{1/(1+\\beta)}.\n\nThe marginal function is\\frac{d(K/L)}{d(P_L/P_K)}\n=\n\\frac{h}{1+\\beta}\n\\left(\\frac{P_L}{P_K}\\right)^{1/(1+\\beta)-1}.\n\nThe average function is\\frac{K/L}{P_L/P_K}\n=\nh\\left(\\frac{P_L}{P_K}\\right)^{1/(1+\\beta)-1}.\n\nDividing the marginal function by the average function, we obtain the elasticity\nof substitution:\\text{MRS}\n=\n\\frac{d(K/L)}{d(P_L/P_K)} \\Big/ \\frac{K/L}{P_L/P_K}\n=\n\\frac{1}{1+\\beta}.\n\nThis is constant, hence the CES production function exhibits constant elasticity of substitution.\n\nInterpretation\n\nIf -1 < \\beta < 0, then MRS > 1.\n\nIf \\beta = 0, then MRS = 1 (Cobb–Douglas case).\n\nIf 0 < \\beta < \\infty, then MRS < 1.\n\nIntuition: elasticity of substitution\n\nThe elasticity of substitution measures how easily a firm can substitute labor for capital when their relative prices change. In a CES production function, this elasticity is constant: it does not depend on the levels of K, L, or output. When the elasticity is high, firms can adjust input combinations easily in response to wage or rental-rate changes; when it is low, substitution is difficult and input proportions are relatively rigid. The parameter \\beta governs this flexibility: values of \\beta close to zero imply unit elasticity (the Cobb–Douglas case), while larger values of \\beta imply more limited substitution.\n\nA function need not be defined at the point a in order to have a limit as x \\to a.\nFor example,f(x) = \\frac{x^2 - 1}{x - 1}\n\nis not defined at x = 1, but\\lim_{x \\to 1} f(x) = 2.\n\nThe two functions discussed above are not continuous.\nThe first is not continuous because f(a) is not defined.\nThe second is not continuous because f does not converge to a limit as x \\to a.\nFor example, iff(x) =\n\\begin{cases}\n-1, & x < 0, \\\\\n1, & x > 0,\n\\end{cases}\n\nthen f has no limit as x \\to 0, since the right-hand limit equals 1 while the left-hand limit equals -1.","type":"content","url":"/basic-differentiation#a-ces-production-function-example","position":55},{"hierarchy":{"lvl1":"Math Notes"},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":"Math Notes"},"content":"These notes build intuition for core tools used in economics, data science, and optimization.\n\nTopics (in progress)\n\nLinear Algebra\n\nCalculus\n\nDifference & Differential Equations\n\nNote\n\nThis is a living document. I will revise and expand as the course evolves.","type":"content","url":"/","position":1},{"hierarchy":{"lvl1":"Linear Algebra: Basics"},"type":"lvl1","url":"/linear-algebra-basic","position":0},{"hierarchy":{"lvl1":"Linear Algebra: Basics"},"content":"","type":"content","url":"/linear-algebra-basic","position":1},{"hierarchy":{"lvl1":"Linear Algebra: Basics","lvl2":"Vector"},"type":"lvl2","url":"/linear-algebra-basic#vector","position":2},{"hierarchy":{"lvl1":"Linear Algebra: Basics","lvl2":"Vector"},"content":"A vector is a mathematical quantity that has both magnitude (or size) and direction.\n\nGeometrically, a vector is represented as a directed line segment, like an arrow, where the length signifies the magnitude and the arrowhead indicates the direction.\n\nMore conveniently, you may write a vector as \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} or simply \\vec{v}. Can you tell what the magnitude (or length or norm) of the above vector is?\n\nAnother way to represent a vector is by using basis vectors, i.e. \\hat{\\imath} and \\hat{\\jmath}, where \\hat{\\imath} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} and \\hat{\\jmath} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}. Hence the vector \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} can be written as \\vec{v} = 1 \\hat{\\imath} + 2 \\hat{\\jmath}.\n\nMake sure you know how to add, subtract vectors, and also multiply/scale vectors by some scalar.\n\nTry these ^^\n\n(1) Express the above two vectors as column vectors and in \\hat{\\imath}, \\hat{\\jmath} notation.\n\n(2) Find also:\n\n(a) \\vec{v} + \\vec{w}    (b) \\vec{v} - \\vec{w}   \n(c) \\vec{v} + 2\\vec{w}    (d) \\vec{v} - 2\\vec{w}\n\n(3) Consider the following vectors:\\vec{u} = \\begin{bmatrix} -1 \\\\ 2 \\\\ 3 \\end{bmatrix}, \\quad\n\\vec{v} = \\begin{bmatrix} -1 \\\\ 2 \\\\ 3 \\end{bmatrix}, \\quad\n\\vec{w} = \\begin{bmatrix} 4 \\\\ 5 \\\\ -2 \\end{bmatrix}\n\nFind:\n\n(a) \\vec{u} + \\vec{v}     (b) -\\vec{u} + 2\\vec{v} - \\vec{w}(c) \\vec{u} \\cdot \\vec{v} (i.e. the dot product)    (d) \\vec{u}^T \\vec{v}(e) Which of the above vectors are orthogonal?(f) Express each of the vectors in \\hat{\\imath}, \\hat{\\jmath}, \\hat{k} notation.","type":"content","url":"/linear-algebra-basic#vector","position":3},{"hierarchy":{"lvl1":"Linear Algebra: Basics","lvl2":"Matrix Basics"},"type":"lvl2","url":"/linear-algebra-basic#matrix-basics","position":4},{"hierarchy":{"lvl1":"Linear Algebra: Basics","lvl2":"Matrix Basics"},"content":"We have already introduced the basis vectors \\hat{\\imath} and \\hat{\\jmath}, which we can put into a matrix. That is, the \\hat{\\imath} and \\hat{\\jmath} vectors placed side-by-side in a 2 \\times 2 matrix:\\begin{bmatrix} \n1 & 0 \\\\ \n0 & 1 \n\\end{bmatrix}\n\nNow suppose we want to rotate \\vec{v} = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} anticlockwise by 90°. Where will it go?\n\nWe can rotate the 2-D space, such that \\hat{\\imath} will go to \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} and \\hat{\\jmath} will go to \\begin{bmatrix} -1 \\\\ 0 \\end{bmatrix}. Combining these into a matrix gives:\\begin{bmatrix} 0 & -1 \\\\ 1 & 0 \\end{bmatrix}\n\nThis can easily be done by pre-multiplying the vector by the transformation matrix, as follows:\\begin{bmatrix} 0 & -1 \\\\ 1 & 0 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} = \\begin{bmatrix} -2 \\\\ 1 \\end{bmatrix}\n\nBasically, a matrix can be viewed as a way to transform/change a vector!\n\nMatrix and vector multiplication\n\nThe proper way to multiply a matrix and a vector is:1 \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} + 2 \\begin{bmatrix} -1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} -2 \\\\ 1 \\end{bmatrix}\n\nMatrix addition and subtraction: If \\mathbf{A} = (a_{ij}) and \\mathbf{B} = (b_{ij}) are two m \\times n matrices of same dimension, then \\mathbf{A} + \\mathbf{B} is defined as (a_{ij} + b_{ij}). That is, we add element by element the two matrices.Clearly \\mathbf{A} + \\mathbf{B} = \\mathbf{B} + \\mathbf{A}. The rule applies to matrix subtraction.\n\nTranspose of a matrix: If \\mathbf{A} is an m \\times n matrix, then \\mathbf{A}' is the n \\times m matrix whose rows are the columns of \\mathbf{A}. So \\mathbf{A}' = (a_{ji}). For example:\\begin{bmatrix}\na & b \\\\\nc & d\n\\end{bmatrix} = \\begin{bmatrix}\na & c \\\\\nb & d\n\\end{bmatrix}\n\nNote: (\\mathbf{A} + \\mathbf{B})' = \\mathbf{A}' + \\mathbf{B}', however (\\mathbf{A}\\mathbf{B})' = \\mathbf{B}'\\mathbf{A}'.\n\nNull matrix: Has all elements as 0. Clearly \\mathbf{A} + \\mathbf{0}_{m,n} = \\mathbf{0}_{m,n} + \\mathbf{A} = \\mathbf{A} for all m \\times n matrices.\n\nScalar multiplication: If \\mathbf{A} = (a_{ij}) then for any constant k, define k\\mathbf{A} = (k a_{ij}). That is, multiply each element in \\mathbf{A} by k.\n\nMatrix multiplication: Say \\mathbf{A} is m \\times n, and \\mathbf{B} is n \\times p, then the m \\times p matrix \\mathbf{A}\\mathbf{B} is the product of \\mathbf{A} and \\mathbf{B}. For example:\\begin{bmatrix}\na & b \\\\\nc & d\n\\end{bmatrix}\n\\begin{bmatrix}\ne & f \\\\\ng & h\n\\end{bmatrix}\n= \\begin{bmatrix}\nae + bg & af + bh \\\\\nce + dg & cf + dh\n\\end{bmatrix}\n\nand the matrices \\mathbf{A} and \\mathbf{B} are conformable.\n\nFro example, given the matrices:A = \\begin{bmatrix} 1 & 2 \\\\ 3 & -4 \\end{bmatrix}, \\quad B = \\begin{bmatrix} 5 & 0 \\\\ -6 & 7 \\end{bmatrix}\n\nTo multiply A and B, we apply the row-by-column rule:\\begin{align*}\n\\begin{bmatrix} 1 & 2 \\\\ 3 & -4 \\end{bmatrix}\n\\begin{bmatrix} 5 & 0 \\\\ -6 & 7 \\end{bmatrix}\n&= \\begin{bmatrix}\n(1 \\times 5) + (2 \\times -6) & (1 \\times 0) + (2 \\times 7) \\\\\n(3 \\times 5) + (-4 \\times -6) & (3 \\times 0) + (-4 \\times 7)\n\\end{bmatrix} \\\\\n\n&= \\begin{bmatrix}\n5 - 12 & 0 + 14 \\\\\n15 + 24 & 0 - 28\n\\end{bmatrix} = \\begin{bmatrix}\n-7 & 14 \\\\\n39 & -28\n\\end{bmatrix}\n\\end{align*}\n\nLong story!!\n\nMatrix Multiplication: The Column-Wise (Basis Vector) Method\n\nInstead of dot products, we can view AB as taking the columns of A (the transformed \\hat{\\imath} and \\hat{\\jmath}) and scaling them by the components in each column of B.\n\nFrom the sam eexample above, let the columns of A be:\\vec{a}_1 = \\begin{bmatrix} 1 \\\\ 3 \\end{bmatrix}, \\quad \\vec{a}_2 = \\begin{bmatrix} 2 \\\\ -4 \\end{bmatrix}\n\nFinding Column 1 of the Result:\n\nWe use the first column of B \\begin{bmatrix} 5 \\\\ -6 \\end{bmatrix} as scalars:5\\begin{bmatrix} 1 \\\\ 3 \\end{bmatrix} + (-6)\\begin{bmatrix} 2 \\\\ -4 \\end{bmatrix} = \\begin{bmatrix} 5 \\\\ 15 \\end{bmatrix} + \\begin{bmatrix} -12 \\\\ 24 \\end{bmatrix} = \\begin{bmatrix} -7 \\\\ 39 \\end{bmatrix}\n\nFinding Column 2 of the Result:\n\nWe use the second column of B \\begin{bmatrix} 0 \\\\ 7 \\end{bmatrix} as scalars:0\\begin{bmatrix} 1 \\\\ 3 \\end{bmatrix} + 7\\begin{bmatrix} 2 \\\\ -4 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} + \\begin{bmatrix} 14 \\\\ -28 \\end{bmatrix} = \\begin{bmatrix} 14 \\\\ -28 \\end{bmatrix}\n\nFinal Result:AB = \\begin{bmatrix} -7 & 14 \\\\ 39 & -28 \\end{bmatrix}\n\nas before!\n\nTry these ^^\n\n(1) For the following matrices:A = \\begin{bmatrix} 1 & 2 \\\\ 3 & -4 \\end{bmatrix}, \\quad\nB = \\begin{bmatrix} 5 & 0 \\\\ -6 & 7 \\end{bmatrix},C = \\begin{bmatrix} 1 & -3 & 4 \\\\ 2 & 6 & -5 \\end{bmatrix}, \\quad\nD = \\begin{bmatrix} 3 & 7 & -1 \\\\ 4 & -8 & 9 \\end{bmatrix}.\n\nFind:\n\n(a) 5A - 2B    (b)  2A + 3B    (c)  2C - 3D(d)  AB and (AB)C    (e) BC and A(BC) [Note that (AB)C = A(BC)](f) A^2 and A^3    (g) AD and BD(h) C^T D [Note that we cannot get CD. Why?](i) A'    (j) B'    (k) A' B'    (l) (AB)' [Note that A' B' \\neq (AB)']\n\nDiagonal matrix: A square n \\times n matrix \\mathbf{A} is diagonal if all entries off the ‘main diagonal’ are zero, i.e.\\mathbf{A} = \\begin{bmatrix}\na_{11} & 0 & \\cdots & 0 \\\\\n0 & a_{22} & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & a_{nn}\n\\end{bmatrix}\n\nNote: The elements in the diagonal of \\mathbf{A} are the same as those in \\mathbf{A}'.\n\nTrace of a square matrix: If \\mathbf{A} is a square matrix, the trace of \\mathbf{A}, denoted \\operatorname{tr}(\\mathbf{A}), is the sum of the elements on the main/leading diagonal of \\mathbf{A}.\n\nIdentity matrix: Denoted by \\mathbf{I}_n, the n \\times n diagonal matrix with a_{ii} = 1 for all i. That is:\\mathbf{I}_n = \\begin{bmatrix}\n1 & 0 & \\cdots & 0 \\\\\n0 & 1 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & 1\n\\end{bmatrix}\n\nSymmetric matrix: A square matrix \\mathbf{A} is symmetric if \\mathbf{A} = \\mathbf{A}'. The identity matrix and the square null matrix are symmetric.\n\nIdempotent matrix: A square matrix is said to be idempotent if \\mathbf{A}^n = \\cdots = \\mathbf{A}^2 = \\mathbf{A}. Below is an example (try squaring the matrix and see what you get).\\begin{bmatrix}\n1 & 0 \\\\\n0 & 0\n\\end{bmatrix}\n\nSingular Matrix: A matrix that is linearly dependent (in the columns or rows) is singular and has determinant of zero. Note that any two (or three) vectors \\mathbf{x} and \\mathbf{y} (and \\mathbf{z}) are said to be linearly dependent iff one can be written as a scalar multiple of the other. Two vectors \\mathbf{x} \\neq \\mathbf{0} and \\mathbf{y} \\neq \\mathbf{0} are said to be linearly independent iff the ONLY solution to a\\mathbf{x} + b\\mathbf{y} = \\mathbf{0} is a = 0 AND b = 0. And \\mathbf{x} and \\mathbf{y} are said to be orthogonal.\n\nTry these ^^\n\nFind:(a) \\quad \n\\begin{bmatrix} 1 & 1 \\\\ 0 & 1 \\end{bmatrix}^2 \\quad \\text{and} \\quad \\begin{bmatrix} 1 & 1 \\\\ 0 & 1 \\end{bmatrix}^3(b) \\quad \n\\begin{bmatrix} 0 & 0 \\\\ 0 & 1 \\end{bmatrix}^2 \\quad \\text{and} \\quad \\begin{bmatrix} 0 & 0 \\\\ 0 & 1 \\end{bmatrix}^3","type":"content","url":"/linear-algebra-basic#matrix-basics","position":5},{"hierarchy":{"lvl1":"Linear Algebra: Basics","lvl3":"Some Notes on Matrices","lvl2":"Matrix Basics"},"type":"lvl3","url":"/linear-algebra-basic#some-notes-on-matrices","position":6},{"hierarchy":{"lvl1":"Linear Algebra: Basics","lvl3":"Some Notes on Matrices","lvl2":"Matrix Basics"},"content":"Usually \\mathbf{A}\\mathbf{B} \\neq \\mathbf{B}\\mathbf{A}. For example, try \\mathbf{A} = \\begin{bmatrix}2 & 0 \\\\ 3 & 1\\end{bmatrix} and \\mathbf{B} = \\begin{bmatrix}0 & 1 \\\\ 0 & 0\\end{bmatrix}.\n\nWe can have \\mathbf{A}\\mathbf{B} = \\mathbf{0} even if \\mathbf{A} or \\mathbf{B} are not zero. For example, try \\mathbf{A} = \\begin{bmatrix}6 & -12 \\\\ -3 & 6\\end{bmatrix} and \\mathbf{B} = \\begin{bmatrix}12 & 6 \\\\ 6 & 3\\end{bmatrix}.\n\nSay, \\mathbf{A} = \\begin{bmatrix}4 & 8 \\\\ 1 & 2\\end{bmatrix}, \\mathbf{B} = \\begin{bmatrix}2 & 1 \\\\ 2 & 2\\end{bmatrix}, and \\mathbf{C} = \\begin{bmatrix}-2 & 1 \\\\ 4 & 2\\end{bmatrix}. We find that \\mathbf{A}\\mathbf{B} = \\mathbf{A}\\mathbf{C} even though \\mathbf{B} \\neq \\mathbf{C}.\n\nSay \\mathbf{A} and \\mathbf{B} are singular matrices. Then \\mathbf{A}\\mathbf{B} will not be zero, although the product will be a singular matrix.","type":"content","url":"/linear-algebra-basic#some-notes-on-matrices","position":7},{"hierarchy":{"lvl1":"Linear Algebra: Basics","lvl2":"Systems of Equations in Matrix Form"},"type":"lvl2","url":"/linear-algebra-basic#systems-of-equations-in-matrix-form","position":8},{"hierarchy":{"lvl1":"Linear Algebra: Basics","lvl2":"Systems of Equations in Matrix Form"},"content":"One of the uses of linear algebra in economics is to represent and then solve systems of equations. For instance, consider the system of two equations:2x_1 + x_2 = 5 \\\\\n-x_1 + x_2 = 2\n\nHere, we can define:\\mathbf{A} = \\begin{bmatrix} 2 & 1 \\\\ -1 & 1 \\end{bmatrix}, \\quad \\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}, \\quad \\mathbf{b} = \\begin{bmatrix} 5 \\\\ 2 \\end{bmatrix}\n\nThen we see that:\\mathbf{A}\\mathbf{x} = \\begin{bmatrix} 2 & 1 \\\\ -1 & 1 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} = \\begin{bmatrix} 2x_1 + x_2 \\\\ -x_1 + x_2 \\end{bmatrix}\n\nThe original system is in fact equivalent to the matrix form:\\mathbf{A}\\mathbf{x} = \\mathbf{b}","type":"content","url":"/linear-algebra-basic#systems-of-equations-in-matrix-form","position":9},{"hierarchy":{"lvl1":"Linear Algebra: Basics","lvl2":"Matrix Row Operations"},"type":"lvl2","url":"/linear-algebra-basic#matrix-row-operations","position":10},{"hierarchy":{"lvl1":"Linear Algebra: Basics","lvl2":"Matrix Row Operations"},"content":"Matrix row operations, also known as elementary row operations, are three basic actions performed on a matrix: swapping two rows, multiplying a row by a non-zero constant, and adding a multiple of one row to another row.\n\nInterchanging two rows (Row Swapping): You can swap the positions of any two rows in a matrix. As we will see later, this operation is useful for changing the order of equations in a system without altering the solution.\n\nMultiplying a row by a non-zero constant (Scalar Multiplication): You can multiply every element in a specific row by any non-zero number. This is equivalent to multiplying both sides of an equation by a constant.\n\nAdding a multiple of one row to another row (Row Addition): You can multiply one row by a constant and then add the result to another row. The original row and the row being multiplied remain unchanged. This operation is often the most powerful for simplifying systems, as it corresponds to adding a modified version of one equation to another.\n\nDefinition: REF (Row Echelon Form)\n\nA matrix is in Row Echelon Form if...\n\nEvery non-zero row begins with a leading one.\n\nA leading one in a lower row is further to the right.\n\nZero rows are at the bottom of the matrix.\n\nNote: In some books, leading by one is not required.\n\nDefinition: RREF Reduced Row Echelon Form\n\nA matrix is in Reduced Row Echelon Form if...\n\nEvery non-zero row begins with a leading one.\n\nA leading one in a lower row is further to the right.\n\nZero rows are at the bottom of the matrix.\n\nEvery column with a leading one has zeros elsewhere.\n\nWhile there can exist several row echelon forms for a matrix, there is only one (a unique) reduced row echelon form.\n\nTry these ^^\n\nFor the following matrices:A = \\begin{bmatrix} 2 & 0 & 0 \\\\ 0 & 3 & 0 \\\\ 0 & 0 & 5 \\end{bmatrix}, \\quad \nB = \\begin{bmatrix} 7 & 0 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & -4 \\end{bmatrix},C = \\begin{bmatrix} 2 & 1 & 2 \\\\ 0 & 3 & 0 \\\\ 0 & 0 & 5 \\end{bmatrix}, \\quad \nD = \\begin{bmatrix} 7 & 0 & 0 \\\\ -2 & 0 & 0 \\\\ 3 & 1 & -4 \\end{bmatrix}.\n\nFind:\n\n(a) Is the matrix A in REF or RREF? What are its pivots?(b) How about matrix B and C? Explain.(c) Change all matrices A to D to RREF (if they aren’t in RREF yet) and determine their rank.\n\nWe can solve system of equation by row operations.\n\nTo solve for x_1 and x_2, we can use an augmented matrix and perform elementary row operations—swapping rows, scalar multiplication, and row addition—to reach Reduced Row Echelon Form (RREF).\n\n1. Set up the augmented matrix:\\begin{bmatrix} 2 & 1 & | & 5 \\\\ -1 & 1 & | & 2 \\end{bmatrix}\n\n2. Create a leading one in Row 1:\n\nWe can swap R_1 and R_2:\\begin{bmatrix} -1 & 1 & | & 2 \\\\ 2 & 1 & | & 5 \\end{bmatrix}\n\nThen multiply R_1 by -1 to get a leading one (R_1 \\leftarrow -1 \\cdot R_1):\\begin{bmatrix} 1 & -1 & | & -2 \\\\ 2 & 1 & | & 5 \\end{bmatrix}\n\n3. Create a zero below the leading one:\n\nAdd -2 times R_1 to R_2 (R_2 \\leftarrow R_2 - 2R_1):\\begin{bmatrix} 1 & -1 & | & -2 \\\\ 0 & 3 & | & 9 \\end{bmatrix}\n\n4. Create a leading one in Row 2:\n\nMultiply R_2 by 1/3 (R_2 \\leftarrow \\frac{1}{3}R_2):\\begin{bmatrix} 1 & -1 & | & -2 \\\\ 0 & 1 & | & 3 \\end{bmatrix}\n\n5. Create a zero above the leading one (RREF):\n\nTo reach reduced row echelon form, every column with a leading one must have zeros elsewhere. Add R_2 to R_1 (R_1 \\leftarrow R_1 + R_2):\\begin{bmatrix} 1 & 0 & | & 1 \\\\ 0 & 1 & | & 3 \\end{bmatrix}\n\nBasically, a matrix can be viewed as a way to transform or change a vector to solve these systems!\n\nFinal Result\n\nThe system provides the unique solution:x_1 = 1, \\quad x_2 = 3\n\nBasically, a matrix can be viewed as a way to transform or change a vector to solve these systems!\n\nNote: We need not reach RREF, and could stop at Step 4 (or even Step 3). At Step 4, the last row tells us that x_2 = 3. We can then use back-substitution into the first row (x_1 - x_2 = -2). HEnce x_1 - 3 = -2 \\implies x_1 = 1.\n\nThis confirms the same result without performing the final row addition.","type":"content","url":"/linear-algebra-basic#matrix-row-operations","position":11},{"hierarchy":{"lvl1":"Linear Algebra: Basics","lvl2":"The Determinant of a Matrix"},"type":"lvl2","url":"/linear-algebra-basic#the-determinant-of-a-matrix","position":12},{"hierarchy":{"lvl1":"Linear Algebra: Basics","lvl2":"The Determinant of a Matrix"},"content":"The determinant is a scalar value uniquely associated with a square non-singular matrix, and is usually denoted as |\\mathbf{A}|. The question of whether or not a matrix is nonsingular and therefore invertible is linked to the value of its determinant.\n\nWe can write a generic 2 \\times 2 matrix as:\\mathbf{A} = \\begin{bmatrix} a_{11} & a_{12} \\\\ a_{21} & a_{22} \\end{bmatrix}\n\nand its determinant is defined as:|\\mathbf{A}| = a_{11}a_{22} - a_{12}a_{21}\n\nFor a 3 \\times 3 matrix, say:\\mathbf{A} = \\begin{bmatrix} a & b & c \\\\ d & e & f \\\\ g & h & i \\end{bmatrix}\n\nits determinant is:|\\mathbf{A}| = a(ei - fh) - b(di - fg) + c(dh - ef)\n\nNote the sign in front of b is begative, because we imppose (multiply by) the sign matrix \\begin{bmatrix} + & - & + \\\\ - & + & - \\\\ + & - & + \\end{bmatrix}.\n\nVisually:\n\nThe ‘yellow’ bits called minors are ‘smaller’ determinants, now 2 by 2 and easier to handle. Note that the elements of the minor come from remaining elements after deleting the rows and columns of the corresponding elements in the selected row (or column).\n\nLet’s take a numerical example.\n\nTo find the determinant of matrix A using Cofactor Expansion along the first row (technically we could pick any row to work with) involves multiplying each element of the first row by its corresponding 2 \\times 2 minor, following the sign pattern: (+ , - , +) for the first row.\n\nGiven:A = \\begin{bmatrix} 2 & -3 & 1 \\\\ 1 & -1 & 2 \\\\ 3 & 1 & -1 \\end{bmatrix}\n\nThe formula for expansion along the first row is:\\det(A) = a(M_{11}) - b(M_{12}) + c(M_{13})\n\n1. First Element (a = 2):+2 \\cdot \\begin{vmatrix} -1 & 2 \\\\ 1 & -1 \\end{vmatrix} = 2 \\cdot [(-1)(-1) - (2)(1)] = 2(1 - 2) = -2\n\n2. Second Element (b = -3):\nNote the negative sign from the checkerboard pattern:-(-3) \\cdot \\begin{vmatrix} 1 & 2 \\\\ 3 & -1 \\end{vmatrix} = 3 \\cdot [(1)(-1) - (2)(3)] = 3(-1 - 6) = -21\n\n3. Third Element (c = 1):+1 \\cdot \\begin{vmatrix} 1 & -1 \\\\ 3 & 1 \\end{vmatrix} = 1 \\cdot [(1)(1) - (-1)(3)] = 1(1 + 3) = 4\n\nFinal Result\n\nSumming the results from each step gives \\det(A) = -2 - 21 + 4 = -19.","type":"content","url":"/linear-algebra-basic#the-determinant-of-a-matrix","position":13},{"hierarchy":{"lvl1":"Linear Algebra: Basics","lvl2":"Properties of Determinants"},"type":"lvl2","url":"/linear-algebra-basic#properties-of-determinants","position":14},{"hierarchy":{"lvl1":"Linear Algebra: Basics","lvl2":"Properties of Determinants"},"content":"Property 1: |\\mathbf{A}| = |\\mathbf{A}'| and |\\mathbf{A}| \\cdot |\\mathbf{B}| = |\\mathbf{B}| \\cdot |\\mathbf{A}|.\n\nProperty 2: Interchanging any two rows or columns will affect the sign of the determinant, but not its absolute value.\n\nProperty 3: Multiplying a single row or column by a scalar will cause the value of the determinant to be multiplied by the scalar.\n\nProperty 4: Addition or subtraction of a nonzero multiple of any row or column to or from another row or column does not change the value of the determinant.\n\nProperty 5: The determinant of a triangular matrix is equal to the product of the elements along the principal diagonal.\n\nProperty 6: If any of the rows or columns equal zero, the determinant is also zero.\n\nProperty 7: If two rows or columns are identical or proportional, i.e. linearly dependent, then the determinant is zero.\n\nNote: Properties 2,3 and 4  are standard row operations already discussed above.","type":"content","url":"/linear-algebra-basic#properties-of-determinants","position":15},{"hierarchy":{"lvl1":"Linear Algebra: Basics","lvl2":"Matrix Inversion"},"type":"lvl2","url":"/linear-algebra-basic#matrix-inversion","position":16},{"hierarchy":{"lvl1":"Linear Algebra: Basics","lvl2":"Matrix Inversion"},"content":"Consider,the following.\\mathbf{A} = \\begin{bmatrix} 1 & -2 \\\\ 3 & 4 \\end{bmatrix}\n\nThen pre-multiplying by:\\begin{bmatrix} 4 & 2 \\\\ -3 & 1 \\end{bmatrix}\n\nGives:\\begin{bmatrix} 4 & 2 \\\\ -3 & 1 \\end{bmatrix} \\begin{bmatrix} 1 & -2 \\\\ 3 & 4 \\end{bmatrix} = \\begin{bmatrix} 10 & 0 \\\\ 0 & 10 \\end{bmatrix} = 10 \\mathbf{I}\n\nWhere does the 10 in the matrix after the = sign come from? It is the determinant of \\mathbf{A}.\n\nThen, (pre-)multiplying both sides of the equation by \\frac{1}{10} gives:\\frac{1}{10} \\begin{bmatrix} 4 & 2 \\\\ -3 & 1 \\end{bmatrix} \\begin{bmatrix} 1 & -2 \\\\ 3 & 4 \\end{bmatrix} = \\mathbf{I}\n\nThe matrix \\frac{1}{10} \\begin{bmatrix} 4 & 2 \\\\ -3 & 1 \\end{bmatrix} is in fact the inverse of \\mathbf{A}.\n\nHence we have the relationship \\mathbf{A}^{-1} \\mathbf{A} = \\mathbf{I}. In fact, \\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{I}.","type":"content","url":"/linear-algebra-basic#matrix-inversion","position":17},{"hierarchy":{"lvl1":"Linear Algebra: Basics","lvl2":"Finding the Inverse of a Matrix by Row Operations"},"type":"lvl2","url":"/linear-algebra-basic#finding-the-inverse-of-a-matrix-by-row-operations","position":18},{"hierarchy":{"lvl1":"Linear Algebra: Basics","lvl2":"Finding the Inverse of a Matrix by Row Operations"},"content":"We can find the inverse of a matrix using row operations or Gauss-Jordan Elimination method. The objective is to transform the matrix A into the Identity matrix I. By applying the same sequence of elementary row operations to an Identity matrix simultaneously, that Identity matrix transforms into A^{-1}.\n\nGiven the matrix A from the example aboveA = \\begin{bmatrix} 1 & -2 \\\\ 3 & 4 \\end{bmatrix}\n\nWe set up an augmented matrix [A | I] by placing the 2 \\times 2 Identity matrix to the right of A:[A | I] = \\begin{bmatrix} 1 & -2 & | & 1 & 0 \\\\ 3 & 4 & | & 0 & 1 \\end{bmatrix}\n\nStep-by-Step Row Operations\n\n1. Create a zero below the first leading one:\n\nSubtract 3 times the first row from the second row (R_2 \\leftarrow R_2 - 3R_1):\\begin{bmatrix} 1 & -2 & | & 1 & 0 \\\\ 0 & 10 & | & -3 & 1 \\end{bmatrix}\n\n2. Create a leading one in the second row:\n\nMultiply the second row by 1/10 (R_2 \\leftarrow \\frac{1}{10}R_2):\\begin{bmatrix} 1 & -2 & | & 1 & 0 \\\\ 0 & 1 & | & -3/10 & 1/10 \\end{bmatrix}\n\n3. Create a zero above the second leading one:\n\nTo reach Reduced Row Echelon Form (RREF), we add 2 times the second row to the first row (R_1 \\leftarrow R_1 + 2R_2):\\begin{bmatrix} 1 & 0 & | & 1 + 2(-3/10) & 0 + 2(1/10) \\\\ 0 & 1 & | & -3/10 & 1/10 \\end{bmatrix}\n\nSimplifying the arithmetic in the first row:\\begin{bmatrix} 1 & 0 & | & 4/10 & 2/10 \\\\ 0 & 1 & | & -3/10 & 1/10 \\end{bmatrix}\n\nFinal Result\n\nThe left side of the augmented matrix has been transformed into the Identity matrix. Therefore, the right side is now the inverse, A^{-1}:A^{-1} = \\begin{bmatrix} 0.4 & 0.2 \\\\ -0.3 & 0.1 \\end{bmatrix}\n\nVerification\n\nA matrix multiplied by its inverse must result in the Identity matrix:\\begin{bmatrix} 1 & -2 \\\\ 3 & 4 \\end{bmatrix}\n\\begin{bmatrix} 0.4 & 0.2 \\\\ -0.3 & 0.1 \\end{bmatrix}\n= \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}","type":"content","url":"/linear-algebra-basic#finding-the-inverse-of-a-matrix-by-row-operations","position":19},{"hierarchy":{"lvl1":"Linear Algebra: Basics","lvl2":"Properties of Inverse"},"type":"lvl2","url":"/linear-algebra-basic#properties-of-inverse","position":20},{"hierarchy":{"lvl1":"Linear Algebra: Basics","lvl2":"Properties of Inverse"},"content":"Here are some properties of the inverse of a matrix worth knowing:\n\nProperty 1: For any nonsingular matrix \\mathbf{A}, (\\mathbf{A}^{-1})^{-1} = \\mathbf{A}.\n\nProperty 2: The determinant of the inverse of a matrix is equal to the reciprocal of the determinant of the matrix. That is |\\mathbf{A}^{-1}| = \\frac{1}{|\\mathbf{A}|}.\n\nProperty 3: The inverse of matrix \\mathbf{A} is unique.\n\nProperty 4: For any nonsingular matrix \\mathbf{A}, the inverse of the transpose of a matrix is equal to the transpose of the inverse of the matrix. (\\mathbf{A}')^{-1} = (\\mathbf{A}^{-1})'.\n\nProperty 5: If \\mathbf{A} and \\mathbf{B} are nonsingular and of the same dimension, then \\mathbf{A}\\mathbf{B} is also nonsingular.\n\nProperty 6: The inverse of the product of two matrices is equal to the product of their inverses in reverse order. (\\mathbf{A}\\mathbf{B})^{-1} = \\mathbf{B}^{-1}\\mathbf{A}^{-1}.","type":"content","url":"/linear-algebra-basic#properties-of-inverse","position":21},{"hierarchy":{"lvl1":"Linear Algebra: Basics","lvl2":"Finding the Inverse of a Matrix (Standard Approach)"},"type":"lvl2","url":"/linear-algebra-basic#finding-the-inverse-of-a-matrix-standard-approach","position":22},{"hierarchy":{"lvl1":"Linear Algebra: Basics","lvl2":"Finding the Inverse of a Matrix (Standard Approach)"},"content":"The inverse of a matrix is defined as:\\mathbf{A}^{-1} = \\frac{1}{|\\mathbf{A}|} \\operatorname{adj}(\\mathbf{A})\n\nLet’s say we have the following linear system:2x_1 - 3x_2 + x_3 = -1 \\\\\nx_1 - x_2 + 2x_3 = -3 \\\\\n3x_1 + x_2 - x_3 = 9\n\nWriting the above system in the form \\mathbf{A}\\mathbf{x} = \\mathbf{b}, where:\\mathbf{A} = \\begin{bmatrix} 2 & -3 & 1 \\\\ 1 & -1 & 2 \\\\ 3 & 1 & -1 \\end{bmatrix}, \\quad \\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix}, \\quad \\mathbf{b} = \\begin{bmatrix} -1 \\\\ -3 \\\\ 9 \\end{bmatrix}\n\nWe can therefore solve the system by \\mathbf{x} = \\mathbf{A}^{-1}\\mathbf{b}, which means that we need the inverse of \\mathbf{A}.\n\nStep 1. The minor |M_{ij}| is the determinant of the submatrix formed by deleting the i-th row and j-th column of the original matrix. So we have:\\begin{bmatrix}\n-1 & -7 & 4 \\\\\n2 & -5 & 11 \\\\\n-5 & 3 & 1\n\\end{bmatrix}\n\nStep 2: The cofactor C_{ij} is a minor with the prescribed sign that follows the rule:C_{ij} = (-1)^{i+j} |M_{ij}|\n\nHence, we have:\\begin{bmatrix}\n-1 & 7 & 4 \\\\\n-2 & -5 & -11 \\\\\n-5 & -3 & 1\n\\end{bmatrix}\n\nStep 3: An adjoint matrix is simply the transpose of a cofactor matrix. Continuing the example above, we have:\\begin{bmatrix}\n-1 & -2 & -5 \\\\\n7 & -5 & -3 \\\\\n4 & -11 & 1\n\\end{bmatrix}\n\nSince |\\mathbf{A}| = -19, we then have:\\mathbf{A}^{-1} = - \\frac{1}{19} \\begin{bmatrix}\n-1 & -2 & -5 \\\\\n7 & -5 & -3 \\\\\n4 & -11 & 1\n\\end{bmatrix}\n\nand\\mathbf{x} = \\mathbf{A}^{-1}\\mathbf{b} = - \\frac{1}{19} \\begin{bmatrix}\n-1 & -2 & -5 \\\\\n7 & -5 & -3 \\\\\n4 & -11 & 1\n\\end{bmatrix} \\begin{bmatrix} -1 \\\\ -3 \\\\ 9 \\end{bmatrix}\n\nwhich gives x_1 = 2, x_2 = 1 and x_3 = -2.\n\nTry these ^^\n\nGiven:A = \\begin{bmatrix} 3 & -4 \\\\ 1 & 2 \\end{bmatrix}, \\quad \nB = \\begin{bmatrix} -2 & 1 \\\\ -4 & 2 \\end{bmatrix},C = \\begin{bmatrix} 2 & 0 & 0 \\\\ 0 & 3 & 0 \\\\ 0 & 0 & 5 \\end{bmatrix}, \\quad \nD = \\begin{bmatrix} 7 & 0 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & -4 \\end{bmatrix},E = \\begin{bmatrix} 2 & 1 & 2 \\\\ 0 & 3 & 0 \\\\ 0 & 0 & 5 \\end{bmatrix}, \\quad \nF = \\begin{bmatrix} 7 & 0 & 0 \\\\ -2 & 0 & 0 \\\\ 3 & 1 & -4 \\end{bmatrix}.\n\nFind:\n\n(a) AB, A^2, B^2    (b) CD, C^2, D^2    (c) EF, E^2, F^2(d) AB, CE and DF    (d) Determinant of all matrices A to F(e) Inverse of all matrices A to F","type":"content","url":"/linear-algebra-basic#finding-the-inverse-of-a-matrix-standard-approach","position":23},{"hierarchy":{"lvl1":"Linear Algebra: Basics","lvl2":"Cramer’s Rule"},"type":"lvl2","url":"/linear-algebra-basic#cramers-rule","position":24},{"hierarchy":{"lvl1":"Linear Algebra: Basics","lvl2":"Cramer’s Rule"},"content":"Cramer came up with a simplified method for solving a system of linear equations through the use of determinants.\n\nBasically, given \\mathbf{A}\\mathbf{x} = \\mathbf{b}, we have:\\mathbf{A} = \\begin{bmatrix} a_{11} & a_{12} & a_{13} \\\\ a_{21} & a_{22} & a_{23} \\\\ a_{31} & a_{32} & a_{33} \\end{bmatrix}, \\quad \\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix}, \\quad \\mathbf{b} = \\begin{bmatrix} b_1 \\\\ b_2 \\\\ b_3 \\end{bmatrix}\n\nDefine:\\Delta = |\\mathbf{A}|\n\nand\\Delta_1 = \\begin{vmatrix} b_1 & a_{12} & a_{13} \\\\ b_2 & a_{22} & a_{23} \\\\ b_3 & a_{32} & a_{33} \\end{vmatrix}, \\quad \\Delta_2 = \\begin{vmatrix} a_{11} & b_1 & a_{13} \\\\ a_{21} & b_2 & a_{23} \\\\ a_{31} & b_3 & a_{33} \\end{vmatrix}, \\quad \\Delta_3 = \\begin{vmatrix} a_{11} & a_{12} & b_1 \\\\ a_{21} & a_{22} & b_2 \\\\ a_{31} & a_{32} & b_3 \\end{vmatrix}\n\nwhere the columns in the \\mathbf{A} matrix are replaced one by one by the \\mathbf{b} vector as shown above.\n\nThen:x_i = \\frac{\\Delta_i}{\\Delta}\n\nSo, for the previous example:\\mathbf{A} = \\begin{bmatrix} 2 & -3 & 1 \\\\ 1 & -1 & 2 \\\\ 3 & 1 & -1 \\end{bmatrix} \\quad \\text{and} \\quad \\det(A) = -19, \\quad \\mathbf{b} = \\begin{bmatrix} -1 \\\\ -3 \\\\ 9 \\end{bmatrix}\n\nIt follows that\\Delta_1 = \\begin{vmatrix} -1 & -3 & 1 \\\\ -3 & -1 & 2 \\\\ 9 & 1 & -1 \\end{vmatrix}, \\quad \\Delta_2 = \\begin{vmatrix} 2 & -1 & 1 \\\\ 1 & -3 & 2 \\\\ 3 & 9 & -1 \\end{vmatrix}, \\quad \\Delta_3 = \\begin{vmatrix} 2 & -3 & -1 \\\\ 1 & -1 & -3 \\\\ 3 & 1 & 9 \\end{vmatrix}\n\nwhere \\Delta_1 = -38, \\Delta_2 = -19, and \\Delta_3 = 38.\n\nSo dividing through, you can easily verify that you get the soluitons \\{2, 1, -2\\} as before!","type":"content","url":"/linear-algebra-basic#cramers-rule","position":25},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices"},"type":"lvl1","url":"/linear-algebra-special","position":0},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices"},"content":"Let’s look at some special matrices and determinants that are useful in economic analysis.","type":"content","url":"/linear-algebra-special","position":1},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices","lvl2":"The Jacobian"},"type":"lvl2","url":"/linear-algebra-special#the-jacobian","position":2},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices","lvl2":"The Jacobian"},"content":"The Jacobian (named after German mathematician Karl Gustav Jacobi, 1804–1851) is generally used in conjunction with partial derivatives to provide an easy test for the existence of functional dependence (linear and nonlinear). A Jacobian determinant |\\mathbf{J}| is composed of all the first‑order partial derivatives of a system of equations, arranged in order sequence. Giveny_{1} = f_{1}(x_{1},x_{2},x_{3}) \\\\\ny_{2} = f_{2}(x_{1},x_{2},x_{3}) \\\\\ny_{3} = f_{3}(x_{1},x_{2},x_{3})\n\nThen|\\mathbf{J}| = \n\\begin{vmatrix}\n\\frac{\\partial y_{1}}{\\partial x_{1}} & \\frac{\\partial y_{1}}{\\partial x_{2}} & \\frac{\\partial y_{1}}{\\partial x_{3}} \\\\[4pt]\n\\frac{\\partial y_{2}}{\\partial x_{1}} & \\frac{\\partial y_{2}}{\\partial x_{2}} & \\frac{\\partial y_{2}}{\\partial x_{3}} \\\\[4pt]\n\\frac{\\partial y_{3}}{\\partial x_{1}} & \\frac{\\partial y_{3}}{\\partial x_{2}} & \\frac{\\partial y_{3}}{\\partial x_{3}}\n\\end{vmatrix}\n\nFor example, say you havey_{1} = 5x_{1} + 3x_{2} \\\\\ny_{2} = 25x_{1}^{2} + 30x_{1}x_{2} + 9x_{2}^{2}\n\nThen taking the first‑order partials gives{\\partial y_1}{\\partial x_1} = 5,\\qquad \n\\frac{\\partial y_1}{\\partial x_2} = 3,\\qquad \n\\frac{\\partial y_2}{\\partial x_1} = 50x_1 + 30x_2,\\qquad \n\\frac{\\partial y_2}{\\partial x_2} = 30x_1 + 18x_2\n\nAnd the Jacobian is|\\mathbf{J}| = \n\\begin{vmatrix}\n5 & 3 \\\\\n50x_1 + 30x_2 & 30x_1 + 18x_2\n\\end{vmatrix}\n\nThe determinant of the Jacobian matrix is|\\mathbf{J}| = 5(30x_1 + 18x_2) - 3(50x_1 + 30x_2) = 0\n\nSince |\\mathbf{J}| = 0, there is functional dependence between the equations.\n\nNote: (5x_{1} + 3x_{2})^{2} = 25x_{1}^{2} + 30x_{1}x_{2} + 9x_{2}^{2}.","type":"content","url":"/linear-algebra-special#the-jacobian","position":3},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices","lvl2":"The Hessian"},"type":"lvl2","url":"/linear-algebra-special#the-hessian","position":4},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices","lvl2":"The Hessian"},"content":"Given that the first‑order conditions z_{x} = z_{y} = 0 are met, a sufficient condition for a multivariate function z = f(x,y) to be an optimum is\n\nz_{xx}, z_{yy} > 0 for a minimum; z_{xx}, z_{yy} < 0 for a maximum.\n\nz_{xx} z_{yy} > (z_{xy})^{2}.\n\nA convenient test for this second‑order condition is provided by the Hessian.A Hessian |\\mathbf{H}| is a determinant composed of all the second‑order partial derivatives, with the second‑order direct partials on the principal diagonal and the second‑order cross partials off the principal diagonal. That is,|\\mathbf{H}| = \n\\begin{vmatrix}\nz_{xx} & z_{xy} \\\\\nz_{yx} & z_{yy}\n\\end{vmatrix}\n\nwhere (by Young’s Theorem) z_{xy} = z_{yx}.\n\nIf the first element on the principal diagonal, a.k.a. the first principal minor, |H_{1}| = z_{xx} is positive, and the second principal minor also|H_{2}| = \n\\begin{vmatrix}\nz_{xx} & z_{xy} \\\\\nz_{yx} & z_{yy}\n\\end{vmatrix}\n= z_{xx} z_{yy} - (z_{xy})^{2} > 0,\n\nthen the second‑order conditions for a minimum are set. That is, when |H_{1}| > 0 and |H_{2}| > 0, the Hessian |\\mathbf{H}| is said to be positive definite. And a positive definite Hessian fulfills the second‑order conditions for a minimum.\n\nIf however the first principal minor |H_{1}| < 0, while the second principal minor remains positive, then the second‑order conditions for a maximum are met. That is, when |H_{1}| < 0 and |H_{2}| > 0, the Hessian |\\mathbf{H}| is said to be negative definite and fulfills the second‑order conditions for a maximum.\n\nLet’s take a numerical example:\n\nGivenz = f(x,y) = 2x^{2} - xy + 2y^{2} - 5x - 6y + 20,\n\nthenz_x = 4x - y - 5,\\quad z_y = -x + 4y - 6,\\quad z_{xx} = 4,\\quad z_{yy} = 4,\\quad z_{xy} = z_{yx} = -1.\n\nThe Hessian is therefore|\\mathbf{H}| = \n\\begin{vmatrix}\n4 & -1 \\\\\n-1 & 4\n\\end{vmatrix}\n\nWe have |H_{1}| = 4 > 0 and |H_{2}| = (4)(4) - (-1)^2 = 16 - 1 = 15 > 0, i.e. both princpal minnores are positive and hence the Hessian is said to be positive definite and the function z is characterized by a minimum at the critical values (can you find these?).","type":"content","url":"/linear-algebra-special#the-hessian","position":5},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices","lvl2":"The Discriminant"},"type":"lvl2","url":"/linear-algebra-special#the-discriminant","position":6},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices","lvl2":"The Discriminant"},"content":"Determinants can be used to test for positive and negative definiteness of any quadratic form. The determinant of a quadratic form is called a discriminant |\\mathbf{D}|. Given the quadratic formz = a x^{2} + b x y + c y^{2},\n\nthe determinant is formed by placing the coefficients of the squared terms on the principal diagonal and dividing the coefficients of the non‑squared terms equally between the off‑diagonal positions. Hence, we have|\\mathbf{D}| = \n\\begin{vmatrix}\na & b/2 \\\\\nb/2 & c\n\\end{vmatrix}\n\nWe then evaluate the principal minors like we did for the Hessian test, where|D_{1}| = a \\quad \\text{and} \\quad |D_{2}| = \n\\begin{vmatrix}\na & b/2 \\\\\nb/2 & c\n\\end{vmatrix}\n= a c - \\frac{b^{2}}{4}.\n\nIf |D_{1}|, |D_{2}| > 0, |\\mathbf{D}| is positive definite and z is positive for all nonzero values of x and y. If |D_{1}| < 0 and |D_{2}| > 0, |\\mathbf{D}| is negative definite and z is negative for all nonzero values of x and y. If |D_{2}| \\neq 0, z is not sign definite and z may assume both positive and negative values.\n\nLet’s take an example to test for sign definiteness of the following quadratic formz = 2x^{2} + 5xy + 8y^{2}.\n\nWe can easily form the discriminant|\\mathbf{D}| = \n\\begin{vmatrix}\n2 & 2.5 \\\\\n2.5 & 8\n\\end{vmatrix}\n\nThen evaluating the principal minors gives|D_{1}| = 2 > 0, \\qquad |D_{2}| = \n\\begin{vmatrix}\n2 & 2.5 \\\\\n2.5 & 8\n\\end{vmatrix}\n= 16 - 6.25 = 9.75 > 0.\n\nHence, z is positive definite, meaning that it will be greater than zero for all nonzero values of x and y.\n\nTry these ^^\n\nBy inspecting its Discriminant, can you tell whether the function below is positive or negative for all nonzero x and y?\n\n(1) f(x,y) = 2x^2 + 5xy + 8y^2(2) f(x,y) = -3x + 4xy - 4y^2(3) f(x,y,z) = 5x^2 - 6xy + 3y^2 - 2yz + 8z^2 - 3xy","type":"content","url":"/linear-algebra-special#the-discriminant","position":7},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices","lvl2":"The Quadratic Form"},"type":"lvl2","url":"/linear-algebra-special#the-quadratic-form","position":8},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices","lvl2":"The Quadratic Form"},"content":"A quadratic form is defined as a polynomial expression in which each component term has a uniform degree.\n\nHere are some examples:6x^{2} - 2xy + 3y^{2} \\quad \\text{is a quadratic form in 2 variables.} \\\\\nx^{2} + 2xy + 4xz + 2yz + y^{2} + z^{2} \\quad \\text{is a quadratic form in 3 variables.}\n\nIt would be useful to determine the sign definiteness so we can make statements concerning the optimum value of the function as to whether it is a minimum or a maximum.\n\nMore generally, a quadratic form in n variables (x_{1},x_{2},\\ldots ,x_{n}) can be written as \\mathbf{x}^{\\prime}\\mathbf{A}\\mathbf{x} where\n\n\\mathbf{x}^{\\prime} is a row vector [x_{1},x_{2},\\dots,x_{n}] and\\mathbf{A} is an n\\times n matrix of scalar elements.\n\nTo see this more clearly, letA = \\begin{bmatrix} 2 & 1 \\\\ 1 & 3 \\end{bmatrix}\n\nThen the quadratic form is:\\begin{aligned}\nQ(x, y) &= \\begin{bmatrix} x & y \\end{bmatrix} \\begin{bmatrix} 2 & 1 \\\\ 1 & 3 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\end{bmatrix} \\\\\n&= \\begin{bmatrix} 2x + y & x + 3y \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\end{bmatrix} \\\\\n&= (2x + y)x + (x + 3y)y \\\\\n&= 2x^2 + 2xy + 3y^2\n\\end{aligned}\n\nFurthermore, completing the square gives:\\begin{aligned}\nQ(x, y) &= 2x^2 + 2xy + 3y^2 = 2(x^2 + xy) + 3y^2 \\\\\n&= 2\\left(x + \\frac{y}{2}\\right)^2 - \\frac{1}{2}y^2 + 3y^2 = 2\\left(x + \\frac{y}{2}\\right)^2 + \\frac{5}{2}y^2\n\\end{aligned}\n\nSince both squared terms have positive coefficients, Q(x, y) > 0 for all (x, y) \\neq (0, 0), the quadratic form is positive definite.\n\nSign Definiteness of Quadratic Form\n\nA quadratic form is said to be:\n\nPositive definite if \\mathbf{x}^{\\prime}\\mathbf{A}\\mathbf{x} > 0 for all \\mathbf{x} \\neq \\mathbf{0}.\n\nPositive semi‑definite if \\mathbf{x}^{\\prime}\\mathbf{A}\\mathbf{x} \\geq 0 and \\exists \\mathbf{x} \\neq \\mathbf{0} such that \\mathbf{x}^{\\prime}\\mathbf{A}\\mathbf{x} = 0.\n\nNegative definite if \\mathbf{x}^{\\prime}\\mathbf{A}\\mathbf{x} < 0 for all \\mathbf{x} \\neq \\mathbf{0}.\n\nNegative semi‑definite if \\mathbf{x}^{\\prime}\\mathbf{A}\\mathbf{x} \\leq 0 and \\exists \\mathbf{x} \\neq \\mathbf{0} such that \\mathbf{x}^{\\prime}\\mathbf{A}\\mathbf{x} = 0.\n\nSign indefinite if it takes both positive and negative values.\n\nQuadratic forms are used in many areas such as:\n\nOptimization: To determine if a critical point is a minimum, maximum, or saddle point (via the Hessian matrix).\n\nGeometry: Describing conic sections (ellipses, hyperbolas) and quadric surfaces.\n\nPhysics: Representing energy functions (e.g., kinetic energy in mechanics).\n\nQuadratic forms are particularly useful in determining the concavity or convexity of a differentiable function.\n\nFor a function y = f(x_{1},x_{2},\\dots,x_{n}), we can form the Hessian consisting of second‑order partial derivatives and construct a quadratic form as \\mathbf{x}^{\\prime}\\mathbf{H}\\mathbf{x}. Then\n\na) The function y is strictly convex if the quadratic form is positive (implying that the Hessian is positive definite, i.e., the determinants of the principal minors are all positive).b) The function y is strictly concave if the quadratic form is negative (that is, the Hessian is negative definite, i.e. the determinants of the principal minors alternate in sign).\n\nLet’s take an example to see this more clearly.\n\nSay we havez = x^{2} + y^{2}.\n\nThenz_{x} = 2x,\\quad z_{xx} = 2,\\quad z_{xy} = 0, \\\\\nz_{y} = 2y,\\quad z_{yx} = 0,\\quad z_{yy} = 2.\n\nBut\\mathbf{x}^{\\prime}\\mathbf{H}\\mathbf{x} = z_{xx}x^{2} + z_{yy}y^{2} + z_{xy}z_{yx}xy = 2x^{2} + 2y^{2} > 0.\n\nHence the function z = x^{2} + y^{2} is characterized by a minimum at its optimal value and is therefore a strictly convex function. And it is easy to see that the Hessian is positive definite (i.e. the value of the function is always positive for any non-zero values of x and y).\n\nHere’s yet another example:\n\nSay we have a quadratic form in 2 variablesz = 8x^{2} + 6xy + 2y^{2}.\n\nThis can be rearranged asz = 8x^{2} + 3xy + 3xy + 2y^{2}.\n\nWe can write this as\\begin{aligned}\n\\mathbf{x}^{\\prime}\\mathbf{H}\\mathbf{x} &= z_{xx}x^{2} + z_{yy}y^{2} + (z_{xy} + z_{yx})xy \\\\\n&= 8x^{2} + 2y^{2} + (3 + 3)xy \\\\\n&= 8x^{2} + 2y^{2} + 6xy\n\\end{aligned}\n\nThe Hessian is\\mathbf{H} = \n\\begin{bmatrix}\n8 & 3 \\\\\n3 & 2\n\\end{bmatrix}\n\nThe principal minors are |H_{1}| = 8 and |H_{2}| = \\begin{vmatrix}\n8 & 3 \\\\\n3 & 2\n\\end{vmatrix} = 16 - 9 = 7. Both are positive so we have the Hessian and the quadratic form as positive definite!","type":"content","url":"/linear-algebra-special#the-quadratic-form","position":9},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices","lvl2":"Higher Order Hessian"},"type":"lvl2","url":"/linear-algebra-special#higher-order-hessian","position":10},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices","lvl2":"Higher Order Hessian"},"content":"Given y = f(x_{1},x_{2},x_{3}), the third‑order Hessian is|\\mathbf{H}| = \n\\begin{vmatrix}\ny_{11} & y_{12} & y_{13} \\\\\ny_{21} & y_{22} & y_{23} \\\\\ny_{31} & y_{32} & y_{33}\n\\end{vmatrix}\n\nwhere the elements are the various second‑order partial derivatives of y:y_{11} = \\frac{\\partial^{2}y}{\\partial x_{1}^{2}},\\quad \ny_{12} = \\frac{\\partial^{2}y}{\\partial x_{2}\\partial x_{1}},\\quad \ny_{23} = \\frac{\\partial^{2}y}{\\partial x_{3}\\partial x_{2}},\\quad \\text{etc.}\n\nConditions for a relative minimum or maximum depend on the signs of the first, second, and third principal minors, respectively. If|H_{1}| > 0,\\quad \n|H_{2}| = \n\\begin{vmatrix}\ny_{11} & y_{12} \\\\\ny_{21} & y_{22}\n\\end{vmatrix} > 0,\\quad \n\\text{and} \\quad |H_{3}| = |\\mathbf{H}| > 0,\n\nthen |\\mathbf{H}| is positive definite and fulfills the second‑order conditions for a minimum.\n\nIf|H_{1}| < 0,\\quad \n|H_{2}| = \n\\begin{vmatrix}\ny_{11} & y_{12} \\\\\ny_{21} & y_{22}\n\\end{vmatrix} > 0,\\quad \n\\text{and} \\quad |H_{3}| = |\\mathbf{H}| < 0,\n\nthen |\\mathbf{H}| is negative definite and fulfills the second‑order conditions for a maximum.\n\nLet’s take an example. Given the function:y = -5x_{1}^{2} + 10x_{1} + x_{1}x_{3} - 2x_{2}^{2} + 4x_{2} + 2x_{2}x_{3} - 4x_{3}^{2}.\n\nThe first order conditions (F.O.C) are\\frac{\\partial y}{\\partial x_1} = y_1 = -10x_1 + 10 + x_3 = 0, \\\\\n\\frac{\\partial y}{\\partial x_2} = y_2 = -4x_2 + 2x_3 + 4 = 0, \\\\\n\\frac{\\partial y}{\\partial x_3} = y_3 = x_1 + 2x_2 - 8x_3 = 0.\n\nwhich can be expressed in matrix form as\\begin{bmatrix}\n-10 & 0 & 1 \\\\\n0 & -4 & 2 \\\\\n1 & 2 & -8\n\\end{bmatrix}\n\\begin{bmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{bmatrix}\n= \n\\begin{bmatrix}\n-10 \\\\ -4 \\\\ 0\n\\end{bmatrix}\n\nUsing Cramer’s rule we get x_1 \\approx 1.04, x_2 \\approx 1.22 and x_3 \\approx 0.43 (please verify this).\n\nTaking the second partial derivatives from the first‑order conditions to create the Hessian,y_{11} = -10,\\quad y_{12} = 0,\\quad y_{13} = 1, \\\\\ny_{21} = 0,\\quad y_{22} = -4,\\quad y_{23} = 2, \\\\\ny_{31} = 1,\\quad y_{32} = 2,\\quad y_{33} = -8.\n\nThus,\\mathbf{H} = \n\\begin{bmatrix}\n-10 & 0 & 1 \\\\\n0 & -4 & 2 \\\\\n1 & 2 & -8\n\\end{bmatrix}\n\nFinally, applying the Hessian test, we have\\begin{aligned}\n|H_1| = -10 < 0, & \\\\\n|H_2| = \\begin{vmatrix} -10 & 0 \\\\ 0 & -4 \\end{vmatrix} = 40 > 0, & \\\\ \n|H_3| = |\\mathbf{H}| = \\begin{vmatrix} -10 & 0 & 1 \\\\ 0 & -4 & 2 \\\\ 1 & 2 & -8 \\end{vmatrix} = -276 < 0. &\n\\end{aligned}\n\nSince the principal minors alternate correctly in sign, the Hessian is negative definite and the function is maximized at x_1 \\approx 1.04, x_2 \\approx 1.22 and x_3 \\approx 0.43.\n\nTry these ^^\n\nFor each equation below find (a) critical values, and (b) the nature of the critical values using the Hessian.\n\n(1) f(x,y) = 3x^2 - xy - 2y^2 - 4x - 6y + 12(2) f(x,y,z) = -5x^2 + 10x + xz -2y^2 _ 4y + 2yz - 4z^2(3) f(x,y,z) = 3x^2 - 5x - xy + 6y^2 - 4y + 2yz + 4z^2 +2z - 3xz","type":"content","url":"/linear-algebra-special#higher-order-hessian","position":11},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices","lvl2":"The Bordered Hessian"},"type":"lvl2","url":"/linear-algebra-special#the-bordered-hessian","position":12},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices","lvl2":"The Bordered Hessian"},"content":"To optimize a function f(x_1, x_2) subject to a constraint g(x_1, x_2) the first‑order conditions can be found by setting up what is known as the Lagrangian function F(x_1, x_2, \\lambda) = f(x_1, x_2) - \\lambda(g(x_1, x_2) - c).\n\nThe second‑order conditions can be expressed in terms of a bordered Hessian |\\bar{\\mathbf{H}}| as|\\bar{\\mathbf{H}}| = \n\\begin{vmatrix}\n0 & g_1 & g_2 \\\\\ng_1 & F_{11} & F_{12} \\\\\ng_2 & F_{21} & F_{22}\n\\end{vmatrix}\n\nor|\\bar{\\mathbf{H}}| = \n\\begin{vmatrix}\n0 & g_1 & g_2 \\\\\ng_1 & f_{11} & f_{12} \\\\\ng_2 & f_{21} & f_{22}\n\\end{vmatrix}\n\nwhich is the usual Hessian bordered by the first derivatives of the constraint with zero on the principal diagonal.\n\nThe order of a bordered principal minor is determined by the order of the principal minor being bordered. Hence |\\bar{H}_2| above represents a second bordered principal minor, because the principal minor being bordered has dimensions 2\\times 2.\n\nFor the bivariate case with a single constraint, we simply look at |\\bar{H}_2|. If this is negative, the bordered Hessian is said to be positive definite and satisfies the second‑order condition for a minimum. However, if it is positive, the bordered Hessian is said to be negative definite, and meets the sufficient conditions for a maximum.\n\nLet’s try optimizing the following objective functionf(x_1, x_2) = 4x_1^2 + 3x_2^2 - 2x_1x_2\n\nsubject tox_1 + x_2 = 56.\n\nSetting up the Lagrangian function, taking the first‑order partials and solving gives x_1^* = 36, x_2^* = 20 (and \\lambda = 348). (You should verify this.)\n\nThe bordered Hessian for this optimization problem is|\\bar{\\mathbf{H}}| = \n\\begin{vmatrix}\n0 & 1 & 1 \\\\\n1 & 8 & -2 \\\\\n1 & -2 & 6\n\\end{vmatrix}\n\nStarting with the second principal minor, we have\\begin{aligned}\n|\\bar{H}_2| = |\\bar{\\mathbf{H}}| &= 0 \\cdot \n\\begin{vmatrix}\n8 & -2 \\\\\n-2 & 6\n\\end{vmatrix}\n- 1 \\cdot \n\\begin{vmatrix}\n1 & -2 \\\\\n1 & 6\n\\end{vmatrix}\n+ 1 \\cdot \n\\begin{vmatrix}\n1 & 8 \\\\\n1 & -2\n\\end{vmatrix} \\\\\n&= - (6+2) + (-2-8) = -8 -10 = -18\n\\end{aligned}\n\nwhich is negative, hence |\\bar{\\mathbf{H}}| is positive definite and we have met sufficient conditions for a minimum.\n\nFor the more general case in which the objective function has say n variables, i.e., f(x_1, \\ldots , x_n) which is subject to some constraint g(x_1, \\ldots , x_n), we can set up the bordered Hessian as|\\bar{\\mathbf{H}}| = \n\\begin{bmatrix}\n0 & g_1 & g_2 & \\dots & g_n \\\\\ng_1 & F_{11} & F_{12} & \\dots & F_{1n} \\\\\ng_2 & F_{21} & F_{22} & \\dots & F_{2n} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\ng_n & F_{n1} & F_{n2} & \\dots & F_{nn}\n\\end{bmatrix}\n\nwhere |\\bar{\\mathbf{H}}| = |\\bar{H}_n| because the n \\times n principal minor is bordered.\n\nIn this case, if all the principal minors are negative, i.e. |\\bar{H}_2|, |\\bar{H}_3|, \\ldots , |\\bar{H}_n| < 0, the bordered Hessian is said to be positive definite and satisfies the second‑order condition for a minimum.\n\nOn the other hand, if the principal minors alternate consistently in sign from positive to negative, i.e. |\\bar{H}_2| > 0, |\\bar{H}_3| < 0, |\\bar{H}_4| > 0 etc., the bordered Hessian is negative definite, and meets the sufficient conditions for a maximum.","type":"content","url":"/linear-algebra-special#the-bordered-hessian","position":13},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices","lvl2":"Input‑Output Analysis"},"type":"lvl2","url":"/linear-algebra-special#input-output-analysis","position":14},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices","lvl2":"Input‑Output Analysis"},"content":"If a_{ij} is a technical coefficient representing the value of input i required to produce one dollar’s worth of product j, the total demand for good i can be expressed asx_{i} = a_{i1}x_{1} + a_{i2}x_{2} + \\dots + a_{in}x_{n} + b_{i}\n\nwhere b_i is the final demand for product i. What is important to realize here is that the total demand for a product consists of that product being the final demand plus that product being an intermediate good required for the production of other products.\n\nIn matrix form we have\\mathbf{X} = \\mathbf{A}\\mathbf{X} + \\mathbf{B}\n\nwhere, for an n sector economy,\\mathbf{X} = \n\\begin{bmatrix}\nx_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n\n\\end{bmatrix},\n\\quad\n\\mathbf{A} = \n\\begin{bmatrix}\na_{11} & a_{12} & \\dots & a_{1n} \\\\\na_{21} & a_{22} & \\dots & a_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{n1} & a_{n2} & \\dots & a_{nn}\n\\end{bmatrix},\n\\quad\n\\mathbf{B} = \n\\begin{bmatrix}\nb_1 \\\\ b_2 \\\\ \\vdots \\\\ b_n\n\\end{bmatrix}.\n\n\\mathbf{A} is called the matrix of technical coefficients.\n\nTo find the output (intermediate and final goods) needed to satisfy demand, all we have to do is to solve for \\mathbf{X}:\\mathbf{X} - \\mathbf{A}\\mathbf{X} = \\mathbf{B} \\quad \\Rightarrow \\quad (\\mathbf{I} - \\mathbf{A})\\mathbf{X} = \\mathbf{B} \\quad \\Rightarrow \\quad \\mathbf{X} = (\\mathbf{I} - \\mathbf{A})^{-1}\\mathbf{B}.\n\nwhere (I-A) is kowns as the Leontief matrix.\n\nThus for a 3-sector economy, we have\\begin{bmatrix}\nx_{1} \\\\\nx_{2} \\\\\nx_{3}\n\\end{bmatrix} = \n\\begin{bmatrix}\n1 - a_{11} & -a_{12} & -a_{13} \\\\\n-a_{21} & 1 - a_{22} & -a_{23} \\\\\n-a_{31} & -a_{32} & 1 - a_{33}\n\\end{bmatrix}^{-1}\n\\begin{bmatrix}\nb_{1} \\\\\nb_{2} \\\\\nb_{n}\n\\end{bmatrix}\n\nSay we are asked to determine total output for three sectors/industries given A and B as below:A = \\begin{bmatrix} 0.3 & 0.4 & 0.1 \\\\ 0.5 & 0.2 & 0.6 \\\\ 0.1 & 0.3 & 0.1 \\end{bmatrix}, \\quad \\text{and} \\quad B = \\begin{bmatrix} 20 \\\\ 10 \\\\ 30 \\end{bmatrix}\n\nSince X = (I - A)^{-1} BI - A = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix} - \\begin{bmatrix} 0.3 & 0.4 & 0.1 \\\\ 0.5 & 0.2 & 0.6 \\\\ 0.1 & 0.3 & 0.1 \\end{bmatrix} = \\begin{bmatrix} 0.7 & -0.4 & -0.1 \\\\ -0.5 & 0.8 & -0.6 \\\\ -0.1 & -0.3 & 0.9 \\end{bmatrix}\n\nAnd taking the inverse(I - A)^{-1} = \\frac{1}{0.151} \\begin{bmatrix} 0.54 & 0.39 & 0.32 \\\\ 0.51 & 0.62 & 0.47 \\\\ 0.23 & 0.25 & 0.36 \\end{bmatrix}\n\nHence,\\begin{aligned}\nX = \\begin{bmatrix} x_{1} \\\\ x_{2} \\\\ x_{3} \\end{bmatrix} &= \\frac{1}{0.151} \\begin{bmatrix} 0.54 & 0.39 & 0.32 \\\\ 0.51 & 0.62 & 0.47 \\\\ 0.23 & 0.25 & 0.36 \\end{bmatrix} \\begin{bmatrix} 20 \\\\ 10 \\\\ 30 \\end{bmatrix} \\\\\n&= \\frac{1}{0.151} \\begin{bmatrix} 24.3 \\\\ 30.5 \\\\ 17.9 \\end{bmatrix} = \\begin{bmatrix} 160.93 \\\\ 201.99 \\\\ 118.54 \\end{bmatrix}\n\\end{aligned}\n\nTry these ^^\n\nDetermine the total demand for industries 1, 2 and 3, given the matrix of technical coefficients A and the final demand vector b below.\\mathbf{A} = \\begin{bmatrix} \n0.2 & 0.3 & 0.2 \\\\ \n0.4 & 0.1 & 0.3 \\\\ \n0.3 & 0.5 & 0.2 \n\\end{bmatrix}, \\quad \nb = \\begin{bmatrix} \n150 \\\\ \n200 \\\\ \n210 \n\\end{bmatrix}","type":"content","url":"/linear-algebra-special#input-output-analysis","position":15},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices","lvl2":"Characteristic Roots and Vectors (Eigenvalues and Eigenvectors)"},"type":"lvl2","url":"/linear-algebra-special#characteristic-roots-and-vectors-eigenvalues-and-eigenvectors","position":16},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices","lvl2":"Characteristic Roots and Vectors (Eigenvalues and Eigenvectors)"},"content":"The sign and definiteness of a Hessian and a quadratic form has been tested by using the principal minors. Sign definiteness can also be tested by using the characteristic roots of a matrix. Given a square matrix \\mathbf{A}, is possible to find a vector \\mathbf{V} \\neq 0 and a scalar \\lambda such that\\mathbf{AV} = \\lambda\\mathbf{V}\n\nthe scalar \\lambda is called the characteristic root, latent value or eigenvalue; and the vector \\mathbf{V} is called the characteristic vector, latent vector or eigenvector. The above can be written as\\mathbf{AV} - \\lambda\\mathbf{V} = 0\n\nwhich can be rearranged so that\\begin{aligned}\n\\mathbf{AV} - \\lambda\\mathbf{IV} &= 0 \\\\\n(\\mathbf{A} - \\lambda\\mathbf{I})\\mathbf{V} &= 0\n\\end{aligned}\n\nwhere \\mathbf{A} - \\lambda\\mathbf{I} is called the characteristic matrix of \\mathbf{A}. Since we have \\mathbf{V} \\neq 0, the characteristic matrix \\mathbf{A} - \\lambda\\mathbf{I} must be singular and thus its determinant is zero.\n\nIf \\mathbf{A} is a 3 \\times 3 matrix, then|\\mathbf{A} - \\lambda\\mathbf{I}| = \n\\begin{vmatrix}\na_{11} - \\lambda & a_{12} & a_{13} \\\\\na_{21} & a_{22} - \\lambda & a_{23} \\\\\na_{31} & a_{32} & a_{33} - \\lambda\n\\end{vmatrix} = 0\n\nWith |\\mathbf{A} - \\lambda\\mathbf{I}| = 0, there will be an infinite number of solutions for \\mathbf{V}. To force a unique solution, the solution may be normalized by requiring of the elements v_i of \\mathbf{V} such that \\sum v_i^2 = 1.\n\nSign Definiteness and Characteristc Roots\n\nFor a square matrix \\mathbf{A} if\n\nAll characteristic roots \\lambda are positive \\Rightarrow \\mathbf{A} is positive definite.\n\nAll \\lambda’s are negative \\Rightarrow \\mathbf{A} is negative definite.\n\nAll \\lambda’s are nonnegative and at least one \\lambda = 0 \\Rightarrow \\mathbf{A} is positive semi‑definite.\n\nAll \\lambda’s are nonpositive and at least one \\lambda = 0 \\Rightarrow \\mathbf{A} is negative semi‑definite.\n\nSome \\lambda’s are positive and some negative \\Rightarrow \\mathbf{A} is sign indefinite.\n\nLet’s take an example. Given a square matrix\\mathbf{A} = \n\\begin{bmatrix}\n-6 & 3 \\\\\n3 & -6\n\\end{bmatrix}\n\nTo find the characteristic roots (eigenvalues) of \\mathbf{A}, we simply set |\\mathbf{A} - \\lambda \\mathbf{I}| = 0:|\\mathbf{A} - \\lambda \\mathbf{I}| = \n\\begin{vmatrix}\n-6 - \\lambda & 3 \\\\\n3 & -6 - \\lambda\n\\end{vmatrix} = 0\n\nThis means(-6 - \\lambda)(-6 - \\lambda) - 9 = 0 \\\\\n\\lambda^2 + 12\\lambda + 27 = 0 \\\\\n(\\lambda + 9)(\\lambda + 3) = 0 \\\\\n\\lambda = -9, -3.\n\nSince both characteristic roots \\lambda are negative, we say \\mathbf{A} is negative definite.\n\nNote:\\text{(i) } \\sum \\lambda_i = \\operatorname{tr}(\\mathbf{A}), \\qquad\n\\text{(ii) } \\prod \\lambda_i = |\\mathbf{A}|.\n\nLet’s continue with the example above to find the characteristic vector.\n\nWe know one of the roots \\lambda = -9, so substituting in the characteristic matrix gives(\\mathbf{A} - \\lambda \\mathbf{I})\\mathbf{v} = \\mathbf{0} \\quad \\Rightarrow \\quad\n\\begin{bmatrix}\n-6 - (-9) & 3 \\\\\n3 & -6 - (-9)\n\\end{bmatrix}\n\\begin{bmatrix}\nv_1 \\\\ v_2\n\\end{bmatrix}\n= \n\\begin{bmatrix}\n3 & 3 \\\\\n3 & 3\n\\end{bmatrix}\n\\begin{bmatrix}\nv_1 \\\\ v_2\n\\end{bmatrix}\n= \n\\begin{bmatrix}\n0 \\\\ 0\n\\end{bmatrix}.\n\nSince the coefficient matrix is linearly dependent, there are infinite number of solutions. The product of the matrices gives two equations which are identical:3v_1 + 3v_2 = 0 \\quad \\Rightarrow \\quad v_2 = -v_1.\n\nBy normalizing we havev_1^{2} + v_2^{2} = 1.\n\nSubstituting v_2 = -v_1 givesv_1^{2} + (-v_1)^{2} = 1 \\quad \\Rightarrow \\quad 2v_1^{2} = 1 \\quad \\Rightarrow \\quad v_1^{2} = \\frac{1}{2}.\n\nTaking the positive square root gives v_1 = \\sqrt{1/2} = \\frac{\\sqrt{2}}{2} and substituting into v_2 = -v_1 gives v_2 = -\\frac{\\sqrt{2}}{2}. That is\\mathbf{v}_1 = \n\\begin{bmatrix}\n\\frac{\\sqrt{2}}{2} \\\\[4pt]\n-\\frac{\\sqrt{2}}{2}\n\\end{bmatrix}.\n\nUsing the second characteristic root \\lambda = -3:(\\mathbf{A} - \\lambda \\mathbf{I})\\mathbf{v} = \n\\begin{bmatrix}\n-6 - (-3) & 3 \\\\\n3 & -6 - (-3)\n\\end{bmatrix}\n\\begin{bmatrix}\nv_1 \\\\ v_2\n\\end{bmatrix}\n= \n\\begin{bmatrix}\n-3 & 3 \\\\\n3 & -3\n\\end{bmatrix}\n\\begin{bmatrix}\nv_1 \\\\ v_2\n\\end{bmatrix}\n= \n\\begin{bmatrix}\n0 \\\\ 0\n\\end{bmatrix}.\n\nMultiplying out gives -3v_1 + 3v_2 = 0 and 3v_1 - 3v_2 = 0, so v_1 = v_2.\n\nNormalizing as before:v_1^2 + v_2^2 = 1 \\quad \\Rightarrow \\quad 2v_1^2 = 1 \\quad \\Rightarrow \\quad v_1 = \\frac{\\sqrt{2}}{2}.\n\nHence,\\mathbf{v}_2 = \n\\begin{bmatrix}\n\\frac{\\sqrt{2}}{2} \\\\[4pt]\n\\frac{\\sqrt{2}}{2}\n\\end{bmatrix}.","type":"content","url":"/linear-algebra-special#characteristic-roots-and-vectors-eigenvalues-and-eigenvectors","position":17},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices","lvl2":"Diagonalization"},"type":"lvl2","url":"/linear-algebra-special#diagonalization","position":18},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices","lvl2":"Diagonalization"},"content":"A square matrix A is diagonalizable if it can be written as:\\mathbf{A} = \\mathbf{T} \\mathbf{D} \\mathbf{T}^{-1}.\n\nwhere\n\n\\mathbf{D} is a diagonal matrix (entries only on the main diagonal) consisting of eignevalues of \\mathbf{A},\n\n\\mathbf{T} is an invertible matrix whose columns are eigenvectors of \\mathbf{A}.\n\nNote that not all matrices are diagonalizable, however.\n\nFor example, givenA =\n\\begin{bmatrix}\n2 & 1 \\\\\n1 & 2\n\\end{bmatrix}\n\nLet us start by finding its eigenvalues.\n\nThe characteristic polynomial is given by\\det(A - \\lambda I)\n= (2 - \\lambda)^2 - 1\n= \\lambda^2 - 4\\lambda + 3 = 0.\n\nHence,\\lambda = 1, \\qquad \\lambda = 3.\n\nFor \\lambda = 1:(A - I)\\mathbf{v}\n=\n\\begin{pmatrix}\n1 & 1 \\\\\n1 & 1\n\\end{pmatrix}\n\\mathbf{v}\n= \\mathbf{0}.\n\nA corresponding eigenvector is\\mathbf{v}_1 =\n\\begin{pmatrix}\n1 \\\\\n-1\n\\end{pmatrix}.\n\nFor \\lambda = 3:(A - 3I)\\mathbf{v}\n=\n\\begin{pmatrix}\n-1 & 1 \\\\\n1 & -1\n\\end{pmatrix}\n\\mathbf{v}\n= \\mathbf{0}.\n\nA corresponding eigenvector is\\mathbf{v}_2 =\n\\begin{pmatrix}\n1 \\\\\n1\n\\end{pmatrix}.\n\nThen,T =\n\\begin{pmatrix}\n1 & 1 \\\\\n-1 & 1\n\\end{pmatrix},\n\\qquad\nD =\n\\begin{pmatrix}\n1 & 0 \\\\\n0 & 3\n\\end{pmatrix}.\n\nThe inverse of T isT^{-1}\n= \\tfrac{1}{2}\n\\begin{pmatrix}\n1 & -1 \\\\\n1 & 1\n\\end{pmatrix}.\n\nWe can verify that A = TDT^{-1}.TDT^{-1}\n=\n\\begin{pmatrix}\n1 & 1 \\\\\n-1 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\n1 & 0 \\\\\n0 & 3\n\\end{pmatrix}\n\\cdot\n\\tfrac{1}{2}\n\\begin{pmatrix}\n1 & -1 \\\\\n1 & 1\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n2 & 1 \\\\\n1 & 2\n\\end{pmatrix}\n= A.\n\nNote (1):\n\nAll symmetric matrices are diagonalizable (even with repeated eigenvalues).\n\nDiagonalizable \\neq invertible.For example,\\begin{bmatrix}\n1 & 0 \\\\\n0 & 0\n\\end{bmatrix}\n\nis diagonalizable but not invertible.\n\nIf A is diagonalizable, then A^n and e^{A} are easy to compute.\n\nNote (2):This is closely related to the transformation form of diagonalization, which states that if A is diagonalizable, then there exists an invertible matrix T and a diagonal matrix D such thatT^{-1} A T = D.\n\nExercise. Can you show this?\n\nTry these ^^\n\nGiven:A = \\begin{bmatrix}  -4 & -2 \\\\ -2 & -6 \\end{bmatrix}, \\quad \nB = \\begin{bmatrix} 3 & 0 \\\\ 1 & 2 \\end{bmatrix}C = \\begin{bmatrix} 6 & 3 \\\\ 3 & -2 \\end{bmatrix}, \\quad\nD = \\begin{bmatrix} 4 & 6 & 3 \\\\ 0 & 2 & 5 \\\\ 0 & 1 & 3 \\end{bmatrix}\n\nFind:\n\n(a) Find the eigenvalues and eigenvectors for each of the matrices above(b) What can you say about the sign definiteness of each mattix(c) Verify \\mathbf{A} = \\mathbf{T} \\mathbf{D} \\mathbf{T}^{-1}(d) Find \\mathbf{A}^5","type":"content","url":"/linear-algebra-special#diagonalization","position":19}]}
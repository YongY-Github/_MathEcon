{"version":"1","records":[{"hierarchy":{"lvl1":"Basic Differentiation"},"type":"lvl1","url":"/basic-differentiation","position":0},{"hierarchy":{"lvl1":"Basic Differentiation"},"content":"Before we study the differentiation of single-variable functions, we briefly review several foundational mathematical concepts.","type":"content","url":"/basic-differentiation","position":1},{"hierarchy":{"lvl1":"Basic Differentiation","lvl2":"Functions"},"type":"lvl2","url":"/basic-differentiation#functions","position":2},{"hierarchy":{"lvl1":"Basic Differentiation","lvl2":"Functions"},"content":"A function  f  from a set  X  to a set  Y , written  f: X \\to Y , is a rule that assigns exactly one element of  Y  to each element of  X .\n\nX is called the domain\n\nThe range is the set of values in Y that are actually attained\n\nUsing x \\in X and y \\in Y, a function is written as\n y = f(x) ,\nwhere x is the independent variable and y the dependent variable.","type":"content","url":"/basic-differentiation#functions","position":3},{"hierarchy":{"lvl1":"Basic Differentiation","lvl2":"Graphs"},"type":"lvl2","url":"/basic-differentiation#graphs","position":4},{"hierarchy":{"lvl1":"Basic Differentiation","lvl2":"Graphs"},"content":"If X and Y are sets of real numbers, the graph of a function f is the set of points\n(x, y) such that y = f(x).\n\nEconomic convention.Economists often draw demand curves with quantity on the horizontal axis and price on the vertical axis, even when the function is written as  q = f(p) .","type":"content","url":"/basic-differentiation#graphs","position":5},{"hierarchy":{"lvl1":"Basic Differentiation","lvl2":"Slope"},"type":"lvl2","url":"/basic-differentiation#slope","position":6},{"hierarchy":{"lvl1":"Basic Differentiation","lvl2":"Slope"},"content":"The slope of a line through points (x, y) and (x', y') ism = \\frac{y' - y}{x' - x}.\n\nDifferentiation is the method of finding the slope of a function and is denoted by  f'(x) .","type":"content","url":"/basic-differentiation#slope","position":7},{"hierarchy":{"lvl1":"Basic Differentiation","lvl2":"Limits"},"type":"lvl2","url":"/basic-differentiation#limits","position":8},{"hierarchy":{"lvl1":"Basic Differentiation","lvl2":"Limits"},"content":"We say that a function f has limit L as x \\to a if, for any \\varepsilon > 0, there exists a \\delta > 0 such that|f(x) - L| < \\varepsilon\n\\quad \\text{whenever} \\quad\n0 < |x - a| < \\delta.\n\nWhen this condition holds, we write\\lim_{x \\to a} f(x) = L.","type":"content","url":"/basic-differentiation#limits","position":9},{"hierarchy":{"lvl1":"Basic Differentiation","lvl2":"Continuity"},"type":"lvl2","url":"/basic-differentiation#continuity","position":10},{"hierarchy":{"lvl1":"Basic Differentiation","lvl2":"Continuity"},"content":"A function f is continuous at a if:\n\nf(a) is defined\n\n\\lim_{x \\to a} f(x) exists\n\n\\lim_{x \\to a} f(x) = f(a)","type":"content","url":"/basic-differentiation#continuity","position":11},{"hierarchy":{"lvl1":"Basic Differentiation","lvl2":"Derivative at a Point"},"type":"lvl2","url":"/basic-differentiation#derivative-at-a-point","position":12},{"hierarchy":{"lvl1":"Basic Differentiation","lvl2":"Derivative at a Point"},"content":"Let y = f(x). When x changes by \\Delta x, the change in y is\\frac{\\Delta y}{\\Delta x}\n=\n\\frac{f(x+\\Delta x) - f(x)}{\\Delta x}.","type":"content","url":"/basic-differentiation#derivative-at-a-point","position":13},{"hierarchy":{"lvl1":"Basic Differentiation","lvl2":"Derivative as a Function"},"type":"lvl2","url":"/basic-differentiation#derivative-as-a-function","position":14},{"hierarchy":{"lvl1":"Basic Differentiation","lvl2":"Derivative as a Function"},"content":"The derivative of f at x is defined asf'(x)\n=\n\\lim_{\\Delta x \\to 0}\n\\frac{f(x+\\Delta x) - f(x)}{\\Delta x}.\n\nIf the derivative exists for every x in the domain of f, then the derivative itself defines a new function, denoted f'(x).\n\nGeometrically, f'(x) is the slope of the tangent line to the graph of f at (x, f(x)).\n\nCommon notations include:\n\n f'(x) \n\n \\dfrac{dy}{dx} \n\n Df(x) \n\nNote that what we usually think of as a variable x is held constant while \\Delta x varies and converges to zero. It is useful to keep in mind that the derivative of a function f at x is the slope of a line tangent to the graph of the function f at the point (x, f(x)). It is crucial to understand the implications of the existence of the derivative at a point x. The function must be smooth—meaning it is both continuous and differentiable—at the point x. The tangent line provides a high-quality linear approximation to the graph of the function near x. In general, if we know that the function f is differentiable at a, then the tangent line approximation to f at a is:y = f(a) + f'(a)(x - a)\n\nwhere a, f(a), \\text{ and } f'(a) are constants, x is the independent variable, and y is the dependent variable. We will see this point again with Taylor series expansions. Many important concepts in economics—such as marginal cost or marginal utility—are based on this derivative function.","type":"content","url":"/basic-differentiation#derivative-as-a-function","position":15},{"hierarchy":{"lvl1":"Basic Differentiation","lvl2":"Second Derivative"},"type":"lvl2","url":"/basic-differentiation#second-derivative","position":16},{"hierarchy":{"lvl1":"Basic Differentiation","lvl2":"Second Derivative"},"content":"The second derivative is the derivative of the derivative and is written asf''(x)\n=\n\\frac{d^2 f(x)}{dx^2}.\n\nEconomic interpretation.If  \\ln p(t)  describes log prices over time, then:\n\nthe first derivative is inflation\n\nthe second derivative is the change in inflation","type":"content","url":"/basic-differentiation#second-derivative","position":17},{"hierarchy":{"lvl1":"Basic Differentiation","lvl2":"Basic Rules of Differentiation"},"type":"lvl2","url":"/basic-differentiation#basic-rules-of-differentiation","position":18},{"hierarchy":{"lvl1":"Basic Differentiation","lvl2":"Basic Rules of Differentiation"},"content":"Let y = f(x).","type":"content","url":"/basic-differentiation#basic-rules-of-differentiation","position":19},{"hierarchy":{"lvl1":"Basic Differentiation","lvl3":"Constant-function Rule","lvl2":"Basic Rules of Differentiation"},"type":"lvl3","url":"/basic-differentiation#constant-function-rule","position":20},{"hierarchy":{"lvl1":"Basic Differentiation","lvl3":"Constant-function Rule","lvl2":"Basic Rules of Differentiation"},"content":"The derivative of a contsant function y=f(x)=k is zero, for all values of x-it has zero slope!\\frac{d}{dx}(k) = 0.","type":"content","url":"/basic-differentiation#constant-function-rule","position":21},{"hierarchy":{"lvl1":"Basic Differentiation","lvl3":"Power-function Rule","lvl2":"Basic Rules of Differentiation"},"type":"lvl3","url":"/basic-differentiation#power-function-rule","position":22},{"hierarchy":{"lvl1":"Basic Differentiation","lvl3":"Power-function Rule","lvl2":"Basic Rules of Differentiation"},"content":"The derivative of a power function f(x) = x^n is:\\frac{d}{dx}(x^n) = n x^{n-1}.","type":"content","url":"/basic-differentiation#power-function-rule","position":23},{"hierarchy":{"lvl1":"Basic Differentiation","lvl3":"Generalized Power-function Rule","lvl2":"Basic Rules of Differentiation"},"type":"lvl3","url":"/basic-differentiation#generalized-power-function-rule","position":24},{"hierarchy":{"lvl1":"Basic Differentiation","lvl3":"Generalized Power-function Rule","lvl2":"Basic Rules of Differentiation"},"content":"When a multiplicaytive constant k appears in the power fuction, so that f(x) = kx^n, then:\\frac{d}{dx}(k x^n) = k n x^{n-1}.","type":"content","url":"/basic-differentiation#generalized-power-function-rule","position":25},{"hierarchy":{"lvl1":"Basic Differentiation","lvl3":"Logarithmic Rule","lvl2":"Basic Rules of Differentiation"},"type":"lvl3","url":"/basic-differentiation#logarithmic-rule","position":26},{"hierarchy":{"lvl1":"Basic Differentiation","lvl3":"Logarithmic Rule","lvl2":"Basic Rules of Differentiation"},"content":"The derivatice of the log-function f(x) = lnx is:\\frac{d}{dx}(\\ln x) = \\frac{1}{x}.","type":"content","url":"/basic-differentiation#logarithmic-rule","position":27},{"hierarchy":{"lvl1":"Basic Differentiation","lvl3":"Exponential Rule","lvl2":"Basic Rules of Differentiation"},"type":"lvl3","url":"/basic-differentiation#exponential-rule","position":28},{"hierarchy":{"lvl1":"Basic Differentiation","lvl3":"Exponential Rule","lvl2":"Basic Rules of Differentiation"},"content":"For some exponential function f(x) = a^x, where a is some constant, then:\\frac{d}{dx}(a^x) = a^x \\ln a.\n\nNote that a particular case of the above is\\frac{d}{d x} e^x = e^x\n\nWhile\\frac{d}{d x} \\ln x = \\frac{1}{x}\n\nNow, let’s consider some further useful rules of differentiation involving two or more functions of the same variable. Specifically, suppose f(x) and g(x) are two different functions of x and that f'(x) and g'(x) exist. That is, let f(x) and g(x) be differentiable, then:","type":"content","url":"/basic-differentiation#exponential-rule","position":29},{"hierarchy":{"lvl1":"Basic Differentiation","lvl3":"Sum-difference Rules","lvl2":"Basic Rules of Differentiation"},"type":"lvl3","url":"/basic-differentiation#sum-difference-rules","position":30},{"hierarchy":{"lvl1":"Basic Differentiation","lvl3":"Sum-difference Rules","lvl2":"Basic Rules of Differentiation"},"content":"The derivative of a sum (difference) of two functions is the sum (difference) of the derivatives of the two functions.\\frac{d}{dx}[f(x) \\pm g(x)] = f'(x) \\pm g'(x).\n\nTry these ^^\n\nDetermine the derivative of each of the functions below.\n\ny = 30x + 10\n\ny = 8x^2 - 6x + 12\n\ny = 6\n\ny = \\sqrt{3} - 2x^2\n\nAnswers\n\n1.\\; f'(x) = 30 \\quad\n2.\\; f'(x) = 16x - 6 \\quad\n3.\\; f'(x) = 0 \\quad\n4.\\; f'(x) = -4x","type":"content","url":"/basic-differentiation#sum-difference-rules","position":31},{"hierarchy":{"lvl1":"Basic Differentiation","lvl3":"Product Rule","lvl2":"Basic Rules of Differentiation"},"type":"lvl3","url":"/basic-differentiation#product-rule","position":32},{"hierarchy":{"lvl1":"Basic Differentiation","lvl3":"Product Rule","lvl2":"Basic Rules of Differentiation"},"content":"The derivative of the product of two (differentiable) functions is equal to the first function times the derivative of the second function plus the second function times the derivative of the first function.\\frac{d}{dx}[f(x)g(x)]\n=\nf'(x)g(x) + f(x)g'(x).","type":"content","url":"/basic-differentiation#product-rule","position":33},{"hierarchy":{"lvl1":"Basic Differentiation","lvl3":"Quotient Rule","lvl2":"Basic Rules of Differentiation"},"type":"lvl3","url":"/basic-differentiation#quotient-rule","position":34},{"hierarchy":{"lvl1":"Basic Differentiation","lvl3":"Quotient Rule","lvl2":"Basic Rules of Differentiation"},"content":"The derivative of the quotient of two (differentiable) functions, f(x)/g(x), is\\frac{d}{dx} \\left[ \\frac{f(x)}{g(x)} \\right] = \\frac{g(x)f'(x) - f(x)g'(x)}{[g(x)]^2}\n\nprovided that g(x) \\neq 0. Note that [g(x)]^2 = g^2(x).\n\nTry these ^^\n\nDifferentiate the following functions with respect to x.\n\ny=\\dfrac{3x(2x-1)}{5x-2}\n\ny=3x(4x-5)^2\n\ny=(5x-1)(3x+4)^3\n\ny=(3x-4)\\dfrac{5x+1}{2x+7}\n\ny=\\dfrac{(8x-5)^3}{7x+4}\n\ny=\\left(\\dfrac{3x+4}{2x+5}\\right)^2\n\nAnswers\n\n1.\\; y'=\\frac{30x^2-24x+6}{(5x-2)^2} \\quad\n2.\\; y'=144x^2-240x+75 \\quad\n3.\\;  y'=(45x-9)(3x+4)^2+5(3x+4)^3 \\quad\n4.\\;  y'=\\frac{30x^2+210x-111}{(2x+7)^2} \\quad\n5.\\;  y'=\\frac{(168x+96)(8x-5)^2-7(8x-5)^3}{(7x+4)^2} \\quad\n6.\\;  y'=\\frac{42x+56}{(2x+5)^3}\n\nUse the quotient rule to differentiate each function.\n\nf(x)=\\dfrac{2x+7}{x^2-1}\n\nf(x)=\\dfrac{bx^3+cx^2+x-4}{x}\n\nf(x)=\\dfrac{e^{2x}}{x^2}\n\nf(x)=\\dfrac{(3x+2)^2}{x}\n\nAnswers\n\n1.\\; f'(x)=\\frac{-2x^2-14x-2}{(x^2-1)^2} \\quad\n2.\\; f'(x)=\\frac{2bx^3+cx^2+4}{x^2} \\\\[6pt]\n3.\\; f'(x)=\\frac{2xe^{2x}-2e^{2x}}{x^3} \\quad\n4.\\; f'(x)=\\frac{9x^2-4}{x^2}\n\nSecond Derivatives\n\nFind the second derivative of each function.\n\ny = 9 - 3x + 7x^2 - x^3\n\ny = \\dfrac{4x+5}{x}\n\ny = \\ln(4x)\n\ny = x^2 e^x\n\ny = (x-6)^4\n\nAnswers\n\n1.\\; y'' = 14 - 6x \\quad\n2.\\; y'' = 10x^{-3} \\quad\n3.\\; y'' = -x^{-2} \\quad\n4.\\; y'' = 2e^x + 4xe^x + x^2 e^x \\quad\n5.\\; y'' = 12(x-6)^2","type":"content","url":"/basic-differentiation#quotient-rule","position":35},{"hierarchy":{"lvl1":"Basic Differentiation","lvl3":"Chain Rule","lvl2":"Basic Rules of Differentiation"},"type":"lvl3","url":"/basic-differentiation#chain-rule","position":36},{"hierarchy":{"lvl1":"Basic Differentiation","lvl3":"Chain Rule","lvl2":"Basic Rules of Differentiation"},"content":"If z = f(y) and y = g(x), then\\frac{dz}{dx}\n=\n\\frac{dz}{dy}\\frac{dy}{dx}.\n\nThe chain rule provides a convenient way to study how one variable (say, x) affects another variable (z) through its influence on some intermediate variable (y).\n\nSometimes, we can write for a composite function y = f(g(x)):\\frac{dy}{dx} = f'(g(x)) \\cdot g'(x)","type":"content","url":"/basic-differentiation#chain-rule","position":37},{"hierarchy":{"lvl1":"Basic Differentiation","lvl3":"Chain Rule for Exponential and Logarithmic Functions","lvl2":"Basic Rules of Differentiation"},"type":"lvl3","url":"/basic-differentiation#chain-rule-for-exponential-and-logarithmic-functions","position":38},{"hierarchy":{"lvl1":"Basic Differentiation","lvl3":"Chain Rule for Exponential and Logarithmic Functions","lvl2":"Basic Rules of Differentiation"},"content":"","type":"content","url":"/basic-differentiation#chain-rule-for-exponential-and-logarithmic-functions","position":39},{"hierarchy":{"lvl1":"Basic Differentiation","lvl4":"The general exponential function rule","lvl3":"Chain Rule for Exponential and Logarithmic Functions","lvl2":"Basic Rules of Differentiation"},"type":"lvl4","url":"/basic-differentiation#the-general-exponential-function-rule","position":40},{"hierarchy":{"lvl1":"Basic Differentiation","lvl4":"The general exponential function rule","lvl3":"Chain Rule for Exponential and Logarithmic Functions","lvl2":"Basic Rules of Differentiation"},"content":"\\frac{d}{d x} e^{g(x)} = e^{g(x)} g'(x)\n\nFor example:\\frac{d}{d x} e^{ax} = \\frac{d}{d (ax)} e^{ax} \\frac{d}{d x} (ax) = e^{ax} a = ae^{ax}\n\nIf we are using a base other than e:\\frac {d}{d x}(a^{g(x)}) = a^{g(x)} g'(x) \\ln a, \\text{ where } a > 0, a \\neq 0","type":"content","url":"/basic-differentiation#the-general-exponential-function-rule","position":41},{"hierarchy":{"lvl1":"Basic Differentiation","lvl4":"The general natural logarithmic function rule","lvl3":"Chain Rule for Exponential and Logarithmic Functions","lvl2":"Basic Rules of Differentiation"},"type":"lvl4","url":"/basic-differentiation#the-general-natural-logarithmic-function-rule","position":42},{"hierarchy":{"lvl1":"Basic Differentiation","lvl4":"The general natural logarithmic function rule","lvl3":"Chain Rule for Exponential and Logarithmic Functions","lvl2":"Basic Rules of Differentiation"},"content":"\\frac{d}{d x} \\ln(g(x)) = \\frac{g'(x)}{g(x)}\n\nInterestingly:\\frac{d}{d x} \\ln(ax) = \\frac{d}{d(ax)} \\ln(ax) \\frac{d}{d x}(ax) = \\frac{1}{ax} a = 1/x\n\nwhile\\frac{d}{d x} \\ln(x^2) = \\frac{d}{d(x^2)} \\ln(x^2) \\frac{d}{d x}(x^2) = \\frac{1}{x^2} 2x = 2/x\n\nNote also when considered base other than e. Because\\log_b(x) = \\frac{\\ln(x)}{\\ln(b)}\n\nwe have\\frac{d}{d x} \\log_b(x) = \\frac{1}{x} \\frac{1}{\\ln(b)}\n\nOr more generally:\\begin{aligned} \n\\frac{d}{d x} \\log_b g(x) &= \\frac{g'(x)}{g(x)} \\frac{1}{\\ln b}, \\text{ where } b > 0, b \\neq 1 \\\\ \n&= \\frac{g'(x)}{g(x)} \\log_b e \n\\end{aligned}\n\nNote that \\log_b e = \\displaystyle \\frac{1}{\\ln b}.\n\nTry these ^^\n\nExponential Functions\n\nUse the rules of differentiating exponential functions to find the derivative with respect to x of each of the following functions.\n\ny = x^2 e^{5x}\n\ny = \\dfrac{e^{5x}-1}{e^{5x}+1}\n\ny = a^{2x}\n\ny = a^{5x^2}\n\ny = 4^{2x+7}\n\ny = x^3 2^x\n\nAnswers\n\n1.\\; y' = x e^{5x}(5x+2) \\quad\n2.\\; y' = \\frac{10e^{5x}}{(e^{5x}+1)^2} \\quad\n3.\\; y' = 2a^{2x}\\ln a \\quad\n4.\\; y' = 10x\\,a^{5x^2}\\ln a \\quad\n5.\\; y' = 2\\ln(4)\\,4^{2x+7} \\quad\n6.\\; y' = x^2 2^x(x\\ln 2 + 3)\n\nLogarithmic Functions\n\nUse the rules of differentiating logarithms to find the derivative of each function.\n\nf(x)=x^{-4}+\\ln(ax)\n\nf(x)=4x^3\\ln x^2\n\nf(x)=\\ln x-\\ln(1+x)\n\nf(x)=\\ln\\!\\left(\\dfrac{2x^2}{5x}\\right)\n\nf(x)=\\log_2(6x)\n\nf(x)=\\log_4(9x^3)\n\nAnswers\n\n1.\\; f'(x)=-4x^{-5}+x^{-1} \\quad\n2.\\; f'(x)=8x^2+24x^2\\ln x \\quad\n3.\\; f'(x)=\\frac{1}{x(1+x)} \\quad\n4.\\; f'(x)=\\frac{1}{x} \\quad\n5.\\; f'(x)=\\frac{1}{x\\ln 2} \\quad\n6.\\; f'(x)=\\frac{3}{x\\ln 4}\n\nChain Rule\n\nUse the chain rule to find the derivative, f'(x), of the following:\n\nf(x) = (x + 1)^3 + (x^2 - 2x)^2 - 5\n\nf(x) = (2x + 4)^{99}\n\nf(x) = (5x^2 + 10x + 3)^{20}\n\nf(x) = (e^x)^{ab}\n\nf(x) = (e^{x^a})^{b}\n\nf(x) = (e^{a + bx + cx^2})^{10}\n\nAnswers\n\n1.\\; f'(x)=4x^3-9x^2+14x+3 \\quad\n2.\\; f'(x)=198(2x+4)^{98} \\quad\n3.\\; f'(x)=(200x+200)(5x^2+10x+3)^{19} \\\\[6pt]\n4.\\; f'(x)=ab\\,e^{abx} \\quad\n5.\\; f'(x)=ab\\,x^{a-1}e^{bx^a} \\quad\n6.\\; f'(x)=10(b+2cx)(e^{a+bx+cx^2})^{10}\n\nDerivatives of Exponential and Logarithmic Functions\n\n1. Exponential Rule with Base a\n\nProblem: Find the derivative of f(x) = 5^{x^3 + 2x}.\n\nSolution:\nUsing the rule \\frac{d}{d x}(a^{g(x)}) = a^{g(x)} g'(x) \\ln a:\n\nLet a = 5\n\nLet g(x) = x^3 + 2x, so g'(x) = 3x^2 + 2f'(x) = 5^{x^3 + 2x} \\cdot (3x^2 + 2) \\cdot \\ln 5\n\nGeneral Natural Logarithmic Rule\n\nProblem: Find the derivative of f(x) = \\ln(\\sin(x)).\n\nSolution:\nUsing the rule \\frac{d}{d x} \\ln(g(x)) = \\frac{g'(x)}{g(x)}:\n\nLet g(x) = \\sin(x), so g'(x) = \\cos(x)f'(x) = \\frac{\\cos(x)}{\\sin(x)} = \\cot(x)\n\nLogarithm with Base b\n\nProblem: Find the derivative of f(x) = \\log_{10}(x^2 + 1).\n\nSolution:\nUsing the rule \\frac{d}{d x} \\log_b g(x) = \\frac{g'(x)}{g(x) \\ln b}:\n\nLet b = 10\n\nLet g(x) = x^2 + 1, so g'(x) = 2xf'(x) = \\frac{2x}{(x^2 + 1) \\ln(10)}\n\nComparison of \\ln(ax) vs \\ln(x^n)\n\nProblem: Differentiate y = \\ln(7x) and y = \\ln(x^7) to see the difference.\n\nCase A: For \\ln(7x), the constant a=7 cancels out:\\frac{dy}{dx} = \\frac{7}{7x} = \\frac{1}{x}\n\nCase B: For \\ln(x^7), the power n=7 remains in the numerator:\\frac{dy}{dx} = \\frac{7x^6}{x^7} = \\frac{7}{x}","type":"content","url":"/basic-differentiation#the-general-natural-logarithmic-function-rule","position":43},{"hierarchy":{"lvl1":"Basic Differentiation","lvl2":"The Differential"},"type":"lvl2","url":"/basic-differentiation#the-differential","position":44},{"hierarchy":{"lvl1":"Basic Differentiation","lvl2":"The Differential"},"content":"Define dx as an arbitrary change in x from its initial value x_0 and dy as the resulting change in y along the tangent line from the initial value of the function y_0 = f(x_0).\n\nThe differential of y=f(x_0) evaluated at x_0 isdy = f'(x_0)\\, dx.\n\nThis represents the change in y along the tangent line at x_0. Graphically, this is shown in \n\nFigure 1.\n\n\n\nFigure 1:Differential\n\nTry these ^^\n\nDifferentials\n\nFind the differential dy for a given change in x, dx, for each function.\n\ny = 7x^2 - 3x + 5\n\ny = 10x - \\dfrac14 x^2\n\ny = -x^2\n\ny = x^3 + 3x - 6\n\nAnswers\n\n1.\\; dy = (14x - 3)\\,dx \\quad\n2.\\; dy = \\left(10 - \\tfrac12 x\\right)dx \\quad\n3.\\; dy = (-2x)\\,dx \\quad\n4.\\; dy = (3x^2 + 3)\\,dx","type":"content","url":"/basic-differentiation#the-differential","position":45},{"hierarchy":{"lvl1":"Basic Differentiation","lvl2":"Taylor Series"},"type":"lvl2","url":"/basic-differentiation#taylor-series","position":46},{"hierarchy":{"lvl1":"Basic Differentiation","lvl2":"Taylor Series"},"content":"A smooth complex function z(x) can be approximated around x=a byf(x)\n=\nz(a)\n+ z'(a)(x-a)\n+ \\frac{1}{2}z''(a)(x-a)^2\n+ \\frac{1}{6}f'''(a)(x-a)^3\n+ \\cdots\n\nThis idea underlies many approximation methods in economics.\n\n\n\nFigure 2:Taylor expansion of a smooth function around a point.\n\nAs shown in \n\nFigure 2 a function z(x) being approximated by three different Taylor polynomials (or Taylor series expansions) centered around the point x=a.\n\nThe simplest approximation perhaps would simply be g(x) = a. This constant-valued function does not work well, especially if we move away from the point a.\n\nA better approximation would be a linear function of the form h(x) = z(a) + b(x-a), where b is some slope. But what would be a good value of b? We saw above that the differential is an equation for the tangent line (or slope) at the point x = a. So, we could argue that the best linear approximation to the function around this point would beh(x) = z(a) + z'(a)(x-a)\n\nwhere z'(a) is the derivative of the function evaluated at x=a.\n\nBut why stop here? We could improve on this. A better approximation could allow for some curvature. The general form would then be, say,  f(x) = z(a) + z'(a) .(x-a) + c.(x-a)^2. Again, we ask, “What would be the best value for c?” The rate of change of the slope of the quadratic approximation should be equal to the rate of change of change of the function at the a. And since the second derivative of z(x) is 2c, then for f''(x) to equal z''(x) at x = a, we need c = 1/2 z''(a). Hence the quadratic approximation to the function aound x = a is:f(x) = z(a) + z'(a)(x-a) + \\frac{1}{2}f''(a)(x-a)^2\n\nExteding the above argument for cubic and higher-degree approximations, we could find the nth-degree approximation to the function z(x), which we could call m(x), around the point x = a ism(x)\n=\n\\frac{z(a)}{0!}\n+ \\frac{z'(a)}{1!}(x-a)\n+ \\frac{z''(a)}{2!}(x-a)^2\n+ \\cdots\n+ \\frac{f^{(n)}(a)}{n!}(x-a)^n\n\nwhere f^{(n)}(a) is the n the derivative of z(x) evaluated at x = a. The function m(x) above is called the n-th degree Taylor expansion series of z(x) evaluated at x=a.\n\nTo sum, z(x) is the original function being approximated (the solid curve). g(x) represents a constant function. h(x) represents the first-order Taylor polynomial, i.e. a straight line that has the same value and slope as z(x) at x=a (or a tangent to z(x) at x=a). The formula for f(x) is f(x) = z(a) + z^{\\prime }(a)(x-a) + \\frac{1}{2} z''(a)(x-a)^2 representing a second-order (quadratic) polynomial-the dashed curve, which matches better the function’s value, slope, and concavity (curvature) at x=a. It is a better approximation of z(x) near x=a than the linear approximation h(x) and of course the constant function g(a). The graph demonstrates that as more terms are included in the Taylor polynomial, the approximation of the original function becomes more accurate over a larger range around the center point x=a.\n\nExample\n\nFor example, consider the functiony = e^{x/2} - e^{-x/2},\n\nexpanded around the point x = 2.\n\nLinear approximation\n\nThe linear approximation to this function ish(x)\n= \\bigl(e^{1} - e^{-1}\\bigr)\n+ \\left(\\tfrac12\\bigl(e^{1} + e^{-1}\\bigr)\\right)(x - 2).\n\nQuadratic approximation\n\nThe quadratic approximation isj(x)\n= \\bigl(e^{1} - e^{-1}\\bigr)\n+ \\left(\\tfrac12\\bigl(e^{1} + e^{-1}\\bigr)\\right)(x - 2)\n+ \\left(\\tfrac18\\bigl(e^{1} - e^{-1}\\bigr)\\right)(x - 2)^2.","type":"content","url":"/basic-differentiation#taylor-series","position":47},{"hierarchy":{"lvl1":"Basic Differentiation","lvl2":"Implicit Differentiation"},"type":"lvl2","url":"/basic-differentiation#implicit-differentiation","position":48},{"hierarchy":{"lvl1":"Basic Differentiation","lvl2":"Implicit Differentiation"},"content":"Let’s consider a very simple function,xy = 7.\n\nHere, possible solutions include (x,y)=(1,7), (7,1), and so on.If we want to find the slope of this function, we can differentiate it.\n\nFinding y'\n\nTo find y', we proceed as follows.\n\n(a) We make the main assumption that y is a function of x, i.e. y=f(x).We then differentiate both sides of the equation with respect to x.\n\nHence, we obtain\\frac{d}{dx}[x f(x)] = 0.\n\nUsing the product rule, this gives1\\cdot f(x) + x f'(x) = 0.\n\nEquivalently,y + x y' = 0.\n\n(b) Solving the resulting equation for y' givesy' = -\\frac{y}{x}.\n\nSo, if we substitute, for example, x=1 and y=5, we obtain the slope of the function at that point:y' = -5.","type":"content","url":"/basic-differentiation#implicit-differentiation","position":49},{"hierarchy":{"lvl1":"Basic Differentiation","lvl2":"Inverse Function Rule for Implicit Functions"},"type":"lvl2","url":"/basic-differentiation#inverse-function-rule-for-implicit-functions","position":50},{"hierarchy":{"lvl1":"Basic Differentiation","lvl2":"Inverse Function Rule for Implicit Functions"},"content":"We can show that\\frac{dy}{dx} = -\\frac{f_x}{f_y}.\n\nThat is, if we have an implicit function written asf(x,y) = 0,\n\nthen the derivative of y with respect to x can be obtained by:\n\ndifferentiating f with respect to x to obtain f_x,\n\ndifferentiating f with respect to y to obtain f_y,\n\ntaking the ratio -\\dfrac{f_x}{f_y}.\n\nThis gives the derivative of the implicit function y with respect to x.\n\nIt often feels like magic — but it is simply a consequence of the chain rule.\n\nWhy this works\n\nSince f(x,y)=0 holds along the curve, differentiating both sides with respect to x\nand solving for y' naturally leads to the ratio -\\dfrac{f_x}{f_y}.","type":"content","url":"/basic-differentiation#inverse-function-rule-for-implicit-functions","position":51},{"hierarchy":{"lvl1":"Basic Differentiation","lvl2":"Some Uses of Differentiation in Economics"},"type":"lvl2","url":"/basic-differentiation#some-uses-of-differentiation-in-economics","position":52},{"hierarchy":{"lvl1":"Basic Differentiation","lvl2":"Some Uses of Differentiation in Economics"},"content":"Some common applications of differentiation include:\n\nIncreasing and decreasing functions\n\nRelative extrema (maximum or minimum)\n\nInflection points\n\nOptimization of functions\n\netc.","type":"content","url":"/basic-differentiation#some-uses-of-differentiation-in-economics","position":53},{"hierarchy":{"lvl1":"Basic Differentiation","lvl3":"A CES production function example","lvl2":"Some Uses of Differentiation in Economics"},"type":"lvl3","url":"/basic-differentiation#a-ces-production-function-example","position":54},{"hierarchy":{"lvl1":"Basic Differentiation","lvl3":"A CES production function example","lvl2":"Some Uses of Differentiation in Economics"},"content":"Given the CES production functionQ = A\\bigl[\\alpha K^{-\\beta} + (1-\\alpha)L^{-\\beta}\\bigr]^{-1/\\beta},\n\nwe can show that the elasticity of substitution is constant, as follows.\n\nFirst-order conditions\n\nThe first-order conditions require that\\frac{\\partial Q / \\partial L}{\\partial Q / \\partial K}\n=\n\\frac{P_L}{P_K}.\n\nUsing the generalized power function rule, we take the first-order partial derivatives.\n\nFor labor,\\frac{\\partial Q}{\\partial L}\n=\n-\\frac{1}{\\beta}\nA\\bigl[\\alpha K^{-\\beta} + (1-\\alpha)L^{-\\beta}\\bigr]^{-(1/\\beta+1)}\n(-\\beta)(1-\\alpha)L^{-\\beta-1}.\n\nCanceling the -\\beta terms, rearranging (1-\\alpha), and adding the exponents\n-(1/\\beta)-1, we obtain\\frac{\\partial Q}{\\partial L}\n=\n(1-\\alpha)A\n\\bigl[\\alpha K^{-\\beta} + (1-\\alpha)L^{-\\beta}\\bigr]^{-(1+\\beta)/\\beta}\nL^{-(1+\\beta)}.\n\nSubstituting A^{1+\\beta}/A^{\\beta}=A, we can write\\frac{\\partial Q}{\\partial L}\n=\n(1-\\alpha)\\frac{A^{1+\\beta}}{A^\\beta}\n\\bigl[\\alpha K^{-\\beta} + (1-\\alpha)L^{-\\beta}\\bigr]^{-(1+\\beta)/\\beta}\nL^{-(1+\\beta)}.\n\nFrom the CES production function,A^{1+\\beta}\n\\bigl[\\alpha K^{-\\beta} + (1-\\alpha)L^{-\\beta}\\bigr]^{-(1+\\beta)/\\beta}\n=\nQ^{1+\\beta},\n\nandL^{-(1+\\beta)} = \\frac{1}{L^{1+\\beta}}.\n\nThus,\\frac{\\partial Q}{\\partial L}\n=\n\\frac{1-\\alpha}{A^\\beta}\n\\left(\\frac{Q}{L}\\right)^{1+\\beta}.\n\nThe marginal product of capital\n\nSimilarly,\\frac{\\partial Q}{\\partial K}\n=\n\\frac{\\alpha}{A^\\beta}\n\\left(\\frac{Q}{K}\\right)^{1+\\beta}.\n\nDividing the two equations and equating the result to P_L/P_K (from the FOC)\nleads to the cancellation of A^\\beta and Q:\\frac{1-\\alpha}{\\alpha}\n\\left(\\frac{K}{L}\\right)^{1+\\beta}\n=\n\\frac{P_L}{P_K}.\n\nRearranging,\\left(\\frac{K}{L}\\right)^{1+\\beta}\n=\n\\frac{\\alpha}{1-\\alpha}\n\\frac{P_L}{P_K},\n\nand therefore,\\frac{K}{L}\n=\n\\left(\\frac{\\alpha}{1-\\alpha}\\right)^{1/(1+\\beta)}\n\\left(\\frac{P_L}{P_K}\\right)^{1/(1+\\beta)}.\n\nElasticity of substitution\n\nSince \\alpha and \\beta are constants, we can treat K/L as a function of\nP_L/P_K.\n\nLeth = \\left(\\frac{\\alpha}{1-\\alpha}\\right)^{1/(1+\\beta)}.\n\nThen\\frac{K}{L} = h\\left(\\frac{P_L}{P_K}\\right)^{1/(1+\\beta)}.\n\nThe marginal function is\\frac{d(K/L)}{d(P_L/P_K)}\n=\n\\frac{h}{1+\\beta}\n\\left(\\frac{P_L}{P_K}\\right)^{1/(1+\\beta)-1}.\n\nThe average function is\\frac{K/L}{P_L/P_K}\n=\nh\\left(\\frac{P_L}{P_K}\\right)^{1/(1+\\beta)-1}.\n\nDividing the marginal function by the average function, we obtain the elasticity\nof substitution:\\text{MRS}\n=\n\\frac{d(K/L)}{d(P_L/P_K)} \\Big/ \\frac{K/L}{P_L/P_K}\n=\n\\frac{1}{1+\\beta}.\n\nThis is constant, hence the CES production function exhibits constant elasticity of substitution.\n\nInterpretation\n\nIf -1 < \\beta < 0, then MRS > 1.\n\nIf \\beta = 0, then MRS = 1 (Cobb–Douglas case).\n\nIf 0 < \\beta < \\infty, then MRS < 1.\n\nIntuition: elasticity of substitution\n\nThe elasticity of substitution measures how easily a firm can substitute labor for capital when their relative prices change. In a CES production function, this elasticity is constant: it does not depend on the levels of K, L, or output. When the elasticity is high, firms can adjust input combinations easily in response to wage or rental-rate changes; when it is low, substitution is difficult and input proportions are relatively rigid. The parameter \\beta governs this flexibility: values of \\beta close to zero imply unit elasticity (the Cobb–Douglas case), while larger values of \\beta imply more limited substitution.\n\nA function need not be defined at the point a in order to have a limit as x \\to a.\nFor example,f(x) = \\frac{x^2 - 1}{x - 1}\n\nis not defined at x = 1, but\\lim_{x \\to 1} f(x) = 2.\n\nThe two functions discussed above are not continuous.\nThe first is not continuous because f(a) is not defined.\nThe second is not continuous because f does not converge to a limit as x \\to a.\nFor example, iff(x) =\n\\begin{cases}\n-1, & x < 0, \\\\\n1, & x > 0,\n\\end{cases}\n\nthen f has no limit as x \\to 0, since the right-hand limit equals 1 while the left-hand limit equals -1.","type":"content","url":"/basic-differentiation#a-ces-production-function-example","position":55},{"hierarchy":{"lvl1":"Math Notes"},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":"Math Notes"},"content":"These notes build intuition for core tools used in economics, data science, and optimization.\n\nTopics (in progress)\n\nLinear Algebra\n\nCalculus\n\nDifference & Differential Equations\n\nNote\n\nThis is a living document. I will revise and expand as the course evolves.","type":"content","url":"/","position":1},{"hierarchy":{"lvl1":"Linear Algebra: Basics"},"type":"lvl1","url":"/linear-algebra-basic","position":0},{"hierarchy":{"lvl1":"Linear Algebra: Basics"},"content":"","type":"content","url":"/linear-algebra-basic","position":1},{"hierarchy":{"lvl1":"Linear Algebra: Basics","lvl2":"Vector"},"type":"lvl2","url":"/linear-algebra-basic#vector","position":2},{"hierarchy":{"lvl1":"Linear Algebra: Basics","lvl2":"Vector"},"content":"A vector is a mathematical quantity that has both magnitude (or size) and direction.\n\nGeometrically, a vector is represented as a directed line segment, like an arrow, where the length signifies the magnitude and the arrowhead indicates the direction.\n\nMore conveniently, you may write a vector as \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} or simply \\vec{v}. Can you tell what the magnitude (or length or norm) of the above vector is?\n\nAnother way to represent a vector is by using basis vectors, i.e. \\hat{\\imath} and \\hat{\\jmath}, where \\hat{\\imath} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} and \\hat{\\jmath} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}. Hence the vector \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} can be written as \\vec{v} = 1 \\hat{\\imath} + 2 \\hat{\\jmath}.\n\nMake sure you know how to add, subtract vectors, and also multiply/scale vectors by some scalar.\n\nTry these ^^\n\n(1) Express the above two vectors as column vectors and in \\hat{\\imath}, \\hat{\\jmath} notation.\n\n(2) Find also:\n\n(a) \\vec{v} + \\vec{w}    (b) \\vec{v} - \\vec{w}   \n(c) \\vec{v} + 2\\vec{w}    (d) \\vec{v} - 2\\vec{w}\n\n(3) Consider the following vectors:\\vec{u} = \\begin{bmatrix} -1 \\\\ 2 \\\\ 3 \\end{bmatrix}, \\quad\n\\vec{v} = \\begin{bmatrix} -1 \\\\ 2 \\\\ 3 \\end{bmatrix}, \\quad\n\\vec{w} = \\begin{bmatrix} 4 \\\\ 5 \\\\ -2 \\end{bmatrix}\n\nFind:\n\n(a) \\vec{u} + \\vec{v}     (b) -\\vec{u} + 2\\vec{v} - \\vec{w}(c) \\vec{u} \\cdot \\vec{v} (i.e. the dot product)    (d) \\vec{u}^T \\vec{v}(e) Which of the above vectors are orthogonal?(f) Express each of the vectors in \\hat{\\imath}, \\hat{\\jmath}, \\hat{k} notation.","type":"content","url":"/linear-algebra-basic#vector","position":3},{"hierarchy":{"lvl1":"Linear Algebra: Basics","lvl2":"Matrix Basics"},"type":"lvl2","url":"/linear-algebra-basic#matrix-basics","position":4},{"hierarchy":{"lvl1":"Linear Algebra: Basics","lvl2":"Matrix Basics"},"content":"We have already introduced the basis vectors \\hat{\\imath} and \\hat{\\jmath}, which we can put into a matrix. That is, the \\hat{\\imath} and \\hat{\\jmath} vectors placed side-by-side in a 2 \\times 2 matrix:\\begin{bmatrix} \n1 & 0 \\\\ \n0 & 1 \n\\end{bmatrix}\n\nNow suppose we want to rotate \\vec{v} = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} anticlockwise by 90°. Where will it go?\n\nWe can rotate the 2-D space, such that \\hat{\\imath} will go to \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} and \\hat{\\jmath} will go to \\begin{bmatrix} -1 \\\\ 0 \\end{bmatrix}. Combining these into a matrix gives:\\begin{bmatrix} 0 & -1 \\\\ 1 & 0 \\end{bmatrix}\n\nThis can easily be done by pre-multiplying the vector by the transformation matrix, as follows:\\begin{bmatrix} 0 & -1 \\\\ 1 & 0 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} = \\begin{bmatrix} -2 \\\\ 1 \\end{bmatrix}\n\nBasically, a matrix can be viewed as a way to transform/change a vector!\n\nMatrix and vector multiplication\n\nThe proper way to multiply a matrix and a vector is:1 \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} + 2 \\begin{bmatrix} -1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} -2 \\\\ 1 \\end{bmatrix}\n\nMatrix addition and subtraction: If \\mathbf{A} = (a_{ij}) and \\mathbf{B} = (b_{ij}) are two m \\times n matrices of same dimension, then \\mathbf{A} + \\mathbf{B} is defined as (a_{ij} + b_{ij}). That is, we add element by element the two matrices.Clearly \\mathbf{A} + \\mathbf{B} = \\mathbf{B} + \\mathbf{A}. The rule applies to matrix subtraction.\n\nTranspose of a matrix: If \\mathbf{A} is an m \\times n matrix, then \\mathbf{A}' is the n \\times m matrix whose rows are the columns of \\mathbf{A}. So \\mathbf{A}' = (a_{ji}). For example:\\begin{bmatrix}\na & b \\\\\nc & d\n\\end{bmatrix} = \\begin{bmatrix}\na & c \\\\\nb & d\n\\end{bmatrix}\n\nNote: (\\mathbf{A} + \\mathbf{B})' = \\mathbf{A}' + \\mathbf{B}', however (\\mathbf{A}\\mathbf{B})' = \\mathbf{B}'\\mathbf{A}'.\n\nNull matrix: Has all elements as 0. Clearly \\mathbf{A} + \\mathbf{0}_{m,n} = \\mathbf{0}_{m,n} + \\mathbf{A} = \\mathbf{A} for all m \\times n matrices.\n\nScalar multiplication: If \\mathbf{A} = (a_{ij}) then for any constant k, define k\\mathbf{A} = (k a_{ij}). That is, multiply each element in \\mathbf{A} by k.\n\nMatrix multiplication: Say \\mathbf{A} is m \\times n, and \\mathbf{B} is n \\times p, then the m \\times p matrix \\mathbf{A}\\mathbf{B} is the product of \\mathbf{A} and \\mathbf{B}. For example:\\begin{bmatrix}\na & b \\\\\nc & d\n\\end{bmatrix}\n\\begin{bmatrix}\ne & f \\\\\ng & h\n\\end{bmatrix}\n= \\begin{bmatrix}\nae + bg & af + bh \\\\\nce + dg & cf + dh\n\\end{bmatrix}\n\nand the matrices \\mathbf{A} and \\mathbf{B} are conformable.\n\nFro example, given the matrices:A = \\begin{bmatrix} 1 & 2 \\\\ 3 & -4 \\end{bmatrix}, \\quad B = \\begin{bmatrix} 5 & 0 \\\\ -6 & 7 \\end{bmatrix}\n\nTo multiply A and B, we apply the row-by-column rule:\\begin{align*}\n\\begin{bmatrix} 1 & 2 \\\\ 3 & -4 \\end{bmatrix}\n\\begin{bmatrix} 5 & 0 \\\\ -6 & 7 \\end{bmatrix}\n&= \\begin{bmatrix}\n(1 \\times 5) + (2 \\times -6) & (1 \\times 0) + (2 \\times 7) \\\\\n(3 \\times 5) + (-4 \\times -6) & (3 \\times 0) + (-4 \\times 7)\n\\end{bmatrix} \\\\\n\n&= \\begin{bmatrix}\n5 - 12 & 0 + 14 \\\\\n15 + 24 & 0 - 28\n\\end{bmatrix} = \\begin{bmatrix}\n-7 & 14 \\\\\n39 & -28\n\\end{bmatrix}\n\\end{align*}\n\nLong story!!\n\nMatrix Multiplication: The Column-Wise (Basis Vector) Method\n\nInstead of dot products, we can view AB as taking the columns of A (the transformed \\hat{\\imath} and \\hat{\\jmath}) and scaling them by the components in each column of B.\n\nFrom the sam eexample above, let the columns of A be:\\vec{a}_1 = \\begin{bmatrix} 1 \\\\ 3 \\end{bmatrix}, \\quad \\vec{a}_2 = \\begin{bmatrix} 2 \\\\ -4 \\end{bmatrix}\n\nFinding Column 1 of the Result:\n\nWe use the first column of B \\begin{bmatrix} 5 \\\\ -6 \\end{bmatrix} as scalars:5\\begin{bmatrix} 1 \\\\ 3 \\end{bmatrix} + (-6)\\begin{bmatrix} 2 \\\\ -4 \\end{bmatrix} = \\begin{bmatrix} 5 \\\\ 15 \\end{bmatrix} + \\begin{bmatrix} -12 \\\\ 24 \\end{bmatrix} = \\begin{bmatrix} -7 \\\\ 39 \\end{bmatrix}\n\nFinding Column 2 of the Result:\n\nWe use the second column of B \\begin{bmatrix} 0 \\\\ 7 \\end{bmatrix} as scalars:0\\begin{bmatrix} 1 \\\\ 3 \\end{bmatrix} + 7\\begin{bmatrix} 2 \\\\ -4 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} + \\begin{bmatrix} 14 \\\\ -28 \\end{bmatrix} = \\begin{bmatrix} 14 \\\\ -28 \\end{bmatrix}\n\nFinal Result:AB = \\begin{bmatrix} -7 & 14 \\\\ 39 & -28 \\end{bmatrix}\n\nas before!\n\nTry these ^^\n\n(1) For the following matrices:A = \\begin{bmatrix} 1 & 2 \\\\ 3 & -4 \\end{bmatrix}, \\quad\nB = \\begin{bmatrix} 5 & 0 \\\\ -6 & 7 \\end{bmatrix},C = \\begin{bmatrix} 1 & -3 & 4 \\\\ 2 & 6 & -5 \\end{bmatrix}, \\quad\nD = \\begin{bmatrix} 3 & 7 & -1 \\\\ 4 & -8 & 9 \\end{bmatrix}.\n\nFind:\n\n(a) 5A - 2B    (b)  2A + 3B    (c)  2C - 3D(d)  AB and (AB)C    (e) BC and A(BC) [Note that (AB)C = A(BC)](f) A^2 and A^3    (g) AD and BD(h) C^T D [Note that we cannot get CD. Why?](i) A'    (j) B'    (k) A' B'    (l) (AB)' [Note that A' B' \\neq (AB)']\n\nDiagonal matrix: A square n \\times n matrix \\mathbf{A} is diagonal if all entries off the ‘main diagonal’ are zero, i.e.\\mathbf{A} = \\begin{bmatrix}\na_{11} & 0 & \\cdots & 0 \\\\\n0 & a_{22} & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & a_{nn}\n\\end{bmatrix}\n\nNote: The elements in the diagonal of \\mathbf{A} are the same as those in \\mathbf{A}'.\n\nTrace of a square matrix: If \\mathbf{A} is a square matrix, the trace of \\mathbf{A}, denoted \\operatorname{tr}(\\mathbf{A}), is the sum of the elements on the main/leading diagonal of \\mathbf{A}.\n\nIdentity matrix: Denoted by \\mathbf{I}_n, the n \\times n diagonal matrix with a_{ii} = 1 for all i. That is:\\mathbf{I}_n = \\begin{bmatrix}\n1 & 0 & \\cdots & 0 \\\\\n0 & 1 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & 1\n\\end{bmatrix}\n\nSymmetric matrix: A square matrix \\mathbf{A} is symmetric if \\mathbf{A} = \\mathbf{A}'. The identity matrix and the square null matrix are symmetric.\n\nIdempotent matrix: A square matrix is said to be idempotent if \\mathbf{A}^n = \\cdots = \\mathbf{A}^2 = \\mathbf{A}. Below is an example (try squaring the matrix and see what you get).\\begin{bmatrix}\n1 & 0 \\\\\n0 & 0\n\\end{bmatrix}\n\nSingular Matrix: A matrix that is linearly dependent (in the columns or rows) is singular and has determinant of zero. Note that any two (or three) vectors \\mathbf{x} and \\mathbf{y} (and \\mathbf{z}) are said to be linearly dependent iff one can be written as a scalar multiple of the other. Two vectors \\mathbf{x} \\neq \\mathbf{0} and \\mathbf{y} \\neq \\mathbf{0} are said to be linearly independent iff the ONLY solution to a\\mathbf{x} + b\\mathbf{y} = \\mathbf{0} is a = 0 AND b = 0. And \\mathbf{x} and \\mathbf{y} are said to be orthogonal.\n\nTry these ^^\n\nFind:(a) \\quad \n\\begin{bmatrix} 1 & 1 \\\\ 0 & 1 \\end{bmatrix}^2 \\quad \\text{and} \\quad \\begin{bmatrix} 1 & 1 \\\\ 0 & 1 \\end{bmatrix}^3(b) \\quad \n\\begin{bmatrix} 0 & 0 \\\\ 0 & 1 \\end{bmatrix}^2 \\quad \\text{and} \\quad \\begin{bmatrix} 0 & 0 \\\\ 0 & 1 \\end{bmatrix}^3","type":"content","url":"/linear-algebra-basic#matrix-basics","position":5},{"hierarchy":{"lvl1":"Linear Algebra: Basics","lvl3":"Some Notes on Matrices","lvl2":"Matrix Basics"},"type":"lvl3","url":"/linear-algebra-basic#some-notes-on-matrices","position":6},{"hierarchy":{"lvl1":"Linear Algebra: Basics","lvl3":"Some Notes on Matrices","lvl2":"Matrix Basics"},"content":"Usually \\mathbf{A}\\mathbf{B} \\neq \\mathbf{B}\\mathbf{A}. For example, try \\mathbf{A} = \\begin{bmatrix}2 & 0 \\\\ 3 & 1\\end{bmatrix} and \\mathbf{B} = \\begin{bmatrix}0 & 1 \\\\ 0 & 0\\end{bmatrix}.\n\nWe can have \\mathbf{A}\\mathbf{B} = \\mathbf{0} even if \\mathbf{A} or \\mathbf{B} are not zero. For example, try \\mathbf{A} = \\begin{bmatrix}6 & -12 \\\\ -3 & 6\\end{bmatrix} and \\mathbf{B} = \\begin{bmatrix}12 & 6 \\\\ 6 & 3\\end{bmatrix}.\n\nSay, \\mathbf{A} = \\begin{bmatrix}4 & 8 \\\\ 1 & 2\\end{bmatrix}, \\mathbf{B} = \\begin{bmatrix}2 & 1 \\\\ 2 & 2\\end{bmatrix}, and \\mathbf{C} = \\begin{bmatrix}-2 & 1 \\\\ 4 & 2\\end{bmatrix}. We find that \\mathbf{A}\\mathbf{B} = \\mathbf{A}\\mathbf{C} even though \\mathbf{B} \\neq \\mathbf{C}.\n\nSay \\mathbf{A} and \\mathbf{B} are singular matrices. Then \\mathbf{A}\\mathbf{B} will not be zero, although the product will be a singular matrix.","type":"content","url":"/linear-algebra-basic#some-notes-on-matrices","position":7},{"hierarchy":{"lvl1":"Linear Algebra: Basics","lvl2":"Systems of Equations in Matrix Form"},"type":"lvl2","url":"/linear-algebra-basic#systems-of-equations-in-matrix-form","position":8},{"hierarchy":{"lvl1":"Linear Algebra: Basics","lvl2":"Systems of Equations in Matrix Form"},"content":"One of the uses of linear algebra in economics is to represent and then solve systems of equations. For instance, consider the system of two equations:2x_1 + x_2 = 5 \\\\\n-x_1 + x_2 = 2\n\nHere, we can define:\\mathbf{A} = \\begin{bmatrix} 2 & 1 \\\\ -1 & 1 \\end{bmatrix}, \\quad \\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}, \\quad \\mathbf{b} = \\begin{bmatrix} 5 \\\\ 2 \\end{bmatrix}\n\nThen we see that:\\mathbf{A}\\mathbf{x} = \\begin{bmatrix} 2 & 1 \\\\ -1 & 1 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} = \\begin{bmatrix} 2x_1 + x_2 \\\\ -x_1 + x_2 \\end{bmatrix}\n\nThe original system is in fact equivalent to the matrix form:\\mathbf{A}\\mathbf{x} = \\mathbf{b}","type":"content","url":"/linear-algebra-basic#systems-of-equations-in-matrix-form","position":9},{"hierarchy":{"lvl1":"Linear Algebra: Basics","lvl2":"Matrix Row Operations"},"type":"lvl2","url":"/linear-algebra-basic#matrix-row-operations","position":10},{"hierarchy":{"lvl1":"Linear Algebra: Basics","lvl2":"Matrix Row Operations"},"content":"Matrix row operations, also known as elementary row operations, are three basic actions performed on a matrix: swapping two rows, multiplying a row by a non-zero constant, and adding a multiple of one row to another row.\n\nInterchanging two rows (Row Swapping): You can swap the positions of any two rows in a matrix. As we will see later, this operation is useful for changing the order of equations in a system without altering the solution.\n\nMultiplying a row by a non-zero constant (Scalar Multiplication): You can multiply every element in a specific row by any non-zero number. This is equivalent to multiplying both sides of an equation by a constant.\n\nAdding a multiple of one row to another row (Row Addition): You can multiply one row by a constant and then add the result to another row. The original row and the row being multiplied remain unchanged. This operation is often the most powerful for simplifying systems, as it corresponds to adding a modified version of one equation to another.\n\nDefinition: REF (Row Echelon Form)\n\nA matrix is in Row Echelon Form if...\n\nEvery non-zero row begins with a leading one.\n\nA leading one in a lower row is further to the right.\n\nZero rows are at the bottom of the matrix.\n\nNote: In some books, leading by one is not required.\n\nDefinition: RREF Reduced Row Echelon Form\n\nA matrix is in Reduced Row Echelon Form if...\n\nEvery non-zero row begins with a leading one.\n\nA leading one in a lower row is further to the right.\n\nZero rows are at the bottom of the matrix.\n\nEvery column with a leading one has zeros elsewhere.\n\nWhile there can exist several row echelon forms for a matrix, there is only one (a unique) reduced row echelon form.\n\nTry these ^^\n\nFor the following matrices:A = \\begin{bmatrix} 2 & 0 & 0 \\\\ 0 & 3 & 0 \\\\ 0 & 0 & 5 \\end{bmatrix}, \\quad \nB = \\begin{bmatrix} 7 & 0 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & -4 \\end{bmatrix},C = \\begin{bmatrix} 2 & 1 & 2 \\\\ 0 & 3 & 0 \\\\ 0 & 0 & 5 \\end{bmatrix}, \\quad \nD = \\begin{bmatrix} 7 & 0 & 0 \\\\ -2 & 0 & 0 \\\\ 3 & 1 & -4 \\end{bmatrix}.\n\nFind:\n\n(a) Is the matrix A in REF or RREF? What are its pivots?(b) How about matrix B and C? Explain.(c) Change all matrices A to D to RREF (if they aren’t in RREF yet) and determine their rank.\n\nWe can solve system of equation by row operations.\n\nTo solve for x_1 and x_2, we can use an augmented matrix and perform elementary row operations—swapping rows, scalar multiplication, and row addition—to reach Reduced Row Echelon Form (RREF).\n\n1. Set up the augmented matrix:\\begin{bmatrix} 2 & 1 & | & 5 \\\\ -1 & 1 & | & 2 \\end{bmatrix}\n\n2. Create a leading one in Row 1:\n\nWe can swap R_1 and R_2:\\begin{bmatrix} -1 & 1 & | & 2 \\\\ 2 & 1 & | & 5 \\end{bmatrix}\n\nThen multiply R_1 by -1 to get a leading one (R_1 \\leftarrow -1 \\cdot R_1):\\begin{bmatrix} 1 & -1 & | & -2 \\\\ 2 & 1 & | & 5 \\end{bmatrix}\n\n3. Create a zero below the leading one:\n\nAdd -2 times R_1 to R_2 (R_2 \\leftarrow R_2 - 2R_1):\\begin{bmatrix} 1 & -1 & | & -2 \\\\ 0 & 3 & | & 9 \\end{bmatrix}\n\n4. Create a leading one in Row 2:\n\nMultiply R_2 by 1/3 (R_2 \\leftarrow \\frac{1}{3}R_2):\\begin{bmatrix} 1 & -1 & | & -2 \\\\ 0 & 1 & | & 3 \\end{bmatrix}\n\n5. Create a zero above the leading one (RREF):\n\nTo reach reduced row echelon form, every column with a leading one must have zeros elsewhere. Add R_2 to R_1 (R_1 \\leftarrow R_1 + R_2):\\begin{bmatrix} 1 & 0 & | & 1 \\\\ 0 & 1 & | & 3 \\end{bmatrix}\n\nBasically, a matrix can be viewed as a way to transform or change a vector to solve these systems!\n\nFinal Result\n\nThe system provides the unique solution:x_1 = 1, \\quad x_2 = 3\n\nBasically, a matrix can be viewed as a way to transform or change a vector to solve these systems!\n\nNote: We need not reach RREF, and could stop at Step 4 (or even Step 3). At Step 4, the last row tells us that x_2 = 3. We can then use back-substitution into the first row (x_1 - x_2 = -2). HEnce x_1 - 3 = -2 \\implies x_1 = 1.\n\nThis confirms the same result without performing the final row addition.","type":"content","url":"/linear-algebra-basic#matrix-row-operations","position":11},{"hierarchy":{"lvl1":"Linear Algebra: Basics","lvl2":"The Determinant of a Matrix"},"type":"lvl2","url":"/linear-algebra-basic#the-determinant-of-a-matrix","position":12},{"hierarchy":{"lvl1":"Linear Algebra: Basics","lvl2":"The Determinant of a Matrix"},"content":"The determinant is a scalar value uniquely associated with a square non-singular matrix, and is usually denoted as |\\mathbf{A}|. The question of whether or not a matrix is nonsingular and therefore invertible is linked to the value of its determinant.\n\nWe can write a generic 2 \\times 2 matrix as:\\mathbf{A} = \\begin{bmatrix} a_{11} & a_{12} \\\\ a_{21} & a_{22} \\end{bmatrix}\n\nand its determinant is defined as:|\\mathbf{A}| = a_{11}a_{22} - a_{12}a_{21}\n\nFor a 3 \\times 3 matrix, say:\\mathbf{A} = \\begin{bmatrix} a & b & c \\\\ d & e & f \\\\ g & h & i \\end{bmatrix}\n\nits determinant is:|\\mathbf{A}| = a(ei - fh) - b(di - fg) + c(dh - ef)\n\nNote the sign in front of b is begative, because we imppose (multiply by) the sign matrix \\begin{bmatrix} + & - & + \\\\ - & + & - \\\\ + & - & + \\end{bmatrix}.\n\nVisually:\n\nThe ‘yellow’ bits called minors are ‘smaller’ determinants, now 2 by 2 and easier to handle. Note that the elements of the minor come from remaining elements after deleting the rows and columns of the corresponding elements in the selected row (or column).\n\nLet’s take a numerical example.\n\nTo find the determinant of matrix A using Cofactor Expansion along the first row (technically we could pick any row to work with) involves multiplying each element of the first row by its corresponding 2 \\times 2 minor, following the sign pattern: (+ , - , +) for the first row.\n\nGiven:A = \\begin{bmatrix} 2 & -3 & 1 \\\\ 1 & -1 & 2 \\\\ 3 & 1 & -1 \\end{bmatrix}\n\nThe formula for expansion along the first row is:\\det(A) = a(M_{11}) - b(M_{12}) + c(M_{13})\n\n1. First Element (a = 2):+2 \\cdot \\begin{vmatrix} -1 & 2 \\\\ 1 & -1 \\end{vmatrix} = 2 \\cdot [(-1)(-1) - (2)(1)] = 2(1 - 2) = -2\n\n2. Second Element (b = -3):\nNote the negative sign from the checkerboard pattern:-(-3) \\cdot \\begin{vmatrix} 1 & 2 \\\\ 3 & -1 \\end{vmatrix} = 3 \\cdot [(1)(-1) - (2)(3)] = 3(-1 - 6) = -21\n\n3. Third Element (c = 1):+1 \\cdot \\begin{vmatrix} 1 & -1 \\\\ 3 & 1 \\end{vmatrix} = 1 \\cdot [(1)(1) - (-1)(3)] = 1(1 + 3) = 4\n\nFinal Result\n\nSumming the results from each step gives \\det(A) = -2 - 21 + 4 = -19.","type":"content","url":"/linear-algebra-basic#the-determinant-of-a-matrix","position":13},{"hierarchy":{"lvl1":"Linear Algebra: Basics","lvl2":"Properties of Determinants"},"type":"lvl2","url":"/linear-algebra-basic#properties-of-determinants","position":14},{"hierarchy":{"lvl1":"Linear Algebra: Basics","lvl2":"Properties of Determinants"},"content":"Property 1: |\\mathbf{A}| = |\\mathbf{A}'| and |\\mathbf{A}| \\cdot |\\mathbf{B}| = |\\mathbf{B}| \\cdot |\\mathbf{A}|.\n\nProperty 2: Interchanging any two rows or columns will affect the sign of the determinant, but not its absolute value.\n\nProperty 3: Multiplying a single row or column by a scalar will cause the value of the determinant to be multiplied by the scalar.\n\nProperty 4: Addition or subtraction of a nonzero multiple of any row or column to or from another row or column does not change the value of the determinant.\n\nProperty 5: The determinant of a triangular matrix is equal to the product of the elements along the principal diagonal.\n\nProperty 6: If any of the rows or columns equal zero, the determinant is also zero.\n\nProperty 7: If two rows or columns are identical or proportional, i.e. linearly dependent, then the determinant is zero.\n\nNote: Properties 2,3 and 4  are standard row operations already discussed above.","type":"content","url":"/linear-algebra-basic#properties-of-determinants","position":15},{"hierarchy":{"lvl1":"Linear Algebra: Basics","lvl2":"Matrix Inversion"},"type":"lvl2","url":"/linear-algebra-basic#matrix-inversion","position":16},{"hierarchy":{"lvl1":"Linear Algebra: Basics","lvl2":"Matrix Inversion"},"content":"Consider,the following.\\mathbf{A} = \\begin{bmatrix} 1 & -2 \\\\ 3 & 4 \\end{bmatrix}\n\nThen pre-multiplying by:\\begin{bmatrix} 4 & 2 \\\\ -3 & 1 \\end{bmatrix}\n\nGives:\\begin{bmatrix} 4 & 2 \\\\ -3 & 1 \\end{bmatrix} \\begin{bmatrix} 1 & -2 \\\\ 3 & 4 \\end{bmatrix} = \\begin{bmatrix} 10 & 0 \\\\ 0 & 10 \\end{bmatrix} = 10 \\mathbf{I}\n\nWhere does the 10 in the matrix after the = sign come from? It is the determinant of \\mathbf{A}.\n\nThen, (pre-)multiplying both sides of the equation by \\frac{1}{10} gives:\\frac{1}{10} \\begin{bmatrix} 4 & 2 \\\\ -3 & 1 \\end{bmatrix} \\begin{bmatrix} 1 & -2 \\\\ 3 & 4 \\end{bmatrix} = \\mathbf{I}\n\nThe matrix \\frac{1}{10} \\begin{bmatrix} 4 & 2 \\\\ -3 & 1 \\end{bmatrix} is in fact the inverse of \\mathbf{A}.\n\nHence we have the relationship \\mathbf{A}^{-1} \\mathbf{A} = \\mathbf{I}. In fact, \\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{I}.","type":"content","url":"/linear-algebra-basic#matrix-inversion","position":17},{"hierarchy":{"lvl1":"Linear Algebra: Basics","lvl2":"Finding the Inverse of a Matrix by Row Operations"},"type":"lvl2","url":"/linear-algebra-basic#finding-the-inverse-of-a-matrix-by-row-operations","position":18},{"hierarchy":{"lvl1":"Linear Algebra: Basics","lvl2":"Finding the Inverse of a Matrix by Row Operations"},"content":"We can find the inverse of a matrix using row operations or Gauss-Jordan Elimination method. The objective is to transform the matrix A into the Identity matrix I. By applying the same sequence of elementary row operations to an Identity matrix simultaneously, that Identity matrix transforms into A^{-1}.\n\nGiven the matrix A from the example aboveA = \\begin{bmatrix} 1 & -2 \\\\ 3 & 4 \\end{bmatrix}\n\nWe set up an augmented matrix [A | I] by placing the 2 \\times 2 Identity matrix to the right of A:[A | I] = \\begin{bmatrix} 1 & -2 & | & 1 & 0 \\\\ 3 & 4 & | & 0 & 1 \\end{bmatrix}\n\nStep-by-Step Row Operations\n\n1. Create a zero below the first leading one:\n\nSubtract 3 times the first row from the second row (R_2 \\leftarrow R_2 - 3R_1):\\begin{bmatrix} 1 & -2 & | & 1 & 0 \\\\ 0 & 10 & | & -3 & 1 \\end{bmatrix}\n\n2. Create a leading one in the second row:\n\nMultiply the second row by 1/10 (R_2 \\leftarrow \\frac{1}{10}R_2):\\begin{bmatrix} 1 & -2 & | & 1 & 0 \\\\ 0 & 1 & | & -3/10 & 1/10 \\end{bmatrix}\n\n3. Create a zero above the second leading one:\n\nTo reach Reduced Row Echelon Form (RREF), we add 2 times the second row to the first row (R_1 \\leftarrow R_1 + 2R_2):\\begin{bmatrix} 1 & 0 & | & 1 + 2(-3/10) & 0 + 2(1/10) \\\\ 0 & 1 & | & -3/10 & 1/10 \\end{bmatrix}\n\nSimplifying the arithmetic in the first row:\\begin{bmatrix} 1 & 0 & | & 4/10 & 2/10 \\\\ 0 & 1 & | & -3/10 & 1/10 \\end{bmatrix}\n\nFinal Result\n\nThe left side of the augmented matrix has been transformed into the Identity matrix. Therefore, the right side is now the inverse, A^{-1}:A^{-1} = \\begin{bmatrix} 0.4 & 0.2 \\\\ -0.3 & 0.1 \\end{bmatrix}\n\nVerification\n\nA matrix multiplied by its inverse must result in the Identity matrix:\\begin{bmatrix} 1 & -2 \\\\ 3 & 4 \\end{bmatrix}\n\\begin{bmatrix} 0.4 & 0.2 \\\\ -0.3 & 0.1 \\end{bmatrix}\n= \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}","type":"content","url":"/linear-algebra-basic#finding-the-inverse-of-a-matrix-by-row-operations","position":19},{"hierarchy":{"lvl1":"Linear Algebra: Basics","lvl2":"Properties of Inverse"},"type":"lvl2","url":"/linear-algebra-basic#properties-of-inverse","position":20},{"hierarchy":{"lvl1":"Linear Algebra: Basics","lvl2":"Properties of Inverse"},"content":"Here are some properties of the inverse of a matrix worth knowing:\n\nProperty 1: For any nonsingular matrix \\mathbf{A}, (\\mathbf{A}^{-1})^{-1} = \\mathbf{A}.\n\nProperty 2: The determinant of the inverse of a matrix is equal to the reciprocal of the determinant of the matrix. That is |\\mathbf{A}^{-1}| = \\frac{1}{|\\mathbf{A}|}.\n\nProperty 3: The inverse of matrix \\mathbf{A} is unique.\n\nProperty 4: For any nonsingular matrix \\mathbf{A}, the inverse of the transpose of a matrix is equal to the transpose of the inverse of the matrix. (\\mathbf{A}')^{-1} = (\\mathbf{A}^{-1})'.\n\nProperty 5: If \\mathbf{A} and \\mathbf{B} are nonsingular and of the same dimension, then \\mathbf{A}\\mathbf{B} is also nonsingular.\n\nProperty 6: The inverse of the product of two matrices is equal to the product of their inverses in reverse order. (\\mathbf{A}\\mathbf{B})^{-1} = \\mathbf{B}^{-1}\\mathbf{A}^{-1}.","type":"content","url":"/linear-algebra-basic#properties-of-inverse","position":21},{"hierarchy":{"lvl1":"Linear Algebra: Basics","lvl2":"Finding the Inverse of a Matrix (Standard Approach)"},"type":"lvl2","url":"/linear-algebra-basic#finding-the-inverse-of-a-matrix-standard-approach","position":22},{"hierarchy":{"lvl1":"Linear Algebra: Basics","lvl2":"Finding the Inverse of a Matrix (Standard Approach)"},"content":"The inverse of a matrix is defined as:\\mathbf{A}^{-1} = \\frac{1}{|\\mathbf{A}|} \\operatorname{adj}(\\mathbf{A})\n\nLet’s say we have the following linear system:2x_1 - 3x_2 + x_3 = -1 \\\\\nx_1 - x_2 + 2x_3 = -3 \\\\\n3x_1 + x_2 - x_3 = 9\n\nWriting the above system in the form \\mathbf{A}\\mathbf{x} = \\mathbf{b}, where:\\mathbf{A} = \\begin{bmatrix} 2 & -3 & 1 \\\\ 1 & -1 & 2 \\\\ 3 & 1 & -1 \\end{bmatrix}, \\quad \\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix}, \\quad \\mathbf{b} = \\begin{bmatrix} -1 \\\\ -3 \\\\ 9 \\end{bmatrix}\n\nWe can therefore solve the system by \\mathbf{x} = \\mathbf{A}^{-1}\\mathbf{b}, which means that we need the inverse of \\mathbf{A}.\n\nStep 1. The minor |M_{ij}| is the determinant of the submatrix formed by deleting the i-th row and j-th column of the original matrix. So we have:\\begin{bmatrix}\n-1 & -7 & 4 \\\\\n2 & -5 & 11 \\\\\n-5 & 3 & 1\n\\end{bmatrix}\n\nStep 2: The cofactor C_{ij} is a minor with the prescribed sign that follows the rule:C_{ij} = (-1)^{i+j} |M_{ij}|\n\nHence, we have:\\begin{bmatrix}\n-1 & 7 & 4 \\\\\n-2 & -5 & -11 \\\\\n-5 & -3 & 1\n\\end{bmatrix}\n\nStep 3: An adjoint matrix is simply the transpose of a cofactor matrix. Continuing the example above, we have:\\begin{bmatrix}\n-1 & -2 & -5 \\\\\n7 & -5 & -3 \\\\\n4 & -11 & 1\n\\end{bmatrix}\n\nSince |\\mathbf{A}| = -19, we then have:\\mathbf{A}^{-1} = - \\frac{1}{19} \\begin{bmatrix}\n-1 & -2 & -5 \\\\\n7 & -5 & -3 \\\\\n4 & -11 & 1\n\\end{bmatrix}\n\nand\\mathbf{x} = \\mathbf{A}^{-1}\\mathbf{b} = - \\frac{1}{19} \\begin{bmatrix}\n-1 & -2 & -5 \\\\\n7 & -5 & -3 \\\\\n4 & -11 & 1\n\\end{bmatrix} \\begin{bmatrix} -1 \\\\ -3 \\\\ 9 \\end{bmatrix}\n\nwhich gives x_1 = 2, x_2 = 1 and x_3 = -2.\n\nTry these ^^\n\nGiven:A = \\begin{bmatrix} 3 & -4 \\\\ 1 & 2 \\end{bmatrix}, \\quad \nB = \\begin{bmatrix} -2 & 1 \\\\ -4 & 2 \\end{bmatrix},C = \\begin{bmatrix} 2 & 0 & 0 \\\\ 0 & 3 & 0 \\\\ 0 & 0 & 5 \\end{bmatrix}, \\quad \nD = \\begin{bmatrix} 7 & 0 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & -4 \\end{bmatrix},E = \\begin{bmatrix} 2 & 1 & 2 \\\\ 0 & 3 & 0 \\\\ 0 & 0 & 5 \\end{bmatrix}, \\quad \nF = \\begin{bmatrix} 7 & 0 & 0 \\\\ -2 & 0 & 0 \\\\ 3 & 1 & -4 \\end{bmatrix}.\n\nFind:\n\n(a) AB, A^2, B^2    (b) CD, C^2, D^2    (c) EF, E^2, F^2(d) AB, CE and DF    (d) Determinant of all matrices A to F(e) Inverse of all matrices A to F","type":"content","url":"/linear-algebra-basic#finding-the-inverse-of-a-matrix-standard-approach","position":23},{"hierarchy":{"lvl1":"Linear Algebra: Basics","lvl2":"Cramer’s Rule"},"type":"lvl2","url":"/linear-algebra-basic#cramers-rule","position":24},{"hierarchy":{"lvl1":"Linear Algebra: Basics","lvl2":"Cramer’s Rule"},"content":"Cramer came up with a simplified method for solving a system of linear equations through the use of determinants.\n\nBasically, given \\mathbf{A}\\mathbf{x} = \\mathbf{b}, we have:\\mathbf{A} = \\begin{bmatrix} a_{11} & a_{12} & a_{13} \\\\ a_{21} & a_{22} & a_{23} \\\\ a_{31} & a_{32} & a_{33} \\end{bmatrix}, \\quad \\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix}, \\quad \\mathbf{b} = \\begin{bmatrix} b_1 \\\\ b_2 \\\\ b_3 \\end{bmatrix}\n\nDefine:\\Delta = |\\mathbf{A}|\n\nand\\Delta_1 = \\begin{vmatrix} b_1 & a_{12} & a_{13} \\\\ b_2 & a_{22} & a_{23} \\\\ b_3 & a_{32} & a_{33} \\end{vmatrix}, \\quad \\Delta_2 = \\begin{vmatrix} a_{11} & b_1 & a_{13} \\\\ a_{21} & b_2 & a_{23} \\\\ a_{31} & b_3 & a_{33} \\end{vmatrix}, \\quad \\Delta_3 = \\begin{vmatrix} a_{11} & a_{12} & b_1 \\\\ a_{21} & a_{22} & b_2 \\\\ a_{31} & a_{32} & b_3 \\end{vmatrix}\n\nwhere the columns in the \\mathbf{A} matrix are replaced one by one by the \\mathbf{b} vector as shown above.\n\nThen:x_i = \\frac{\\Delta_i}{\\Delta}\n\nSo, for the previous example:\\mathbf{A} = \\begin{bmatrix} 2 & -3 & 1 \\\\ 1 & -1 & 2 \\\\ 3 & 1 & -1 \\end{bmatrix} \\quad \\text{and} \\quad \\det(A) = -19, \\quad \\mathbf{b} = \\begin{bmatrix} -1 \\\\ -3 \\\\ 9 \\end{bmatrix}\n\nIt follows that\\Delta_1 = \\begin{vmatrix} -1 & -3 & 1 \\\\ -3 & -1 & 2 \\\\ 9 & 1 & -1 \\end{vmatrix}, \\quad \\Delta_2 = \\begin{vmatrix} 2 & -1 & 1 \\\\ 1 & -3 & 2 \\\\ 3 & 9 & -1 \\end{vmatrix}, \\quad \\Delta_3 = \\begin{vmatrix} 2 & -3 & -1 \\\\ 1 & -1 & -3 \\\\ 3 & 1 & 9 \\end{vmatrix}\n\nwhere \\Delta_1 = -38, \\Delta_2 = -19, and \\Delta_3 = 38.\n\nSo dividing through, you can easily verify that you get the soluitons \\{2, 1, -2\\} as before!","type":"content","url":"/linear-algebra-basic#cramers-rule","position":25},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices"},"type":"lvl1","url":"/linear-algebra-special","position":0},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices"},"content":"Let’s look at some special matrices and determinants that are useful in economic analysis.","type":"content","url":"/linear-algebra-special","position":1},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices","lvl2":"The Jacobian"},"type":"lvl2","url":"/linear-algebra-special#the-jacobian","position":2},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices","lvl2":"The Jacobian"},"content":"The Jacobian (named after German mathematician Karl Gustav Jacobi, 1804–1851) is generally used in conjunction with partial derivatives to provide an easy test for the existence of functional dependence (linear and nonlinear). A Jacobian determinant |\\mathbf{J}| is composed of all the first‑order partial derivatives of a system of equations, arranged in order sequence. Giveny_{1} = f_{1}(x_{1},x_{2},x_{3}) \\\\\ny_{2} = f_{2}(x_{1},x_{2},x_{3}) \\\\\ny_{3} = f_{3}(x_{1},x_{2},x_{3})\n\nThen|\\mathbf{J}| = \n\\begin{vmatrix}\n\\frac{\\partial y_{1}}{\\partial x_{1}} & \\frac{\\partial y_{1}}{\\partial x_{2}} & \\frac{\\partial y_{1}}{\\partial x_{3}} \\\\[4pt]\n\\frac{\\partial y_{2}}{\\partial x_{1}} & \\frac{\\partial y_{2}}{\\partial x_{2}} & \\frac{\\partial y_{2}}{\\partial x_{3}} \\\\[4pt]\n\\frac{\\partial y_{3}}{\\partial x_{1}} & \\frac{\\partial y_{3}}{\\partial x_{2}} & \\frac{\\partial y_{3}}{\\partial x_{3}}\n\\end{vmatrix}\n\nFor example, say you havey_{1} = 5x_{1} + 3x_{2} \\\\\ny_{2} = 25x_{1}^{2} + 30x_{1}x_{2} + 9x_{2}^{2}\n\nThen taking the first‑order partials gives{\\partial y_1}{\\partial x_1} = 5,\\qquad \n\\frac{\\partial y_1}{\\partial x_2} = 3,\\qquad \n\\frac{\\partial y_2}{\\partial x_1} = 50x_1 + 30x_2,\\qquad \n\\frac{\\partial y_2}{\\partial x_2} = 30x_1 + 18x_2\n\nAnd the Jacobian is|\\mathbf{J}| = \n\\begin{vmatrix}\n5 & 3 \\\\\n50x_1 + 30x_2 & 30x_1 + 18x_2\n\\end{vmatrix}\n\nThe determinant of the Jacobian matrix is|\\mathbf{J}| = 5(30x_1 + 18x_2) - 3(50x_1 + 30x_2) = 0\n\nSince |\\mathbf{J}| = 0, there is functional dependence between the equations.\n\nNote: (5x_{1} + 3x_{2})^{2} = 25x_{1}^{2} + 30x_{1}x_{2} + 9x_{2}^{2}.","type":"content","url":"/linear-algebra-special#the-jacobian","position":3},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices","lvl2":"The Hessian"},"type":"lvl2","url":"/linear-algebra-special#the-hessian","position":4},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices","lvl2":"The Hessian"},"content":"Given that the first‑order conditions z_{x} = z_{y} = 0 are met, a sufficient condition for a multivariate function z = f(x,y) to be an optimum is\n\nz_{xx}, z_{yy} > 0 for a minimum; z_{xx}, z_{yy} < 0 for a maximum.\n\nz_{xx} z_{yy} > (z_{xy})^{2}.\n\nA convenient test for this second‑order condition is provided by the Hessian.A Hessian |\\mathbf{H}| is a determinant composed of all the second‑order partial derivatives, with the second‑order direct partials on the principal diagonal and the second‑order cross partials off the principal diagonal. That is,|\\mathbf{H}| = \n\\begin{vmatrix}\nz_{xx} & z_{xy} \\\\\nz_{yx} & z_{yy}\n\\end{vmatrix}\n\nwhere (by Young’s Theorem) z_{xy} = z_{yx}.\n\nIf the first element on the principal diagonal, a.k.a. the first principal minor, |H_{1}| = z_{xx} is positive, and the second principal minor also|H_{2}| = \n\\begin{vmatrix}\nz_{xx} & z_{xy} \\\\\nz_{yx} & z_{yy}\n\\end{vmatrix}\n= z_{xx} z_{yy} - (z_{xy})^{2} > 0,\n\nthen the second‑order conditions for a minimum are set. That is, when |H_{1}| > 0 and |H_{2}| > 0, the Hessian |\\mathbf{H}| is said to be positive definite. And a positive definite Hessian fulfills the second‑order conditions for a minimum.\n\nIf however the first principal minor |H_{1}| < 0, while the second principal minor remains positive, then the second‑order conditions for a maximum are met. That is, when |H_{1}| < 0 and |H_{2}| > 0, the Hessian |\\mathbf{H}| is said to be negative definite and fulfills the second‑order conditions for a maximum.\n\nLet’s take a numerical example:\n\nGivenz = f(x,y) = 2x^{2} - xy + 2y^{2} - 5x - 6y + 20,\n\nthenz_x = 4x - y - 5,\\quad z_y = -x + 4y - 6,\\quad z_{xx} = 4,\\quad z_{yy} = 4,\\quad z_{xy} = z_{yx} = -1.\n\nThe Hessian is therefore|\\mathbf{H}| = \n\\begin{vmatrix}\n4 & -1 \\\\\n-1 & 4\n\\end{vmatrix}\n\nWe have |H_{1}| = 4 > 0 and |H_{2}| = (4)(4) - (-1)^2 = 16 - 1 = 15 > 0, i.e. both princpal minnores are positive and hence the Hessian is said to be positive definite and the function z is characterized by a minimum at the critical values (can you find these?).","type":"content","url":"/linear-algebra-special#the-hessian","position":5},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices","lvl2":"The Discriminant"},"type":"lvl2","url":"/linear-algebra-special#the-discriminant","position":6},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices","lvl2":"The Discriminant"},"content":"Determinants can be used to test for positive and negative definiteness of any quadratic form. The determinant of a quadratic form is called a discriminant |\\mathbf{D}|. Given the quadratic formz = a x^{2} + b x y + c y^{2},\n\nthe determinant is formed by placing the coefficients of the squared terms on the principal diagonal and dividing the coefficients of the non‑squared terms equally between the off‑diagonal positions. Hence, we have|\\mathbf{D}| = \n\\begin{vmatrix}\na & b/2 \\\\\nb/2 & c\n\\end{vmatrix}\n\nWe then evaluate the principal minors like we did for the Hessian test, where|D_{1}| = a \\quad \\text{and} \\quad |D_{2}| = \n\\begin{vmatrix}\na & b/2 \\\\\nb/2 & c\n\\end{vmatrix}\n= a c - \\frac{b^{2}}{4}.\n\nIf |D_{1}|, |D_{2}| > 0, |\\mathbf{D}| is positive definite and z is positive for all nonzero values of x and y. If |D_{1}| < 0 and |D_{2}| > 0, |\\mathbf{D}| is negative definite and z is negative for all nonzero values of x and y. If |D_{2}| \\neq 0, z is not sign definite and z may assume both positive and negative values.\n\nLet’s take an example to test for sign definiteness of the following quadratic formz = 2x^{2} + 5xy + 8y^{2}.\n\nWe can easily form the discriminant|\\mathbf{D}| = \n\\begin{vmatrix}\n2 & 2.5 \\\\\n2.5 & 8\n\\end{vmatrix}\n\nThen evaluating the principal minors gives|D_{1}| = 2 > 0, \\qquad |D_{2}| = \n\\begin{vmatrix}\n2 & 2.5 \\\\\n2.5 & 8\n\\end{vmatrix}\n= 16 - 6.25 = 9.75 > 0.\n\nHence, z is positive definite, meaning that it will be greater than zero for all nonzero values of x and y.\n\nTry these ^^\n\nBy inspecting its Discriminant, can you tell whether the function below is positive or negative for all nonzero x and y?\n\n(1) f(x,y) = 2x^2 + 5xy + 8y^2(2) f(x,y) = -3x + 4xy - 4y^2(3) f(x,y,z) = 5x^2 - 6xy + 3y^2 - 2yz + 8z^2 - 3xy","type":"content","url":"/linear-algebra-special#the-discriminant","position":7},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices","lvl2":"The Quadratic Form"},"type":"lvl2","url":"/linear-algebra-special#the-quadratic-form","position":8},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices","lvl2":"The Quadratic Form"},"content":"A quadratic form is defined as a polynomial expression in which each component term has a uniform degree.\n\nHere are some examples:6x^{2} - 2xy + 3y^{2} \\quad \\text{is a quadratic form in 2 variables.} \\\\\nx^{2} + 2xy + 4xz + 2yz + y^{2} + z^{2} \\quad \\text{is a quadratic form in 3 variables.}\n\nIt would be useful to determine the sign definiteness so we can make statements concerning the optimum value of the function as to whether it is a minimum or a maximum.\n\nMore generally, a quadratic form in n variables (x_{1},x_{2},\\ldots ,x_{n}) can be written as \\mathbf{x}^{\\prime}\\mathbf{A}\\mathbf{x} where\n\n\\mathbf{x}^{\\prime} is a row vector [x_{1},x_{2},\\dots,x_{n}] and\\mathbf{A} is an n\\times n matrix of scalar elements.\n\nTo see this more clearly, letA = \\begin{bmatrix} 2 & 1 \\\\ 1 & 3 \\end{bmatrix}\n\nThen the quadratic form is:\\begin{aligned}\nQ(x, y) &= \\begin{bmatrix} x & y \\end{bmatrix} \\begin{bmatrix} 2 & 1 \\\\ 1 & 3 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\end{bmatrix} \\\\\n&= \\begin{bmatrix} 2x + y & x + 3y \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\end{bmatrix} \\\\\n&= (2x + y)x + (x + 3y)y \\\\\n&= 2x^2 + 2xy + 3y^2\n\\end{aligned}\n\nFurthermore, completing the square gives:\\begin{aligned}\nQ(x, y) &= 2x^2 + 2xy + 3y^2 = 2(x^2 + xy) + 3y^2 \\\\\n&= 2\\left(x + \\frac{y}{2}\\right)^2 - \\frac{1}{2}y^2 + 3y^2 = 2\\left(x + \\frac{y}{2}\\right)^2 + \\frac{5}{2}y^2\n\\end{aligned}\n\nSince both squared terms have positive coefficients, Q(x, y) > 0 for all (x, y) \\neq (0, 0), the quadratic form is positive definite.\n\nSign Definiteness of Quadratic Form\n\nA quadratic form is said to be:\n\nPositive definite if \\mathbf{x}^{\\prime}\\mathbf{A}\\mathbf{x} > 0 for all \\mathbf{x} \\neq \\mathbf{0}.\n\nPositive semi‑definite if \\mathbf{x}^{\\prime}\\mathbf{A}\\mathbf{x} \\geq 0 and \\exists \\mathbf{x} \\neq \\mathbf{0} such that \\mathbf{x}^{\\prime}\\mathbf{A}\\mathbf{x} = 0.\n\nNegative definite if \\mathbf{x}^{\\prime}\\mathbf{A}\\mathbf{x} < 0 for all \\mathbf{x} \\neq \\mathbf{0}.\n\nNegative semi‑definite if \\mathbf{x}^{\\prime}\\mathbf{A}\\mathbf{x} \\leq 0 and \\exists \\mathbf{x} \\neq \\mathbf{0} such that \\mathbf{x}^{\\prime}\\mathbf{A}\\mathbf{x} = 0.\n\nSign indefinite if it takes both positive and negative values.\n\nQuadratic forms are used in many areas such as:\n\nOptimization: To determine if a critical point is a minimum, maximum, or saddle point (via the Hessian matrix).\n\nGeometry: Describing conic sections (ellipses, hyperbolas) and quadric surfaces.\n\nPhysics: Representing energy functions (e.g., kinetic energy in mechanics).\n\nQuadratic forms are particularly useful in determining the concavity or convexity of a differentiable function.\n\nFor a function y = f(x_{1},x_{2},\\dots,x_{n}), we can form the Hessian consisting of second‑order partial derivatives and construct a quadratic form as \\mathbf{x}^{\\prime}\\mathbf{H}\\mathbf{x}. Then\n\na) The function y is strictly convex if the quadratic form is positive (implying that the Hessian is positive definite, i.e., the determinants of the principal minors are all positive).b) The function y is strictly concave if the quadratic form is negative (that is, the Hessian is negative definite, i.e. the determinants of the principal minors alternate in sign).\n\nLet’s take an example to see this more clearly.\n\nSay we havez = x^{2} + y^{2}.\n\nThenz_{x} = 2x,\\quad z_{xx} = 2,\\quad z_{xy} = 0, \\\\\nz_{y} = 2y,\\quad z_{yx} = 0,\\quad z_{yy} = 2.\n\nBut\\mathbf{x}^{\\prime}\\mathbf{H}\\mathbf{x} = z_{xx}x^{2} + z_{yy}y^{2} + z_{xy}z_{yx}xy = 2x^{2} + 2y^{2} > 0.\n\nHence the function z = x^{2} + y^{2} is characterized by a minimum at its optimal value and is therefore a strictly convex function. And it is easy to see that the Hessian is positive definite (i.e. the value of the function is always positive for any non-zero values of x and y).\n\nHere’s yet another example:\n\nSay we have a quadratic form in 2 variablesz = 8x^{2} + 6xy + 2y^{2}.\n\nThis can be rearranged asz = 8x^{2} + 3xy + 3xy + 2y^{2}.\n\nWe can write this as\\begin{aligned}\n\\mathbf{x}^{\\prime}\\mathbf{H}\\mathbf{x} &= z_{xx}x^{2} + z_{yy}y^{2} + (z_{xy} + z_{yx})xy \\\\\n&= 8x^{2} + 2y^{2} + (3 + 3)xy \\\\\n&= 8x^{2} + 2y^{2} + 6xy\n\\end{aligned}\n\nThe Hessian is\\mathbf{H} = \n\\begin{bmatrix}\n8 & 3 \\\\\n3 & 2\n\\end{bmatrix}\n\nThe principal minors are |H_{1}| = 8 and |H_{2}| = \\begin{vmatrix}\n8 & 3 \\\\\n3 & 2\n\\end{vmatrix} = 16 - 9 = 7. Both are positive so we have the Hessian and the quadratic form as positive definite!","type":"content","url":"/linear-algebra-special#the-quadratic-form","position":9},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices","lvl2":"Higher Order Hessian"},"type":"lvl2","url":"/linear-algebra-special#higher-order-hessian","position":10},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices","lvl2":"Higher Order Hessian"},"content":"Given y = f(x_{1},x_{2},x_{3}), the third‑order Hessian is|\\mathbf{H}| = \n\\begin{vmatrix}\ny_{11} & y_{12} & y_{13} \\\\\ny_{21} & y_{22} & y_{23} \\\\\ny_{31} & y_{32} & y_{33}\n\\end{vmatrix}\n\nwhere the elements are the various second‑order partial derivatives of y:y_{11} = \\frac{\\partial^{2}y}{\\partial x_{1}^{2}},\\quad \ny_{12} = \\frac{\\partial^{2}y}{\\partial x_{2}\\partial x_{1}},\\quad \ny_{23} = \\frac{\\partial^{2}y}{\\partial x_{3}\\partial x_{2}},\\quad \\text{etc.}\n\nConditions for a relative minimum or maximum depend on the signs of the first, second, and third principal minors, respectively. If|H_{1}| > 0,\\quad \n|H_{2}| = \n\\begin{vmatrix}\ny_{11} & y_{12} \\\\\ny_{21} & y_{22}\n\\end{vmatrix} > 0,\\quad \n\\text{and} \\quad |H_{3}| = |\\mathbf{H}| > 0,\n\nthen |\\mathbf{H}| is positive definite and fulfills the second‑order conditions for a minimum.\n\nIf|H_{1}| < 0,\\quad \n|H_{2}| = \n\\begin{vmatrix}\ny_{11} & y_{12} \\\\\ny_{21} & y_{22}\n\\end{vmatrix} > 0,\\quad \n\\text{and} \\quad |H_{3}| = |\\mathbf{H}| < 0,\n\nthen |\\mathbf{H}| is negative definite and fulfills the second‑order conditions for a maximum.\n\nLet’s take an example. Given the function:y = -5x_{1}^{2} + 10x_{1} + x_{1}x_{3} - 2x_{2}^{2} + 4x_{2} + 2x_{2}x_{3} - 4x_{3}^{2}.\n\nThe first order conditions (F.O.C) are\\frac{\\partial y}{\\partial x_1} = y_1 = -10x_1 + 10 + x_3 = 0, \\\\\n\\frac{\\partial y}{\\partial x_2} = y_2 = -4x_2 + 2x_3 + 4 = 0, \\\\\n\\frac{\\partial y}{\\partial x_3} = y_3 = x_1 + 2x_2 - 8x_3 = 0.\n\nwhich can be expressed in matrix form as\\begin{bmatrix}\n-10 & 0 & 1 \\\\\n0 & -4 & 2 \\\\\n1 & 2 & -8\n\\end{bmatrix}\n\\begin{bmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{bmatrix}\n= \n\\begin{bmatrix}\n-10 \\\\ -4 \\\\ 0\n\\end{bmatrix}\n\nUsing Cramer’s rule we get x_1 \\approx 1.04, x_2 \\approx 1.22 and x_3 \\approx 0.43 (please verify this).\n\nTaking the second partial derivatives from the first‑order conditions to create the Hessian,y_{11} = -10,\\quad y_{12} = 0,\\quad y_{13} = 1, \\\\\ny_{21} = 0,\\quad y_{22} = -4,\\quad y_{23} = 2, \\\\\ny_{31} = 1,\\quad y_{32} = 2,\\quad y_{33} = -8.\n\nThus,\\mathbf{H} = \n\\begin{bmatrix}\n-10 & 0 & 1 \\\\\n0 & -4 & 2 \\\\\n1 & 2 & -8\n\\end{bmatrix}\n\nFinally, applying the Hessian test, we have\\begin{aligned}\n|H_1| = -10 < 0, & \\\\\n|H_2| = \\begin{vmatrix} -10 & 0 \\\\ 0 & -4 \\end{vmatrix} = 40 > 0, & \\\\ \n|H_3| = |\\mathbf{H}| = \\begin{vmatrix} -10 & 0 & 1 \\\\ 0 & -4 & 2 \\\\ 1 & 2 & -8 \\end{vmatrix} = -276 < 0. &\n\\end{aligned}\n\nSince the principal minors alternate correctly in sign, the Hessian is negative definite and the function is maximized at x_1 \\approx 1.04, x_2 \\approx 1.22 and x_3 \\approx 0.43.\n\nTry these ^^\n\nFor each equation below find (a) critical values, and (b) the nature of the critical values using the Hessian.\n\n(1) f(x,y) = 3x^2 - xy - 2y^2 - 4x - 6y + 12(2) f(x,y,z) = -5x^2 + 10x + xz -2y^2 _ 4y + 2yz - 4z^2(3) f(x,y,z) = 3x^2 - 5x - xy + 6y^2 - 4y + 2yz + 4z^2 +2z - 3xz","type":"content","url":"/linear-algebra-special#higher-order-hessian","position":11},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices","lvl2":"The Bordered Hessian"},"type":"lvl2","url":"/linear-algebra-special#the-bordered-hessian","position":12},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices","lvl2":"The Bordered Hessian"},"content":"To optimize a function f(x_1, x_2) subject to a constraint g(x_1, x_2) the first‑order conditions can be found by setting up what is known as the Lagrangian function F(x_1, x_2, \\lambda) = f(x_1, x_2) - \\lambda(g(x_1, x_2) - c).\n\nThe second‑order conditions can be expressed in terms of a bordered Hessian |\\bar{\\mathbf{H}}| as|\\bar{\\mathbf{H}}| = \n\\begin{vmatrix}\n0 & g_1 & g_2 \\\\\ng_1 & F_{11} & F_{12} \\\\\ng_2 & F_{21} & F_{22}\n\\end{vmatrix}\n\nor|\\bar{\\mathbf{H}}| = \n\\begin{vmatrix}\n0 & g_1 & g_2 \\\\\ng_1 & f_{11} & f_{12} \\\\\ng_2 & f_{21} & f_{22}\n\\end{vmatrix}\n\nwhich is the usual Hessian bordered by the first derivatives of the constraint with zero on the principal diagonal.\n\nThe order of a bordered principal minor is determined by the order of the principal minor being bordered. Hence |\\bar{H}_2| above represents a second bordered principal minor, because the principal minor being bordered has dimensions 2\\times 2.\n\nFor the bivariate case with a single constraint, we simply look at |\\bar{H}_2|. If this is negative, the bordered Hessian is said to be positive definite and satisfies the second‑order condition for a minimum. However, if it is positive, the bordered Hessian is said to be negative definite, and meets the sufficient conditions for a maximum.\n\nLet’s try optimizing the following objective functionf(x_1, x_2) = 4x_1^2 + 3x_2^2 - 2x_1x_2\n\nsubject tox_1 + x_2 = 56.\n\nSetting up the Lagrangian function, taking the first‑order partials and solving gives x_1^* = 36, x_2^* = 20 (and \\lambda = 348). (You should verify this.)\n\nThe bordered Hessian for this optimization problem is|\\bar{\\mathbf{H}}| = \n\\begin{vmatrix}\n0 & 1 & 1 \\\\\n1 & 8 & -2 \\\\\n1 & -2 & 6\n\\end{vmatrix}\n\nStarting with the second principal minor, we have\\begin{aligned}\n|\\bar{H}_2| = |\\bar{\\mathbf{H}}| &= 0 \\cdot \n\\begin{vmatrix}\n8 & -2 \\\\\n-2 & 6\n\\end{vmatrix}\n- 1 \\cdot \n\\begin{vmatrix}\n1 & -2 \\\\\n1 & 6\n\\end{vmatrix}\n+ 1 \\cdot \n\\begin{vmatrix}\n1 & 8 \\\\\n1 & -2\n\\end{vmatrix} \\\\\n&= - (6+2) + (-2-8) = -8 -10 = -18\n\\end{aligned}\n\nwhich is negative, hence |\\bar{\\mathbf{H}}| is positive definite and we have met sufficient conditions for a minimum.\n\nFor the more general case in which the objective function has say n variables, i.e., f(x_1, \\ldots , x_n) which is subject to some constraint g(x_1, \\ldots , x_n), we can set up the bordered Hessian as|\\bar{\\mathbf{H}}| = \n\\begin{bmatrix}\n0 & g_1 & g_2 & \\dots & g_n \\\\\ng_1 & F_{11} & F_{12} & \\dots & F_{1n} \\\\\ng_2 & F_{21} & F_{22} & \\dots & F_{2n} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\ng_n & F_{n1} & F_{n2} & \\dots & F_{nn}\n\\end{bmatrix}\n\nwhere |\\bar{\\mathbf{H}}| = |\\bar{H}_n| because the n \\times n principal minor is bordered.\n\nIn this case, if all the principal minors are negative, i.e. |\\bar{H}_2|, |\\bar{H}_3|, \\ldots , |\\bar{H}_n| < 0, the bordered Hessian is said to be positive definite and satisfies the second‑order condition for a minimum.\n\nOn the other hand, if the principal minors alternate consistently in sign from positive to negative, i.e. |\\bar{H}_2| > 0, |\\bar{H}_3| < 0, |\\bar{H}_4| > 0 etc., the bordered Hessian is negative definite, and meets the sufficient conditions for a maximum.","type":"content","url":"/linear-algebra-special#the-bordered-hessian","position":13},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices","lvl2":"Input‑Output Analysis"},"type":"lvl2","url":"/linear-algebra-special#input-output-analysis","position":14},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices","lvl2":"Input‑Output Analysis"},"content":"If a_{ij} is a technical coefficient representing the value of input i required to produce one dollar’s worth of product j, the total demand for good i can be expressed asx_{i} = a_{i1}x_{1} + a_{i2}x_{2} + \\dots + a_{in}x_{n} + b_{i}\n\nwhere b_i is the final demand for product i. What is important to realize here is that the total demand for a product consists of that product being the final demand plus that product being an intermediate good required for the production of other products.\n\nIn matrix form we have\\mathbf{X} = \\mathbf{A}\\mathbf{X} + \\mathbf{B}\n\nwhere, for an n sector economy,\\mathbf{X} = \n\\begin{bmatrix}\nx_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n\n\\end{bmatrix},\n\\quad\n\\mathbf{A} = \n\\begin{bmatrix}\na_{11} & a_{12} & \\dots & a_{1n} \\\\\na_{21} & a_{22} & \\dots & a_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{n1} & a_{n2} & \\dots & a_{nn}\n\\end{bmatrix},\n\\quad\n\\mathbf{B} = \n\\begin{bmatrix}\nb_1 \\\\ b_2 \\\\ \\vdots \\\\ b_n\n\\end{bmatrix}.\n\n\\mathbf{A} is called the matrix of technical coefficients.\n\nTo find the output (intermediate and final goods) needed to satisfy demand, all we have to do is to solve for \\mathbf{X}:\\mathbf{X} - \\mathbf{A}\\mathbf{X} = \\mathbf{B} \\quad \\Rightarrow \\quad (\\mathbf{I} - \\mathbf{A})\\mathbf{X} = \\mathbf{B} \\quad \\Rightarrow \\quad \\mathbf{X} = (\\mathbf{I} - \\mathbf{A})^{-1}\\mathbf{B}.\n\nwhere (I-A) is kowns as the Leontief matrix.\n\nThus for a 3-sector economy, we have\\begin{bmatrix}\nx_{1} \\\\\nx_{2} \\\\\nx_{3}\n\\end{bmatrix} = \n\\begin{bmatrix}\n1 - a_{11} & -a_{12} & -a_{13} \\\\\n-a_{21} & 1 - a_{22} & -a_{23} \\\\\n-a_{31} & -a_{32} & 1 - a_{33}\n\\end{bmatrix}^{-1}\n\\begin{bmatrix}\nb_{1} \\\\\nb_{2} \\\\\nb_{n}\n\\end{bmatrix}\n\nSay we are asked to determine total output for three sectors/industries given A and B as below:A = \\begin{bmatrix} 0.3 & 0.4 & 0.1 \\\\ 0.5 & 0.2 & 0.6 \\\\ 0.1 & 0.3 & 0.1 \\end{bmatrix}, \\quad \\text{and} \\quad B = \\begin{bmatrix} 20 \\\\ 10 \\\\ 30 \\end{bmatrix}\n\nSince X = (I - A)^{-1} BI - A = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix} - \\begin{bmatrix} 0.3 & 0.4 & 0.1 \\\\ 0.5 & 0.2 & 0.6 \\\\ 0.1 & 0.3 & 0.1 \\end{bmatrix} = \\begin{bmatrix} 0.7 & -0.4 & -0.1 \\\\ -0.5 & 0.8 & -0.6 \\\\ -0.1 & -0.3 & 0.9 \\end{bmatrix}\n\nAnd taking the inverse(I - A)^{-1} = \\frac{1}{0.151} \\begin{bmatrix} 0.54 & 0.39 & 0.32 \\\\ 0.51 & 0.62 & 0.47 \\\\ 0.23 & 0.25 & 0.36 \\end{bmatrix}\n\nHence,\\begin{aligned}\nX = \\begin{bmatrix} x_{1} \\\\ x_{2} \\\\ x_{3} \\end{bmatrix} &= \\frac{1}{0.151} \\begin{bmatrix} 0.54 & 0.39 & 0.32 \\\\ 0.51 & 0.62 & 0.47 \\\\ 0.23 & 0.25 & 0.36 \\end{bmatrix} \\begin{bmatrix} 20 \\\\ 10 \\\\ 30 \\end{bmatrix} \\\\\n&= \\frac{1}{0.151} \\begin{bmatrix} 24.3 \\\\ 30.5 \\\\ 17.9 \\end{bmatrix} = \\begin{bmatrix} 160.93 \\\\ 201.99 \\\\ 118.54 \\end{bmatrix}\n\\end{aligned}\n\nTry these ^^\n\nDetermine the total demand for industries 1, 2 and 3, given the matrix of technical coefficients A and the final demand vector b below.\\mathbf{A} = \\begin{bmatrix} \n0.2 & 0.3 & 0.2 \\\\ \n0.4 & 0.1 & 0.3 \\\\ \n0.3 & 0.5 & 0.2 \n\\end{bmatrix}, \\quad \nb = \\begin{bmatrix} \n150 \\\\ \n200 \\\\ \n210 \n\\end{bmatrix}","type":"content","url":"/linear-algebra-special#input-output-analysis","position":15},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices","lvl2":"Characteristic Roots and Vectors (Eigenvalues and Eigenvectors)"},"type":"lvl2","url":"/linear-algebra-special#characteristic-roots-and-vectors-eigenvalues-and-eigenvectors","position":16},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices","lvl2":"Characteristic Roots and Vectors (Eigenvalues and Eigenvectors)"},"content":"The sign and definiteness of a Hessian and a quadratic form has been tested by using the principal minors. Sign definiteness can also be tested by using the characteristic roots of a matrix. Given a square matrix \\mathbf{A}, is possible to find a vector \\mathbf{V} \\neq 0 and a scalar \\lambda such that\\mathbf{AV} = \\lambda\\mathbf{V}\n\nthe scalar \\lambda is called the characteristic root, latent value or eigenvalue; and the vector \\mathbf{V} is called the characteristic vector, latent vector or eigenvector. The above can be written as\\mathbf{AV} - \\lambda\\mathbf{V} = 0\n\nwhich can be rearranged so that\\begin{aligned}\n\\mathbf{AV} - \\lambda\\mathbf{IV} &= 0 \\\\\n(\\mathbf{A} - \\lambda\\mathbf{I})\\mathbf{V} &= 0\n\\end{aligned}\n\nwhere \\mathbf{A} - \\lambda\\mathbf{I} is called the characteristic matrix of \\mathbf{A}. Since we have \\mathbf{V} \\neq 0, the characteristic matrix \\mathbf{A} - \\lambda\\mathbf{I} must be singular and thus its determinant is zero.\n\nIf \\mathbf{A} is a 3 \\times 3 matrix, then|\\mathbf{A} - \\lambda\\mathbf{I}| = \n\\begin{vmatrix}\na_{11} - \\lambda & a_{12} & a_{13} \\\\\na_{21} & a_{22} - \\lambda & a_{23} \\\\\na_{31} & a_{32} & a_{33} - \\lambda\n\\end{vmatrix} = 0\n\nWith |\\mathbf{A} - \\lambda\\mathbf{I}| = 0, there will be an infinite number of solutions for \\mathbf{V}. To force a unique solution, the solution may be normalized by requiring of the elements v_i of \\mathbf{V} such that \\sum v_i^2 = 1.\n\nSign Definiteness and Characteristc Roots\n\nFor a square matrix \\mathbf{A} if\n\nAll characteristic roots \\lambda are positive \\Rightarrow \\mathbf{A} is positive definite.\n\nAll \\lambda’s are negative \\Rightarrow \\mathbf{A} is negative definite.\n\nAll \\lambda’s are nonnegative and at least one \\lambda = 0 \\Rightarrow \\mathbf{A} is positive semi‑definite.\n\nAll \\lambda’s are nonpositive and at least one \\lambda = 0 \\Rightarrow \\mathbf{A} is negative semi‑definite.\n\nSome \\lambda’s are positive and some negative \\Rightarrow \\mathbf{A} is sign indefinite.\n\nLet’s take an example. Given a square matrix\\mathbf{A} = \n\\begin{bmatrix}\n-6 & 3 \\\\\n3 & -6\n\\end{bmatrix}\n\nTo find the characteristic roots (eigenvalues) of \\mathbf{A}, we simply set |\\mathbf{A} - \\lambda \\mathbf{I}| = 0:|\\mathbf{A} - \\lambda \\mathbf{I}| = \n\\begin{vmatrix}\n-6 - \\lambda & 3 \\\\\n3 & -6 - \\lambda\n\\end{vmatrix} = 0\n\nThis means(-6 - \\lambda)(-6 - \\lambda) - 9 = 0 \\\\\n\\lambda^2 + 12\\lambda + 27 = 0 \\\\\n(\\lambda + 9)(\\lambda + 3) = 0 \\\\\n\\lambda = -9, -3.\n\nSince both characteristic roots \\lambda are negative, we say \\mathbf{A} is negative definite.\n\nNote:\\text{(i) } \\sum \\lambda_i = \\operatorname{tr}(\\mathbf{A}), \\qquad\n\\text{(ii) } \\prod \\lambda_i = |\\mathbf{A}|.\n\nLet’s continue with the example above to find the characteristic vector.\n\nWe know one of the roots \\lambda = -9, so substituting in the characteristic matrix gives(\\mathbf{A} - \\lambda \\mathbf{I})\\mathbf{v} = \\mathbf{0} \\quad \\Rightarrow \\quad\n\\begin{bmatrix}\n-6 - (-9) & 3 \\\\\n3 & -6 - (-9)\n\\end{bmatrix}\n\\begin{bmatrix}\nv_1 \\\\ v_2\n\\end{bmatrix}\n= \n\\begin{bmatrix}\n3 & 3 \\\\\n3 & 3\n\\end{bmatrix}\n\\begin{bmatrix}\nv_1 \\\\ v_2\n\\end{bmatrix}\n= \n\\begin{bmatrix}\n0 \\\\ 0\n\\end{bmatrix}.\n\nSince the coefficient matrix is linearly dependent, there are infinite number of solutions. The product of the matrices gives two equations which are identical:3v_1 + 3v_2 = 0 \\quad \\Rightarrow \\quad v_2 = -v_1.\n\nBy normalizing we havev_1^{2} + v_2^{2} = 1.\n\nSubstituting v_2 = -v_1 givesv_1^{2} + (-v_1)^{2} = 1 \\quad \\Rightarrow \\quad 2v_1^{2} = 1 \\quad \\Rightarrow \\quad v_1^{2} = \\frac{1}{2}.\n\nTaking the positive square root gives v_1 = \\sqrt{1/2} = \\frac{\\sqrt{2}}{2} and substituting into v_2 = -v_1 gives v_2 = -\\frac{\\sqrt{2}}{2}. That is\\mathbf{v}_1 = \n\\begin{bmatrix}\n\\frac{\\sqrt{2}}{2} \\\\[4pt]\n-\\frac{\\sqrt{2}}{2}\n\\end{bmatrix}.\n\nUsing the second characteristic root \\lambda = -3:(\\mathbf{A} - \\lambda \\mathbf{I})\\mathbf{v} = \n\\begin{bmatrix}\n-6 - (-3) & 3 \\\\\n3 & -6 - (-3)\n\\end{bmatrix}\n\\begin{bmatrix}\nv_1 \\\\ v_2\n\\end{bmatrix}\n= \n\\begin{bmatrix}\n-3 & 3 \\\\\n3 & -3\n\\end{bmatrix}\n\\begin{bmatrix}\nv_1 \\\\ v_2\n\\end{bmatrix}\n= \n\\begin{bmatrix}\n0 \\\\ 0\n\\end{bmatrix}.\n\nMultiplying out gives -3v_1 + 3v_2 = 0 and 3v_1 - 3v_2 = 0, so v_1 = v_2.\n\nNormalizing as before:v_1^2 + v_2^2 = 1 \\quad \\Rightarrow \\quad 2v_1^2 = 1 \\quad \\Rightarrow \\quad v_1 = \\frac{\\sqrt{2}}{2}.\n\nHence,\\mathbf{v}_2 = \n\\begin{bmatrix}\n\\frac{\\sqrt{2}}{2} \\\\[4pt]\n\\frac{\\sqrt{2}}{2}\n\\end{bmatrix}.","type":"content","url":"/linear-algebra-special#characteristic-roots-and-vectors-eigenvalues-and-eigenvectors","position":17},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices","lvl2":"Diagonalization"},"type":"lvl2","url":"/linear-algebra-special#diagonalization","position":18},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices","lvl2":"Diagonalization"},"content":"A square matrix A is diagonalizable if it can be written as:\\mathbf{A} = \\mathbf{T} \\mathbf{D} \\mathbf{T}^{-1}.\n\nwhere\n\n\\mathbf{D} is a diagonal matrix (entries only on the main diagonal) consisting of eignevalues of \\mathbf{A},\n\n\\mathbf{T} is an invertible matrix whose columns are eigenvectors of \\mathbf{A}.\n\nNote that not all matrices are diagonalizable, however.\n\nFor example, givenA =\n\\begin{bmatrix}\n2 & 1 \\\\\n1 & 2\n\\end{bmatrix}\n\nLet us start by finding its eigenvalues.\n\nThe characteristic polynomial is given by\\det(A - \\lambda I)\n= (2 - \\lambda)^2 - 1\n= \\lambda^2 - 4\\lambda + 3 = 0.\n\nHence,\\lambda = 1, \\qquad \\lambda = 3.\n\nFor \\lambda = 1:(A - I)\\mathbf{v}\n=\n\\begin{pmatrix}\n1 & 1 \\\\\n1 & 1\n\\end{pmatrix}\n\\mathbf{v}\n= \\mathbf{0}.\n\nA corresponding eigenvector is\\mathbf{v}_1 =\n\\begin{pmatrix}\n1 \\\\\n-1\n\\end{pmatrix}.\n\nFor \\lambda = 3:(A - 3I)\\mathbf{v}\n=\n\\begin{pmatrix}\n-1 & 1 \\\\\n1 & -1\n\\end{pmatrix}\n\\mathbf{v}\n= \\mathbf{0}.\n\nA corresponding eigenvector is\\mathbf{v}_2 =\n\\begin{pmatrix}\n1 \\\\\n1\n\\end{pmatrix}.\n\nThen,T =\n\\begin{pmatrix}\n1 & 1 \\\\\n-1 & 1\n\\end{pmatrix},\n\\qquad\nD =\n\\begin{pmatrix}\n1 & 0 \\\\\n0 & 3\n\\end{pmatrix}.\n\nThe inverse of T isT^{-1}\n= \\tfrac{1}{2}\n\\begin{pmatrix}\n1 & -1 \\\\\n1 & 1\n\\end{pmatrix}.\n\nWe can verify that A = TDT^{-1}.TDT^{-1}\n=\n\\begin{pmatrix}\n1 & 1 \\\\\n-1 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\n1 & 0 \\\\\n0 & 3\n\\end{pmatrix}\n\\cdot\n\\tfrac{1}{2}\n\\begin{pmatrix}\n1 & -1 \\\\\n1 & 1\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n2 & 1 \\\\\n1 & 2\n\\end{pmatrix}\n= A.\n\nNote (1):\n\nAll symmetric matrices are diagonalizable (even with repeated eigenvalues).\n\nDiagonalizable \\neq invertible.For example,\\begin{bmatrix}\n1 & 0 \\\\\n0 & 0\n\\end{bmatrix}\n\nis diagonalizable but not invertible.\n\nIf A is diagonalizable, then A^n and e^{A} are easy to compute.\n\nNote (2):This is closely related to the transformation form of diagonalization, which states that if A is diagonalizable, then there exists an invertible matrix T and a diagonal matrix D such thatT^{-1} A T = D.\n\nExercise. Can you show this?\n\nTry these ^^\n\nGiven:A = \\begin{bmatrix}  -4 & -2 \\\\ -2 & -6 \\end{bmatrix}, \\quad \nB = \\begin{bmatrix} 3 & 0 \\\\ 1 & 2 \\end{bmatrix}C = \\begin{bmatrix} 6 & 3 \\\\ 3 & -2 \\end{bmatrix}, \\quad\nD = \\begin{bmatrix} 4 & 6 & 3 \\\\ 0 & 2 & 5 \\\\ 0 & 1 & 3 \\end{bmatrix}\n\nFind:\n\n(a) Find the eigenvalues and eigenvectors for each of the matrices above(b) What can you say about the sign definiteness of each mattix(c) Verify \\mathbf{A} = \\mathbf{T} \\mathbf{D} \\mathbf{T}^{-1}(d) Find \\mathbf{A}^5","type":"content","url":"/linear-algebra-special#diagonalization","position":19},{"hierarchy":{"lvl1":"Multivariate Calculus"},"type":"lvl1","url":"/multivariate-calculus","position":0},{"hierarchy":{"lvl1":"Multivariate Calculus"},"content":"","type":"content","url":"/multivariate-calculus","position":1},{"hierarchy":{"lvl1":"Multivariate Calculus","lvl2":"Functions of Several Variables and Partial Derivatives"},"type":"lvl2","url":"/multivariate-calculus#functions-of-several-variables-and-partial-derivatives","position":2},{"hierarchy":{"lvl1":"Multivariate Calculus","lvl2":"Functions of Several Variables and Partial Derivatives"},"content":"While there are some subtleties involved in moving from univariate to multivariate calculus, much of the intuition carries over.\n\nConsider a functiony = f(x_1, x_2, \\ldots, x_n).\n\nThe partial derivative of y with respect to its i-th argument x_i is defined as\\frac{\\partial y}{\\partial x_i} \n= \\lim_{\\Delta x_i \\to 0} \n\\frac{f(x_1, \\ldots, x_i + \\Delta x_i, \\ldots, x_n) \n- f(x_1, \\ldots, x_i, \\ldots, x_n)}{\\Delta x_i}\n\nprovided the limit exists.\n\nCommon notations include\\frac{\\partial y}{\\partial x_i},\n\\quad\n\\frac{\\partial}{\\partial x_i} f(x_1,\\ldots,x_n),\n\\quad\nf_i.\n\nFor a function f(x,y), the partial derivative with respect to x is written f_x(x,y) or f_x; For a function f(x_1,x_2), the partial derivative with respect to x_1 is written f_1(x,y) or just f_1, and so on.\n\nIntuition\n\nA partial derivative measures how the function changes when one variable changes and all others are held constant.","type":"content","url":"/multivariate-calculus#functions-of-several-variables-and-partial-derivatives","position":3},{"hierarchy":{"lvl1":"Multivariate Calculus","lvl2":"Rules of Partial Differentiation"},"type":"lvl2","url":"/multivariate-calculus#rules-of-partial-differentiation","position":4},{"hierarchy":{"lvl1":"Multivariate Calculus","lvl2":"Rules of Partial Differentiation"},"content":"The rules of partial differentiation are the same as those for univariate calculus, except that all other variables are treated as constants.\n\nExamples\n\na) Letf(x_1,x_2) = \\sqrt{x_1 + 3x_2^2}.\n\nhas two partial derivatives:f_1 = \\frac{1}{2\\sqrt{x_1 + 3x_2^2}},\n\\qquad\nf_2 = \\frac{3x_2}{\\sqrt{x_1 + 3x_2^2}}.\n\nb) Similarly, leth(x,y,z) = \\ln(5x + 2y - 3z).\n\nthenh_x = \\frac{5}{5x+2y-3z}, \\quad\nh_y = \\frac{2}{5x+2y-3z}, \\quad\nh_z = \\frac{-3}{5x+2y-3z}.","type":"content","url":"/multivariate-calculus#rules-of-partial-differentiation","position":5},{"hierarchy":{"lvl1":"Multivariate Calculus","lvl3":"Example - Cobb-Douglass production function","lvl2":"Rules of Partial Differentiation"},"type":"lvl3","url":"/multivariate-calculus#example-cobb-douglass-production-function","position":6},{"hierarchy":{"lvl1":"Multivariate Calculus","lvl3":"Example - Cobb-Douglass production function","lvl2":"Rules of Partial Differentiation"},"content":"Consider the Cobb–Douglas production functionQ = AK^{1-\\alpha}L^\\alpha.\n\nThe marginal product of labor (MPL) can be found by taking the partial derivative of output with respect to labor:\\frac{\\partial Q}{\\partial L} \n= \\alpha A K^{1 - \\alpha} L^{\\alpha - 1} \n= \\alpha A \\left( \\frac{K}{L} \\right)^{1 - \\alpha}\n\nIn neo-classical economics theory, workers are paid a real wage equal to their marginal productivity of labor. So in the equation above we see that the real wage and MPL increase with larger capital stock relative to labor.\n\nSimilarly,\\frac{\\partial Q}{\\partial K} \n= (1 - \\alpha) A K^{-\\alpha} L^{\\alpha} \n= (1 - \\alpha) A \\left( \\frac{L}{K} \\right)^{\\alpha}\n\nWhich suggests that the rate of return to capital is relatively high in countries that have a relatively large labor-capital ratios. These high rates of returns should cause capital inflow from rich to poor countries. But why do we not see this in reality? See \n\nLucas (1990).\n\nEconomic interpretation\n\nIn neoclassical theory, factors are paid their marginal products. Hence wages rise with capital intensity, and the return to capital is higher where labor is abundant.","type":"content","url":"/multivariate-calculus#example-cobb-douglass-production-function","position":7},{"hierarchy":{"lvl1":"Multivariate Calculus","lvl2":"Multivariate Derivative Rules"},"type":"lvl2","url":"/multivariate-calculus#multivariate-derivative-rules","position":8},{"hierarchy":{"lvl1":"Multivariate Calculus","lvl2":"Multivariate Derivative Rules"},"content":"(1) Product Rule\\begin{aligned}\n\\frac{\\partial}{\\partial x}[f(x,y)g(x,y)]\n&=\ng(x,y)\\frac{\\partial f(x,y)}{\\partial x}\n+\nf(x,y)\\frac{\\partial g(x,y)}{\\partial x}, \\\\[2em]\n\\frac{\\partial}{\\partial y}[f(x,y)g(x,y)]\n&=\ng(x,y)\\frac{\\partial f(x,y)}{\\partial y}\n+\nf(x,y)\\frac{\\partial g(x,y)}{\\partial y}.\n\\end{aligned}\n\n(2) Quotient Rule\n\nThe derivative of the quotient of two (differentiable) functions, f(x,y)/g(x,y), is\\begin{aligned}\n\\frac{\\partial}{\\partial x}\\left[\\frac{f(x,y)}{g(x,y)}\\right]\n&=\n\\frac{\ng(x,y)\\frac{\\partial f(x,y)}{\\partial x}\n-\nf(x,y)\\frac{\\partial g(x,y)}{\\partial x}\n}{\n[g(x,y)]^2\n}, \\\\[2em]\n\\frac{\\partial}{\\partial y}\\left[\\frac{f(x,y)}{g(x,y)}\\right]\n&=\n\\frac{\ng(x,y)\\frac{\\partial f(x,y)}{\\partial y}\n-\nf(x,y)\\frac{\\partial g(x,y)}{\\partial y}\n}{\n[g(x,y)]^2\n}.\n\\end{aligned}\n\nwhere f(x,y) and g(x,y) are differentiable and g(x,y)\\neq 0.\n\n(3) Generalized Power Function\\begin{aligned}\n\\frac{\\partial [g(x,y)]^n}{\\partial x}\n&=\nn[g(x,y)]^{\\,n-1}\\frac{\\partial g(x,y)}{\\partial x}, \\\\[1em]\n\\frac{\\partial [g(x,y)]^n}{\\partial y}\n&=\nn[g(x,y)]^{\\,n-1}\\frac{\\partial g(x,y)}{\\partial y}.\n\\end{aligned}\n\nwhere n is a constant and g(x,y) is differentiable.","type":"content","url":"/multivariate-calculus#multivariate-derivative-rules","position":9},{"hierarchy":{"lvl1":"Multivariate Calculus","lvl2":"Second-Order and Cross-Partial Derivatives"},"type":"lvl2","url":"/multivariate-calculus#second-order-and-cross-partial-derivatives","position":10},{"hierarchy":{"lvl1":"Multivariate Calculus","lvl2":"Second-Order and Cross-Partial Derivatives"},"content":"Fory = f(x_1,x_2),\n\nthe second partial derivatives aref_{11} = \\frac{\\partial^2 y}{\\partial x_1^2},\n\\qquad\nf_{22} = \\frac{\\partial^2 y}{\\partial x_2^2}.\n\nThe cross partial derivatives aref_{12} = \\frac{\\partial^2 y}{\\partial x_1 \\partial x_2},\n\\qquad\nf_{21} = \\frac{\\partial^2 y}{\\partial x_2 \\partial x_1}.\n\nContinuing with the examples above, the functionf(x_1, x_2) = \\sqrt{x_1 + 3x_2^2}\n\nhas two second partial derivatives\\begin{aligned}\nf_{11}(x_1,x_2)\n&=\n-\\frac{1}{4\\left(x_1 + 3x_2^2\\right)^{3/2}}, \\\\[2em]\nf_{22}(x_1,x_2)\n&=\n-\\frac{9x_2^2}{\\left(x_1 + 3x_2^2\\right)^{3/2}}\n+\n\\frac{3}{\\left(x_1 + 3x_2^2\\right)^{1/2}}\n\\end{aligned}\n\nand the cross partial derivativef_{12}(x_1,x_2)\n=\nf_{21}(x_1,x_2)\n=\n-\\frac{3x_2}{2\\left(x_1 + 3x_2^2\\right)^{3/2}}.\n\nFor the functionh(x,y,z) = \\ln(5x + 2y - 3z)\n\nThere are three second partial derivatives:h_{xx}(x,y,z)\n=\n-\\frac{25}{(5x + 2y - 3z)^2},h_{yy}(x,y,z)\n=\n-\\frac{4}{(5x + 2y - 3z)^2},\n\\quad \\text{and} \\quad\nh_{zz}(x,y,z)\n=\n-\\frac{9}{(5x + 2y - 3z)^2}.\n\nThe function also has three cross partial derivatives:h_{xy}(x,y,z)\n=\nh_{yx}(x,y,z)\n=\n-\\frac{10}{(5x + 2y - 3z)^2},h_{xz}(x,y,z)\n=\nh_{zx}(x,y,z)\n=\n-\\frac{15}{(5x + 2y - 3z)^2},\n\\quad \\text{and} \\quadh_{yz}(x,y,z)\n=\nh_{zy}(x,y,z)\n=\n-\\frac{6}{(5x + 2y - 3z)^2}","type":"content","url":"/multivariate-calculus#second-order-and-cross-partial-derivatives","position":11},{"hierarchy":{"lvl1":"Multivariate Calculus","lvl3":"Young’s Theorem","lvl2":"Second-Order and Cross-Partial Derivatives"},"type":"lvl3","url":"/multivariate-calculus#youngs-theorem","position":12},{"hierarchy":{"lvl1":"Multivariate Calculus","lvl3":"Young’s Theorem","lvl2":"Second-Order and Cross-Partial Derivatives"},"content":"If all second-order partial derivatives are continuous, thenf_{ij} = f_{ji}.\n\nWhy this matters\n\nYoung’s Theorem ensures that the order of differentiation does not matter for well-behaved functions, a key assumption in optimization.\n\nMore generally, if all the partial derivatives of the functionf(x_1, x_2, \\ldots, x_n)\n\nexist and are themselves differentiable with continuous derivatives, then\\frac{\\partial}{\\partial x_i}\n\\left(\n\\frac{\\partial f(x_1, x_2, \\ldots, x_n)}{\\partial x_j}\n\\right)\n=\n\\frac{\\partial}{\\partial x_j}\n\\left(\n\\frac{\\partial f(x_1, x_2, \\ldots, x_n)}{\\partial x_i}\n\\right).\n\nOr, written differently,f_{ji}(x_1, x_2, \\ldots, x_n)\n=\nf_{ij}(x_1, x_2, \\ldots, x_n),\n\nfor any i and j from 1 to n.\n\nYoung’s Theorem and second derivatives\n\nYoung’s Theorem shows that a multivariate function that is fully differentiable with respect to all its n arguments has, at most, n distinct second partial\nderivatives, and (n^2 - n)/2 distinct cross partial derivatives.\n\nFor our Cobb–Douglas production function,Q = AK^{1-\\alpha}L^{\\alpha},\n\nthe second partial derivative with respect to L and with respect to K is\\frac{\\partial^2 Q}{\\partial L^2}\n=\n-(1-\\alpha)\\alpha A K^{1-\\alpha}L^{\\alpha-1},\\frac{\\partial^2 Q}{\\partial K^2}\n=\n-(1-\\alpha)\\alpha A K^{-\\alpha-1}L^{\\alpha}.\n\nEach of these second partial derivatives is negative, reflecting diminishing\nmarginal productivity.\n\nThe single cross partial derivative is\\frac{\\partial^2 Q}{\\partial K \\partial L}\n=\n\\frac{\\partial^2 Q}{\\partial L \\partial K}\n=\n(1-\\alpha)\\alpha A K^{-\\alpha}L^{\\alpha-1},\n\nwhich is positive since K and L are positive.\n\nEconomic intuition\n\nA positive cross partial derivative means that capital and labor are complements: increasing the use of one input raises the marginal productivity of the other. At the same time, the negative second partial derivatives capture diminishing marginal productivity of each input on its own.\n\nHessian matrix representation (optional)\n\nThe second-order derivatives of the Cobb–Douglas production function can be\nsummarized by the Hessian matrixH(Q)\n=\n\\begin{bmatrix}\n\\displaystyle \\frac{\\partial^2 Q}{\\partial K^2}\n&\n\\displaystyle \\frac{\\partial^2 Q}{\\partial K \\partial L}\n\\\\[1em]\n\\displaystyle \\frac{\\partial^2 Q}{\\partial L \\partial K}\n&\n\\displaystyle \\frac{\\partial^2 Q}{\\partial L^2}\n\\end{bmatrix}\n\nThe signs of the Hessian entries reflect diminishing marginal productivity\n(negative diagonal terms) and complementarity between inputs (positive\noff-diagonal terms).\n\nTry these ^^\n\nFor each of the following functions:\n\nCompute the first-order partial derivatives\\frac{\\partial y}{\\partial x_1}, \\qquad \\frac{\\partial y}{\\partial x_2}.\n\nFor each functions, evaluate the first-order partial derivatives at the pointx_1 = 1, \\quad x_2 = 4.\n\nFor each functions, compute the second-order partial derivatives\\frac{\\partial^2 y}{\\partial x_1^2}, \\qquad\n\\frac{\\partial^2 y}{\\partial x_2^2},\n\nand the cross partial derivative\\frac{\\partial^2 y}{\\partial x_1 \\partial x_2}.\n\nAssume all functions are defined on domains where the expressions are well-defined.\n\n(i) y = f(x_1,x_2) = 12x_1^4 - 6x_1^2x_2 + 4x_2^3\n\n(ii) y = f(x_1,x_2) = (3x_1^2 + 5x_2 + 1)(x_2 + 4)\n\n(iii) y = f(x_1,x_2) = \\frac{7x_1 - x_1x_2^2}{x_1 - 2}\n\n(iv) y = f(x_1,x_2) = (2e^{x_1})(e^{2x_1}x_2^2)\n\n(v) y = f(x_1,x_2) = 2\\ln(3x_1) - 4\\ln(2x_1x_2)\n\n(vi) y = f(x_1,x_2) = x_1^2 + 2x_1x_2^{1/2} - 4x_2\n\nAnswers: First-Order Partial Derivatives\n\n(i) \\frac{\\partial y}{\\partial x_1} = 48x_1^3 - 12x_1x_2,\n\\qquad\n\\frac{\\partial y}{\\partial x_2} = -6x_1^2 + 12x_2^2\n\n(ii) \\frac{\\partial y}{\\partial x_1} = 6x_1x_2 + 24x_1,\n\\qquad\n\\frac{\\partial y}{\\partial x_2} = 3x_1^2 + 5x_1 + 1\n\n(iii) \\frac{\\partial y}{\\partial x_1} = \\frac{2x_2^2 - 14}{(x_1 - 2)^2},\n\\qquad\n\\frac{\\partial y}{\\partial x_2} =\n-\\frac{2x_1x_2}{x_1 - 2}\n\n(iv) \\frac{\\partial y}{\\partial x_1} = 6x_2^2 e^{3x_1},\n\\qquad\n\\frac{\\partial y}{\\partial x_2} = 4x_2 e^{3x_1}\n\n(v) \n\\frac{\\partial y}{\\partial x_1} = -\\frac{2}{x_1},\n\\qquad\n\\frac{\\partial y}{\\partial x_2} = -\\frac{4}{x_2}\n\n(vi) \\frac{\\partial y}{\\partial x_1} = 2x_1 + 2\\sqrt{x_2},\n\\qquad\n\\frac{\\partial y}{\\partial x_2} = \\frac{x_1}{\\sqrt{x_2}} - 4\n\nAnswers 2\n\n(i) \\left.\\frac{\\partial y}{\\partial x_1}\\right|_{(1,4)} = 0,\n\\qquad\n\\left.\\frac{\\partial y}{\\partial x_2}\\right|_{(1,4)} = 186\n\n(ii) \\left.\\frac{\\partial y}{\\partial x_1}\\right|_{(1,4)} = 88,\n\\qquad\n\\left.\\frac{\\partial y}{\\partial x_2}\\right|_{(1,4)} = 9\n\n(iii) \\left.\\frac{\\partial y}{\\partial x_1}\\right|_{(1,4)} = 18,\n\\qquad\n\\left.\\frac{\\partial y}{\\partial x_2}\\right|_{(1,4)} = 8\n\n(iv) \\left.\\frac{\\partial y}{\\partial x_1}\\right|_{(1,4)} = 1928,\n\\qquad\n\\left.\\frac{\\partial y}{\\partial x_2}\\right|_{(1,4)} = 321\n\n(v) \\left.\\frac{\\partial y}{\\partial x_1}\\right|_{(1,4)} = -2,\n\\qquad\n\\left.\\frac{\\partial y}{\\partial x_2}\\right|_{(1,4)} = -1\n\n(vi) \\left.\\frac{\\partial y}{\\partial x_1}\\right|_{(1,4)} = 6,\n\\qquad\n\\left.\\frac{\\partial y}{\\partial x_2}\\right|_{(1,4)} = -\\frac{7}{2}\n\n3. Second-Order and Cross Partial Derivatives\n\nFor each function, compute: \\dfrac{\\partial^2 y}{\\partial x_1^2}, \\dfrac{\\partial^2 y}{\\partial x_2^2}, and \\dfrac{\\partial^2 y}{\\partial x_1\\partial x_2}.\n\nAnswers: Second-Order and Cross Partial Derivatives\n\n(i) \n\\frac{\\partial^2 y}{\\partial x_1^2} = 144x_1^2 - 12x_2,\n\\qquad\n\\frac{\\partial^2 y}{\\partial x_2^2} = 24x_2,\n\\qquad\n\\frac{\\partial^2 y}{\\partial x_1\\partial x_2} = -12x_1\n\n(ii) \n\\frac{\\partial^2 y}{\\partial x_1^2} = 6x_2 + 24,\n\\qquad\n\\frac{\\partial^2 y}{\\partial x_2^2} = 6x_1 + 5,\n\\qquad\n\\frac{\\partial^2 y}{\\partial x_1\\partial x_2} = 6x_1\n\n(iii) \n\\frac{\\partial^2 y}{\\partial x_1^2} =\n-\\frac{4(x_1x_2^2 - 2x_2^2 - 7x_1 + 14)}{(x_1 - 2)^3},\n\\qquad\n\\frac{\\partial^2 y}{\\partial x_2^2} =\n\\frac{4x_1}{(x_1 - 2)^2},\n\\qquad\n\\frac{\\partial^2 y}{\\partial x_1\\partial x_2} =\n-\\frac{4x_2}{(x_1 - 2)^2}\n\n(iv) \n\\frac{\\partial^2 y}{\\partial x_1^2} = 18x_2^2 e^{3x_1},\n\\qquad\n\\frac{\\partial^2 y}{\\partial x_2^2} = 4e^{3x_1},\n\\qquad\n\\frac{\\partial^2 y}{\\partial x_1\\partial x_2} = 12e^{3x_1}x_2\n\n(v) \n\\frac{\\partial^2 y}{\\partial x_1^2} = \\frac{2}{x_1^2},\n\\qquad\n\\frac{\\partial^2 y}{\\partial x_2^2} = \\frac{4}{x_2^2},\n\\qquad\n\\frac{\\partial^2 y}{\\partial x_1\\partial x_2} = 0\n\n(vi) \n\\frac{\\partial^2 y}{\\partial x_1^2} = 2,\n\\qquad\n\\frac{\\partial^2 y}{\\partial x_2^2} = -\\frac{1}{2}x_1 x_2^{-3/2},\n\\qquad\n\\frac{\\partial^2 y}{\\partial x_1\\partial x_2} = x_2^{-1/2}","type":"content","url":"/multivariate-calculus#youngs-theorem","position":13},{"hierarchy":{"lvl1":"Multivariate Calculus","lvl2":"Composite Functions and Multivariate Chain Rules"},"type":"lvl2","url":"/multivariate-calculus#composite-functions-and-multivariate-chain-rules","position":14},{"hierarchy":{"lvl1":"Multivariate Calculus","lvl2":"Composite Functions and Multivariate Chain Rules"},"content":"In the univariate case, we learnt that the derivative of the composite functiony = f(x) = g(h(x))\n\nis\\frac{dy}{dx} = g'(h(x))h'(x).\n\nA multivariate form of the chain rule can be used with multivariate compositefunctions.","type":"content","url":"/multivariate-calculus#composite-functions-and-multivariate-chain-rules","position":15},{"hierarchy":{"lvl1":"Multivariate Calculus","lvl2":"Multivariate Chain Rule I (Single Parameter)"},"type":"lvl2","url":"/multivariate-calculus#multivariate-chain-rule-i-single-parameter","position":16},{"hierarchy":{"lvl1":"Multivariate Calculus","lvl2":"Multivariate Chain Rule I (Single Parameter)"},"content":"If the arguments of the differentiable functiony = f(x_1, x_2, \\ldots, x_n)\n\nare themselves differentiable functions of a single variable t such thatx_1 = g^1(t), \\quad x_2 = g^2(t), \\quad \\ldots, \\quad x_n = g^n(t),\n\nwhere g^i(t) is the ith univariate function, then\\frac{dy}{dt}\n=\nf_1 \\frac{dx_1}{dt}\n+\nf_2 \\frac{dx_2}{dt}\n+\n\\cdots\n+\nf_n \\frac{dx_n}{dt},\n\nwheref_i = \\frac{\\partial y}{\\partial x_i}.\n\nExample\n\nConsider the functiony = f(x_1, x_2) = x_1^2 x_2,\n\nwherex_1 = t^2\n\\quad \\text{and} \\quad\nx_2 = 3t - 1.\n\nThen\\frac{dx_1}{dt} = 2t\n\\quad \\text{and} \\quad\n\\frac{dx_2}{dt} = 3,\n\nandf_1 = 2x_1 x_2\n\\quad \\text{and} \\quad\nf_2 = x_1^2.\n\nUsing the chain rule,\\frac{dy}{dt}\n=\n(2x_1 x_2)\\frac{dx_1}{dt}\n+\n(x_1^2)\\frac{dx_2}{dt}.\n\nSubstituting,\\frac{dy}{dt}\n=\n(2t^2(3t - 1))(2t)\n+\n3(t^2)^2\n=\n4t^3(3t - 1) + 3t^4.\n\nSimplifying,\\frac{dy}{dt}\n=\n15t^4 - 4t^3.","type":"content","url":"/multivariate-calculus#multivariate-chain-rule-i-single-parameter","position":17},{"hierarchy":{"lvl1":"Multivariate Calculus","lvl2":"Multivariate Chain Rule II (Multiple Parameters)"},"type":"lvl2","url":"/multivariate-calculus#multivariate-chain-rule-ii-multiple-parameters","position":18},{"hierarchy":{"lvl1":"Multivariate Calculus","lvl2":"Multivariate Chain Rule II (Multiple Parameters)"},"content":"If the arguments of the differentiable functiony = f(x_1, x_2, \\ldots, x_n)\n\nare themselves differentiable functions of variables t_1, \\ldots, t_m such thatx_1 = g^1(t_1, \\ldots, t_m), \\quad\n\\ldots, \\quad\nx_n = g^n(t_1, \\ldots, t_m),\n\nthen, for each t_i,\\frac{\\partial y}{\\partial t_i}\n=\nf_1 \\frac{\\partial x_1}{\\partial t_i}\n+\nf_2 \\frac{\\partial x_2}{\\partial t_i}\n+\n\\cdots\n+\nf_n \\frac{\\partial x_n}{\\partial t_i},\n\nwheref_i = \\frac{\\partial y}{\\partial x_i}.\n\nKey intuition\n\nThe multivariate chain rule states that the total change in y with respect to a\nparameter is the sum of each marginal effect multiplied by how the corresponding\nargument changes.\n\nIn other words,\\text{Total change}\n=\n\\sum\n\\left(\n\\text{marginal effect}\n\\times\n\\text{change in argument}\n\\right).\n\nTry these ^^","type":"content","url":"/multivariate-calculus#multivariate-chain-rule-ii-multiple-parameters","position":19},{"hierarchy":{"lvl1":"Multivariate Calculus","lvl3":"Partial Derivatives with Change of Variables","lvl2":"Multivariate Chain Rule II (Multiple Parameters)"},"type":"lvl3","url":"/multivariate-calculus#partial-derivatives-with-change-of-variables","position":20},{"hierarchy":{"lvl1":"Multivariate Calculus","lvl3":"Partial Derivatives with Change of Variables","lvl2":"Multivariate Chain Rule II (Multiple Parameters)"},"content":"For each of the following cases, find the partial derivatives \\displaystyle \\frac{\\partial Z}{\\partial u} and \\displaystyle \\frac{\\partial Z}{\\partial v}.\n\n(i) Let Z = f(x,y) = 4x^2 + 2xy + y^2, where x = 3u^2 \\text{ and } y = u - 2v\n\n(ii) Let Z = f(x,y) = ax^3 - bx^2y + cy, where x = \\gamma u + \\theta v \\text{ and } y = \\theta u - \\gamma v^2\n\n(iii) Let Z = f(x,y) = 2e^x + \\tfrac{1}{2}x^2y - 4y, where x = \\tfrac{1}{4}u \\text{ and } y = u^2 + 6v\n\n(iv) Let Z = f(x,y,u) = 2x^3 - 3xy^2 + 0.75yu - 5u^2, where x = \\sqrt{u+v} \\text{ and } y = v^2\n\nAnswers\n\n(i) \\frac{\\partial Z}{\\partial u} = 48xu + 12yu + 2x + 2y, \\text{ and } \\frac{\\partial Z}{\\partial v} = -4x - 4y\n\n(ii) \\frac{\\partial Z}{\\partial u} = (3ax^2 - 2bxy)\\gamma + (-bx^2 + c)\\theta, \\text{ and } \\frac{\\partial Z}{\\partial v} = (3ax^2 - 2bxy)\\theta + (-bx^2 + c)(-2\\gamma v)\n\n(iii) \\frac{\\partial Z}{\\partial u} = (2e^x + xy)\\tfrac{1}{4} + \\left(\\tfrac{1}{2}x^2 - 4\\right)2u, \\text{ and } \\frac{\\partial Z}{\\partial v} = 3x^2 - 24\n\n(iv) \\frac{\\partial Z}{\\partial u} = (6x^2 - 3y^2)\\frac{1}{2\\sqrt{u+v}} + 0.75y - 10u, \\text{ and } \\frac{\\partial Z}{\\partial v} = (6x^2 - 3y^2)\\frac{1}{2\\sqrt{u+v}} + (-6xy + 0.75u)2v\n\nCommon pitfall: direct vs indirect effects\n\nIn parts (i)–(iii), the variable Z depends on u and v only through the intermediate variables x and y.\nThe multivariate chain rule therefore involves only indirect effects.\n\nIn part (iv), however, Z = f(x,y,u) depends on u both directly and indirectly (through x).\nAs a result,\\frac{\\partial Z}{\\partial u}\n=\nZ_x \\frac{\\partial x}{\\partial u}\n+\nZ_y \\frac{\\partial y}{\\partial u}\n+\nZ_u.\n\nA very common mistake is to omit the direct term Z_u, which leads to an incomplete derivative.","type":"content","url":"/multivariate-calculus#partial-derivatives-with-change-of-variables","position":21},{"hierarchy":{"lvl1":"Multivariate Calculus","lvl2":"Total Differentials"},"type":"lvl2","url":"/multivariate-calculus#total-differentials","position":22},{"hierarchy":{"lvl1":"Multivariate Calculus","lvl2":"Total Differentials"},"content":"The total differential of the multivariate functiony = f(x_1, \\ldots, x_n)\n\nevaluated at the point(x_1^0, x_2^0, \\ldots, x_n^0)\n\nis\\begin{aligned}\ndy\n&=\nf_1(x_1^0, \\ldots, x_n^0)\\,dx_1\n+\nf_2(x_1^0, \\ldots, x_n^0)\\,dx_2 \\\\\n&\\quad\n+\n\\cdots\n+\nf_n(x_1^0, \\ldots, x_n^0)\\,dx_n.\n\\end{aligned}\n\nwhere f_i(x_1^0, x_2^0, \\ldots, x_n^0) represents the partial derivative of the function f(x_1, x_2, \\ldots, x_n) with respect to its ith argument, evaluated at the point (x_1^0, x_2^0, \\ldots, x_n^0).\n\nExample\n\nGivenz = f(x,y) = x^4 + 8xy + 3y^3,\n\nthe total differential isdz = z_x\\,dx + z_y\\,dy.\n\nSincez_x = 4x^3 + 8y\n\\quad \\text{and} \\quad\nz_y = 8x + 9y^2,\n\nwe havedz = (4x^3 + 8y)\\,dx + (8x + 9y^2)\\,dy.","type":"content","url":"/multivariate-calculus#total-differentials","position":23},{"hierarchy":{"lvl1":"Multivariate Calculus","lvl3":"Higher-Order Total Differentials","lvl2":"Total Differentials"},"type":"lvl3","url":"/multivariate-calculus#higher-order-total-differentials","position":24},{"hierarchy":{"lvl1":"Multivariate Calculus","lvl3":"Higher-Order Total Differentials","lvl2":"Total Differentials"},"content":"We can take higher-order total differentials if required. Continuing with the same example, the second-order total differential isd^2 z\n=\n12x^2\\,dx^2\n+\n16\\,dx\\,dy\n+\n18y\\,dy^2.\n\nConnection to the Multivariate Chain Rule\n\nThe total differential provides a direct link to the multivariate chain rule.\n\nIf each variable x_i depends on a single parameter t, thendx_i = \\frac{dx_i}{dt}\\,dt.\n\nSubstituting into the total differential,dy\n=\n\\sum_{i=1}^n\n\\frac{\\partial f}{\\partial x_i}\n\\frac{dx_i}{dt}\\,dt.\n\nDividing both sides by dt yields\\frac{dy}{dt}\n=\n\\sum_{i=1}^n\n\\frac{\\partial f}{\\partial x_i}\n\\frac{dx_i}{dt},\n\nwhich is precisely the multivariate chain rule.\n\nThus, the chain rule can be interpreted as applying the total differential to situations in which the arguments of a multivariate function depend on an underlying parameter.\n\nTry these ^^","type":"content","url":"/multivariate-calculus#higher-order-total-differentials","position":25},{"hierarchy":{"lvl1":"Multivariate Calculus","lvl3":"Total Differentials","lvl2":"Total Differentials"},"type":"lvl3","url":"/multivariate-calculus#total-differentials-1","position":26},{"hierarchy":{"lvl1":"Multivariate Calculus","lvl3":"Total Differentials","lvl2":"Total Differentials"},"content":"1. Find the total differentials of each function.\n\n(i) w = 2x^2 + \\frac{1}{2}xy - 3y^3(ii) w = 4x_1^3 - \\ln(x_1 x_2) + 6x_2(iii) z = \\dfrac{x^2}{y^3 + xy}(iv) y = 2x_1^2 e^{3x_2}\n\n2. For each function, use the total differential to approximate the change in y due to the given changes in x and z.\n\n(i) y = x^2 + 4x - z^2 - 2xz,\nwhere x = 1, z = 4, \\Delta x = 2, \\Delta z = -2\n\n(ii) y = e^{x^2 + 3z},\nwhere x = 1, z = 2, \\Delta x = 2, \\Delta z = -2\n\n(iii) y = \\ln x^3 - 4z + 2xz,\nwhere x = 1, z = 2, \\Delta x = 2, \\Delta z = 4\n\n(iv) y = x^2 + e^{z/2},\nwhere x = 2, z = 2, \\Delta x = 1, \\Delta z = -1\n\nAnswers: Question 1\n\n(i) dw = (4x + \\tfrac{1}{2}y),dx + (\\tfrac{1}{2}x - 9y^2),dy (ii) dw = \\left(12x_1^2 - \\frac{1}{x_1}\\right),dx_1 + \\left(6 - \\frac{1}{x_2}\\right),dx_2 (iii) dz = \\dfrac{2xy^3 + x^2y}{(y^3 + xy)^2},dx - \\dfrac{3x^2y^2 + x^3}{(y^3 + xy)^2},dy (iv) dy = (4x_1 e^{3x_2}),dx_1 + (6x_1^2 e^{3x_2}),dx_2\n\nAnswers: Question 2\n\n(i) \\Delta y = 24, dy = 16 (ii) \\Delta y = 7007, dy = -2194 (iii) \\Delta y = 19.3, dy = 6 (iv) dy = 2x,dx + \\tfrac{1}{2}e^{z/2},dz = 4 - \\tfrac{1}{2}e = 2.64. Actual change is \\Delta y = 3.93\n\nInterpretation\n\nThe total differential provides a local linear approximation to the actual change in a function.\n\nWhen changes are small, dy closely approximates \\Delta y.\nWhen changes are large or the function is highly nonlinear, the approximation may be poor.","type":"content","url":"/multivariate-calculus#total-differentials-1","position":27},{"hierarchy":{"lvl1":"Multivariate Calculus","lvl2":"Total Derivatives"},"type":"lvl2","url":"/multivariate-calculus#total-derivatives","position":28},{"hierarchy":{"lvl1":"Multivariate Calculus","lvl2":"Total Derivatives"},"content":"Given a case with\nz = f(x,y) and y = g(x),\nthat is, when x and y are not independent, a change in x will affect z directly through the function f and indirectly through the function g.\n\nThe total derivative measures the direct effect of x on z, \\partial z / \\partial x, plus the indirect effect of x on z through y, (\\partial z/\\partial y)(dy/dx).\n\nThat is,\\frac{dz}{dx}\n=\nz_x + z_y \\frac{dy}{dx}.\n\nConnection to Total Differentiation\n\nAn alternative method of finding the total derivative is to take the total differential of z:dz = z_x,dx + z_y,dy.\n\nDividing through by dx gives\\frac{dz}{dx}\n=\nz_x \\frac{dx}{dx}\n+\nz_y \\frac{dy}{dx}.\n\nSince dx/dx = 1, this simplifies to\\frac{dz}{dx}\n=\nz_x + z_y \\frac{dy}{dx}.\n\nExample 1\n\nLetz = f(x,y) = 6x^3 + 7y,\n\nwherey = 4x^2 + 3x.\n\nThenz_x = 18x^2,\n\\qquad\nz_y = 7,\n\\qquad\n\\frac{dy}{dx} = 8x + 3.\n\nSubstituting into the total derivative formula,\\frac{dz}{dx}\n=\n18x^2 + 7(8x + 3)\n=\n18x^2 + 56x + 21.\n\nExample 2 (Parametric Dependence)\n\nLetz = f(x,y) = 8x^2 + 3y^2,\n\nwherex = 4t,\n\\qquad\ny = 5t.\n\nThe total derivative of z with respect to t is\\frac{dz}{dt}\n=\nz_x \\frac{dx}{dt}\n+\nz_y \\frac{dy}{dt}.\n\nSincez_x = 16x,\n\\qquad\nz_y = 6y,\n\\qquad\n\\frac{dx}{dt} = 4,\n\\qquad\n\\frac{dy}{dt} = 5,\n\nwe obtain\\frac{dz}{dt}\n=\n16x(4) + 6y(5)\n=\n64x + 30y.\n\nSubstituting x = 4t and y = 5t gives\\frac{dz}{dt}\n=\n64(4t) + 30(5t)\n=\n256t + 150t\n=\n406t.\n\nTry these ^^","type":"content","url":"/multivariate-calculus#total-derivatives","position":29},{"hierarchy":{"lvl1":"Multivariate Calculus","lvl3":"Total Derivatives","lvl2":"Total Derivatives"},"type":"lvl3","url":"/multivariate-calculus#total-derivatives-1","position":30},{"hierarchy":{"lvl1":"Multivariate Calculus","lvl3":"Total Derivatives","lvl2":"Total Derivatives"},"content":"In the following questions, the function f(x,y) depends on x both directly and indirectly through y.\nYou must therefore compute the total derivative, not a partial derivative.\n\nRecall that if y = y(t), then\\frac{df}{dt} = f_x \\frac{dx}{dt} + f_y \\frac{dy}{dt}.\n\n1. Total derivative with respect to x\n\nFind the total derivative df(x,y)/dx for each of the following functions.\n\n(i) f(x,y) = 6x^2 + 15xy + 3y^2, \n\\quad \\text{where } y = 7x^2.\n\n(ii) f(x,y) = \\frac{9x - 7y}{2x + 5y},\n\\quad \\text{where } y = 3x - 4.\n\n(iii) f(x,y) = 8x - 12y,\n\\quad \\text{where } y = \\frac{x+1}{x^2}.\n\n2. Total derivative with respect to w\n\nFind the total derivative df(x,y)/dw for each of the following functions.\n\n(i) f(x,y) = 7x^2 + 4y^2,\n\\quad \\text{where } x = 5w \\text{ and } y = 4w.\n\n(ii) f(x,y) = 10x^2 - 6xy - 12y^2,\n\\quad \\text{where } x = 2w \\text{ and } y = 3w.\n\nCommon pitfall\n\nDo not compute f_x alone.\n\nBecause y depends on the same variable as x (either x itself or w),\nyou must include both terms:\\frac{df}{dt} = f_x \\frac{dx}{dt} + f_y \\frac{dy}{dt}.\n\nAnswers: Question 1\n\n(i)  \\frac{df}{dx}\n= f_x + f_y \\frac{dy}{dx}\n= 210x^2 + 84xy + 12x + 15y.\n\n(ii) \\frac{df}{dx}\n= \\frac{59(y - 3x)}{(2x + 5y)^2}.\n\n(iii) \\frac{df}{dx}\n= 8 + \\frac{12(x+2)}{x^3}.\n\nAnswers: Question 2\n\n(i) \\frac{df}{dw}\n= f_x \\frac{dx}{dw} + f_y \\frac{dy}{dw}\n= 14x(5) + 8y(4)\n= 70x + 32y.\n\n(ii) \\frac{df}{dw}\n= (20x - 6y)(2) + (-6x - 24y)(3)\n= 22x - 84y.","type":"content","url":"/multivariate-calculus#total-derivatives-1","position":31},{"hierarchy":{"lvl1":"Multivariate Calculus","lvl2":"Implicit Multivariate Differentiation"},"type":"lvl2","url":"/multivariate-calculus#implicit-multivariate-differentiation","position":32},{"hierarchy":{"lvl1":"Multivariate Calculus","lvl2":"Implicit Multivariate Differentiation"},"content":"An explicit function expresses the dependent variable directly as a function of independent variables:y = f(x_1, x_2, \\ldots, x_n).\n\nAn implicit function combines the dependent and independent variables in a relation of the formF(y, x_1, x_2, \\ldots, x_n) = k,\n\nwhere k is a constant (possibly zero).\n\nImplicit Function Rule\n\nFor an implicit functionF(y, x_1, x_2, \\ldots, x_n) = k,\n\ndefined at a point (y^0, x_1^0, x_2^0, \\ldots, x_n^0), assume F has continuous first partial derivatives andF_y(y^0, x_1^0, x_2^0, \\ldots, x_n^0) \\neq 0.\n\nThen there exists a functiony = f(x_1, x_2, \\ldots, x_n)\n\ndefined in a neighborhood of (x_1^0, x_2^0, \\ldots, x_n^0) such that:F(f(x_1^0, x_2^0, \\ldots, x_n^0), x_1^0, x_2^0, \\ldots, x_n^0) = k,y^0 = f(x_1^0, x_2^0, \\ldots, x_n^0),\n\nandf_i(x_1^0, x_2^0, \\ldots, x_n^0)\n=\n-\n\\frac{\nF_{x_i}(y^0, x_1^0, x_2^0, \\ldots, x_n^0)\n}{\nF_y(y^0, x_1^0, x_2^0, \\ldots, x_n^0)\n}.\n\nHere,F_{x_i} = \\frac{\\partial F}{\\partial x_i},\n\\qquad\nF_y = \\frac{\\partial F}{\\partial y},\n\nevaluated at (y^0, x_1^0, x_2^0, \\ldots, x_n^0).\n\nThis result is sometimes referred to as the inverse rule for implicit functions.\n\nReminder\n\nWhen using implicit differentiation, always compute both F_x and F_y carefully.\nA sign error or a missing term in either partial derivative will lead to an incorrect result.\n\nTry these ^^","type":"content","url":"/multivariate-calculus#implicit-multivariate-differentiation","position":33},{"hierarchy":{"lvl1":"Multivariate Calculus","lvl3":"Implicit and Inverse Function Rule","lvl2":"Implicit Multivariate Differentiation"},"type":"lvl3","url":"/multivariate-calculus#implicit-and-inverse-function-rule","position":34},{"hierarchy":{"lvl1":"Multivariate Calculus","lvl3":"Implicit and Inverse Function Rule","lvl2":"Implicit Multivariate Differentiation"},"content":"Find the derivative of each implicit function, where\\frac{dy}{dx} = -\\frac{F_x}{F_y},\n\nprovided that F_y \\neq 0.\n\n(i)  F(x,y) = x^2 + y^2 + (xy)^{1/3} = 0\n\n(ii) F(x,y) = x^2 y + y^2 x + xy = 0\n\n(iii) F(x,y) = \\ln x^3 + (xy)^2 - 4y = 0\n\n(iv) F(x,y,w) = w^3 y^3 + x^2 + wxy + 7 = 0 (Find \\partial y / \\partial x)\n\n(v) F(x,y) = xy^2 e^y = 0\n\nAnswers\n\n(i)  \\frac{dy}{dx} = -\\frac{6x + x^{-2/3} y^{1/3}}{6y + y^{-2/3} x^{1/3}} (ii)  \\frac{dy}{dx} = -\\frac{y(2x + y + 1)}{x(2y + x + 1)} (iii)  \\frac{dy}{dx} = -\\frac{3x^{-1} + 2xy^2}{2x^2 y - 4} (iv)  \\frac{\\partial y}{\\partial x} = -\\frac{3x^2 + wy}{3w^3 y^2 + wx} (v)  \\frac{dy}{dx} = -\\frac{y}{x(2 + y)}","type":"content","url":"/multivariate-calculus#implicit-and-inverse-function-rule","position":35},{"hierarchy":{"lvl1":"Multivariate Calculus","lvl2":"Homogeneous Functions and Euler’s Theorem"},"type":"lvl2","url":"/multivariate-calculus#homogeneous-functions-and-eulers-theorem","position":36},{"hierarchy":{"lvl1":"Multivariate Calculus","lvl2":"Homogeneous Functions and Euler’s Theorem"},"content":"A function y = f(x_1, \\ldots, x_n) is homogeneous of degree k if, for any s > 0,f(sx_1, \\ldots, sx_n) = s^k f(x_1, \\ldots, x_n).\n\nTip\n\nTo check homogeneity, scale all arguments by a common factor s and verify whether\nf(sx, sy, sw) = s^k f(x,y,w) for some constant k.\n\nIf different terms scale with different powers of s, the function is not homogeneous.\n\nTry these ^^","type":"content","url":"/multivariate-calculus#homogeneous-functions-and-eulers-theorem","position":37},{"hierarchy":{"lvl1":"Multivariate Calculus","lvl3":"Homogeneous Functions — Practice Problems","lvl2":"Homogeneous Functions and Euler’s Theorem"},"type":"lvl3","url":"/multivariate-calculus#homogeneous-functions-practice-problems","position":38},{"hierarchy":{"lvl1":"Multivariate Calculus","lvl3":"Homogeneous Functions — Practice Problems","lvl2":"Homogeneous Functions and Euler’s Theorem"},"content":"Determine whether each function is homogeneous and, if so, state its degree of homogeneity.\n\nf(x,y,w) = \\dfrac{x}{w} + \\dfrac{3y}{5x}\n\nf(x,y,w) = \\dfrac{x^2}{w} + \\dfrac{2w^2}{y}\n\nf(x,y,w) = \\dfrac{x^3 y}{w} + 2xyw\n\nf(x,y) = \\sqrt{x^2 + y^2}\n\nf(x,y,w) = 3x^2 y - \\dfrac{3y}{w^2}\n\nf(x,y) = x^{1/2} y^{1/4} + y^{5/8}\n\nAnswers\n\n1. Homogeneous of degree 0 2. Homogeneous of degree 1 3. Homogeneous of degree 3 4. Homogeneous of degree 1 5. Not homogeneous 6. Not homogeneous","type":"content","url":"/multivariate-calculus#homogeneous-functions-practice-problems","position":39},{"hierarchy":{"lvl1":"Multivariate Calculus","lvl3":"Euler’s Theorem","lvl2":"Homogeneous Functions and Euler’s Theorem"},"type":"lvl3","url":"/multivariate-calculus#eulers-theorem","position":40},{"hierarchy":{"lvl1":"Multivariate Calculus","lvl3":"Euler’s Theorem","lvl2":"Homogeneous Functions and Euler’s Theorem"},"content":"If y = f(x_1, \\ldots, x_n) is homogeneous of degree k, thenk y\n===\n\nx_1 f_1(x_1, \\ldots, x_n)\n+\n\\cdots\n+\nx_n f_n(x_1, \\ldots, x_n),\n\nwheref_i = \\frac{\\partial f}{\\partial x_i}.","type":"content","url":"/multivariate-calculus#eulers-theorem","position":41},{"hierarchy":{"lvl1":"Multivariate Calculus","lvl3":"Proof of Euler’s Theorem","lvl2":"Homogeneous Functions and Euler’s Theorem"},"type":"lvl3","url":"/multivariate-calculus#proof-of-eulers-theorem","position":42},{"hierarchy":{"lvl1":"Multivariate Calculus","lvl3":"Proof of Euler’s Theorem","lvl2":"Homogeneous Functions and Euler’s Theorem"},"content":"By homogeneity,f(sx_1, \\ldots, sx_n) = s^k y.\n\nTaking the derivative with respect to s of the left-hand side,\\frac{d}{ds} f(sx_1, \\ldots, sx_n)\n=\nx_1 f_1(sx_1, \\ldots, sx_n)\n+\n\\cdots\n+\nx_n f_n(sx_1, \\ldots, sx_n).\n\nDifferentiating the right-hand side,\\frac{d}{ds}(s^k y) = k s^{k-1} y.\n\nSetting s = 1 yields Euler’s Theorem.\n\nNotes\n\nNote 1.\nAny function homogeneous of degree 0 can be written asf!\\left(\n\\frac{x_1}{x_i}, \\frac{x_2}{x_i}, \\ldots, 1, \\ldots, \\frac{x_n}{x_i}\n\\right),\n\nfor any i = 1, \\ldots, n.\n\nNote 2.\nIf f(x_1, \\ldots, x_n) is homogeneous of degree k, then each partial derivativef_i = \\frac{\\partial f}{\\partial x_i}\n\nis homogeneous of degree k-1.\n\nTry these ^^","type":"content","url":"/multivariate-calculus#proof-of-eulers-theorem","position":43},{"hierarchy":{"lvl1":"Multivariate Calculus","lvl3":"Homogeneous Production Function and Euler’s Theorem","lvl2":"Homogeneous Functions and Euler’s Theorem"},"type":"lvl3","url":"/multivariate-calculus#homogeneous-production-function-and-eulers-theorem","position":44},{"hierarchy":{"lvl1":"Multivariate Calculus","lvl3":"Homogeneous Production Function and Euler’s Theorem","lvl2":"Homogeneous Functions and Euler’s Theorem"},"content":"Consider the production functionf(x_1,x_2) = x_1^{1/4} x_2^{1/3}.\n\nQuestion\n\ni. Determine whether this production function is homogeneous. If so, of what degree?\n\nii. Take the partial derivatives of this production function and show that they are homogeneous of degree k-1.\n\niii. Using Euler’s Theorem, show thatx_1 f_1(sx_1,sx_2) + x_2 f_2(sx_1,sx_2)\n=\nk s^{k-1} f(x_1,x_2).\n\nAnswers\n\ni. The function is homogeneous of degreek = \\frac14 + \\frac13 = \\frac{7}{12},\n\nsincef(sx_1,sx_2)\n=\n(sx_1)^{1/4}(sx_2)^{1/3}\n=\ns^{7/12} f(x_1,x_2).\n\nii. The partial derivatives aref_1(x_1,x_2)\n=\n\\frac{\\partial f}{\\partial x_1}\n=\n\\frac14 x_1^{-3/4} x_2^{1/3},\n\nandf_2(x_1,x_2)\n=\n\\frac{\\partial f}{\\partial x_2}\n=\n\\frac13 x_1^{1/4} x_2^{-2/3}.\n\nEach partial derivative is homogeneous of degreek-1 = \\frac{7}{12} - 1 = -\\frac{5}{12},\n\nsincef_1(sx_1,sx_2) = s^{-5/12} f_1(x_1,x_2),\n\\quad\nf_2(sx_1,sx_2) = s^{-5/12} f_2(x_1,x_2).\n\niii. By Euler’s Theorem for homogeneous functions,x_1 f_1(x_1,x_2) + x_2 f_2(x_1,x_2)\n=\nk f(x_1,x_2).\n\nEvaluating this expression at (sx_1,sx_2) and using the homogeneity of the partial derivatives,x_1 f_1(sx_1,sx_2) + x_2 f_2(sx_1,sx_2)\n=\nk s^{k-1} f(x_1,x_2),\n\nwhere k = \\tfrac{7}{12} and f(x_1,x_2) = x_1^{1/4} x_2^{1/3}.","type":"content","url":"/multivariate-calculus#homogeneous-production-function-and-eulers-theorem","position":45},{"hierarchy":{"lvl1":"Multivariate Calculus","lvl2":"Homothetic Functions"},"type":"lvl2","url":"/multivariate-calculus#homothetic-functions","position":46},{"hierarchy":{"lvl1":"Multivariate Calculus","lvl2":"Homothetic Functions"},"content":"A homothetic function is a monotonic transformation of a homogeneous function.\n\nThat is, ify = f(x_1, \\ldots, x_n)\n\nis a homogeneous function, thenz = g(y)\n\nis a homothetic function if g(y) is a strictly monotonic transformation, i.e.,g'(y) > 0 \\quad \\text{for all } y\n\\quad \\text{or} \\quad\ng'(y) < 0 \\quad \\text{for all } y.\n\nNote. While every homogeneous function is a homothetic function (since we can simply choose g(y) = y), not every homothetic function is homogeneous.\n\nExample\n\nFor example, takey = x_1^{\\alpha} x_2^{\\beta},\n\nwhich is homogeneous of degree \\alpha + \\beta.\n\nTaking logarithms, we obtainz = \\ln(y) = \\alpha \\ln(x_1) + \\beta \\ln(x_2).\n\nThe function z is homothetic since the logarithm function is strictly monotonic.\n\nHowever, this homothetic function is not homogeneous in the arguments x_1 and x_2, since\\alpha \\ln(sx_1) + \\beta \\ln(sx_2)\n=\n\\alpha \\ln(x_1) + \\beta \\ln(x_2) + (\\alpha + \\beta)\\ln(s)\n\nand thereforez(sx_1, sx_2)\n=\nz(x_1, x_2) + (\\alpha + \\beta)\\ln(s),\n\nwhich does not satisfy the definition of homogeneity.\n\nHomothetic Functions\n\nHomothetic functions preserve input rankings and marginal rates of substitution, but not proportional scaling.\n\nTry these ^^","type":"content","url":"/multivariate-calculus#homothetic-functions","position":47},{"hierarchy":{"lvl1":"Multivariate Calculus","lvl3":"Homogeneous and Homothetic Functions","lvl2":"Homothetic Functions"},"type":"lvl3","url":"/multivariate-calculus#homogeneous-and-homothetic-functions","position":48},{"hierarchy":{"lvl1":"Multivariate Calculus","lvl3":"Homogeneous and Homothetic Functions","lvl2":"Homothetic Functions"},"content":"Consider the functiony = f(x_1, x_2) = x_1 x_2\n\ndefined over the domain x_1 > 0 and x_2 > 0.\nAlso consider the functionsg(y) = \\ln y,\n\\quad\nh(y) = 10y,\n\\quad\nj(y) = y^2,\n\\quad\nk(y) = e^y.\n\nAnswer the following questions.\n\nIs f(x_1, x_2) a homogeneous function? If so, what is its degree?\n\nIs g(y) a homothetic function?\nIs g(y) a homogeneous function in the arguments x_1 and x_2?\nIf so, what is its degree?\n\nHow about h(y)?\nIs it a homothetic function?\nIs it homogeneous in the arguments x_1 and x_2?\nIf so, what is its degree?\n\nHow about j(y) and k(y)?\nFor each function, state whether it is homothetic and/or homogeneous in (x_1, x_2), and give the degree if applicable.\n\nAnswers\n\nf(x_1, x_2)\n\nYes. Sincef(sx_1, sx_2) = (sx_1)(sx_2) = s^2 f(x_1, x_2),\n\nthe function is homogeneous of degree 2.\n\ng(y)=\\ln y\n\nHomothetic? Yes, because \\ln(\\cdot) is strictly increasing on (0,\\infty), and y=x_1x_2>0 on the given domain.\n\nHomogeneous in (x_1,x_2)? No.Let G(x_1,x_2)=\\ln(f(x_1,x_2))=\\ln(x_1x_2). ThenG(sx_1,sx_2)=\\ln\\big((sx_1)(sx_2)\\big)=\\ln(s^2x_1x_2)=\\ln(x_1x_2)+2\\ln s,\n\nwhich is not of the form s^k G(x_1,x_2) for any constant k.\n\nh(y)=10y\n\nHomothetic? Yes (it is strictly increasing in y).\n\nHomogeneous in (x_1,x_2)? Yes.Let H(x_1,x_2)=h(f(x_1,x_2))=10x_1x_2. ThenH(sx_1,sx_2)=10(sx_1)(sx_2)=s^2\\cdot 10x_1x_2=s^2H(x_1,x_2),\n\nso h(f(x_1,x_2)) is homogeneous of degree 2 in (x_1,x_2).\n\nj(y)=y^2 and k(y)=e^y\n\n(a) j(y)=y^2\n\nHomothetic? Yes on the given domain, because y=x_1x_2>0 and y^2 is strictly increasing for y>0.\n\nHomogeneous in (x_1,x_2)? Yes.Let J(x_1,x_2)=j(f(x_1,x_2))=(x_1x_2)^2. ThenJ(sx_1,sx_2)=\\big((sx_1)(sx_2)\\big)^2=(s^2x_1x_2)^2=s^4(x_1x_2)^2=s^4J(x_1,x_2),\n\nso it is homogeneous of degree 4.\n\n(b) k(y)=e^y\n\nHomothetic? Yes, because e^y is strictly increasing for all y.\n\nHomogeneous in (x_1,x_2)? No.Let K(x_1,x_2)=k(f(x_1,x_2))=e^{x_1x_2}. ThenK(sx_1,sx_2)=e^{(sx_1)(sx_2)}=e^{s^2x_1x_2},\n\nwhich cannot be written as s^k K(x_1,x_2) for a constant k.\n\nSummary\n\nFunction\n\nHomothetic?\n\nHomogeneous in (x_1,x_2)?\n\nDegree\n\nf(x_1,x_2)=x_1x_2\n\n—\n\nYes\n\n2\n\ng(y)=\\ln y\n\nYes\n\nNo\n\n—\n\nh(y)=10y\n\nYes\n\nYes\n\n2\n\nj(y)=y^2\n\nYes\n\nYes\n\n4\n\nk(y)=e^y\n\nYes\n\nNo\n\n—\n\nKey takeaway\n\nEvery homogeneous function is homothetic, because a homogeneous function is already a special case of a monotonic transformation (the identity transformation).\n\nHowever, the converse is not always true:\n\nA homothetic function need not be homogeneous.\n\nMonotonic transformations such as \\ln(\\cdot) or e^{(\\cdot)} generally destroy homogeneity, even though they preserve ordering.\\text{Homogeneous} \\;\\Longrightarrow\\; \\text{Homothetic}, \n\\qquad\n\\text{but not necessarily vice versa}.\n\nThis distinction is crucial in production theory and consumer theory:homothetic functions preserve optimal input proportions, while homogeneous functions additionally impose scale properties.\n\nTry these ^^\n\nShow that each of the following functions is homothetic by transforming it back to its underlying homogeneous form.\n\ni. y = \\ln(x) + \\ln(z)\n\nii. y = 0.3 \\ln(L) + 0.7 \\ln(K)\n\niii. y = 2\\ln(x) + \\ln(y) - \\ln(w)\n\niv. y = e^{xz}","type":"content","url":"/multivariate-calculus#homogeneous-and-homothetic-functions","position":49},{"hierarchy":{"lvl1":"Multivariate Calculus","lvl3":"Answers","lvl2":"Homothetic Functions"},"type":"lvl3","url":"/multivariate-calculus#answers","position":50},{"hierarchy":{"lvl1":"Multivariate Calculus","lvl3":"Answers","lvl2":"Homothetic Functions"},"content":"i.Starting fromy = \\ln(x) + \\ln(z)\n\nExponentiating both sides givese^{y} = xz\n\nThe function xz is homogeneous of degree 2, and since \\ln(\\cdot) is strictly monotonic, the original function is homothetic.\n\nii.Starting fromy = 0.3 \\ln(L) + 0.7 \\ln(K)\n\nExponentiating both sides givese^{y} = L^{0.3} K^{0.7}\n\nThe Cobb–Douglas function L^{0.3} K^{0.7} is homogeneous of degree 1, hence the original function is homothetic.\n\niii.Starting fromy = 2\\ln(x) + \\ln(y) - \\ln(w)\n\nExponentiating both sides givese^{y} = \\dfrac{x^{2} y}{w}\n\nThe function \\dfrac{x^{2} y}{w} is homogeneous of degree 2, so the original function is homothetic.\n\niv.Starting fromy = e^{xz}\n\nTaking logs gives\\ln(y) = xz\n\nThe function xz is homogeneous of degree 2, and since the exponential function is strictly monotonic, y = e^{xz} is homothetic.","type":"content","url":"/multivariate-calculus#answers","position":51},{"hierarchy":{"lvl1":"Multivariate Calculus","lvl2":"Examples in Economics"},"type":"lvl2","url":"/multivariate-calculus#examples-in-economics","position":52},{"hierarchy":{"lvl1":"Multivariate Calculus","lvl2":"Examples in Economics"},"content":"Consider a Cobb–Douglas production functionQ = A K^{1-\\alpha} L^{\\alpha}\n\nThe tangent (slope) to its isoquant is\\frac{dK}{dL}\n=\n\\left( \\frac{\\alpha}{1-\\alpha} \\right)\\frac{K}{L}\n\nAn interesting property of the Cobb–Douglas production function is that the ratio of its marginal products depends on the capital–labor ratio and not on the overall level of production.\n\nHence multiplying K and L by some factor s leaves the slope unchanged:\\frac{dK}{dL}\n=\n\\left( \\frac{\\alpha}{1-\\alpha} \\right)\\frac{K}{L}\n=\n\\left( \\frac{\\alpha}{1-\\alpha} \\right)\\frac{sK}{sL}\n\nMore generally, the slope of the level curves of any homogeneous function is constant along any ray from the origin.\nThe slope of a level curve of a function f(x_1,x_2) is\\frac{dx_2}{dx_1}\n=\n\\frac{f_1(x_1,x_2)}{f_2(x_1,x_2)}\n\nNow recall that for a homogeneous function of degree k,f_i(sx_1,sx_2)\n=\ns^{k-1} f_i(x_1,x_2)\n\nThus along a ray from the origin,\\begin{aligned}\n\\frac{dx_2}{dx_1}\n&=\n\\frac{f_1(sx_1,sx_2)}{f_2(sx_1,sx_2)} [0.5em]\n&=\n\\frac{s^{k-1} f_1(x_1,x_2)}{s^{k-1} f_2(x_1,x_2)} [0.5em]\n&=\n\\frac{f_1(x_1,x_2)}{f_2(x_1,x_2)} .\n\\end{aligned}\n\nPut differently, any proportional scaling s of the two arguments x_1 and x_2 leaves the slope unchanged.\n\nThis property also extends to homothetic functions.\n\nConsider the functiony = f(x_1,x_2),\n\nwhich we assume to be homogeneous of degree k, and the homothetic transformationz = g(y) = g(f(x_1,x_2)),\n\nwhere g'(y) > 0 for all y or g'(y) < 0 for all y.\n\nUsing the chain rule together with the Implicit Function Theorem, we obtain\\begin{aligned}\n\\frac{dx_2}{dx_1}\n&=\n\\frac{g'(y) f_1(sx_1,sx_2)}{g'(y) f_2(sx_1,sx_2)}\n&=\n\\frac{f_1(sx_1,sx_2)}{f_2(sx_1,sx_2)}\n&=\n\\frac{s^{k-1} f_1(x_1,x_2)}{s^{k-1} f_2(x_1,x_2)}\n&=\n\\frac{f_1(x_1,x_2)}{f_2(x_1,x_2)}\n\\end{aligned}\n\nThus, as with homogeneous functions, the scaling factor s does not affect the slope of the level curve.\nThis shows that the slope of the level curves of any homothetic function—a class that includes but is not limited to homogeneous functions—is not altered by proportional scaling of all its arguments.\n\nKey implication\n\nHomogeneous and homothetic functions have constant slopes along rays from the origin, a crucial property in production theory.\n:::{comment}\n## Chapter Summary\n\n::::{admonition} Summary\n:class: important\n\n* Partial derivatives measure marginal effects holding other variables fixed\n* Second and cross partials capture curvature and interaction effects\n* The multivariate chain rule links composite functions\n* Total differentials and derivatives measure combined effects\n* Homogeneity and homotheticity explain scale and substitution properties\n* These tools underpin multivariate and constrained optimization in economics\n::::\n:::","type":"content","url":"/multivariate-calculus#examples-in-economics","position":53},{"hierarchy":{"lvl1":"Optimization"},"type":"lvl1","url":"/optimization","position":0},{"hierarchy":{"lvl1":"Optimization"},"content":"Finding the “best” way to do a specific task in economics often involves what is called an optimization problem.","type":"content","url":"/optimization","position":1},{"hierarchy":{"lvl1":"Optimization","lvl2":"Univariate Optimization"},"type":"lvl2","url":"/optimization#univariate-optimization","position":2},{"hierarchy":{"lvl1":"Optimization","lvl2":"Univariate Optimization"},"content":"","type":"content","url":"/optimization#univariate-optimization","position":3},{"hierarchy":{"lvl1":"Optimization","lvl2":"Stationary Points"},"type":"lvl2","url":"/optimization#stationary-points","position":4},{"hierarchy":{"lvl1":"Optimization","lvl2":"Stationary Points"},"content":"Generally, we say that x^* is a stationary point of a differentiable function f(x) when its slope evaluated at x^* is zero, i.e., whenf'(x^*) = 0.","type":"content","url":"/optimization#stationary-points","position":5},{"hierarchy":{"lvl1":"Optimization","lvl2":"Necessary First-Order Condition (F.O.C)"},"type":"lvl2","url":"/optimization#necessary-first-order-condition-f-o-c","position":6},{"hierarchy":{"lvl1":"Optimization","lvl2":"Necessary First-Order Condition (F.O.C)"},"content":"More formally, suppose a function f(x) is differentiable in some interval I and that x^* is an interior point of I.\nThen for x = x^* to be a maximum or minimum point for f(x) in I, a necessary condition is that it is a stationary point for f(x), i.e., x = x^* satisfiesf'(x^*) = 0.\n\nExample 1\n\nLeth(x) = -x^2 + 8x - 15.\n\nF.O.C:-2x + 8 = 0.\n\nSo the stationary point is x^* = 4 and y^* = 1.\n\nExample 2\n\nLetz(x) = x^2 - 8x + 17.\n\nF.O.C:2x - 8 = 0.\n\nSo the stationary point is x^* = 4 and y^* = 1.\n\nWe have the same stationary point, but clearly these are different functions.\nThe former is \\cap-shaped and the latter is \\cup-shaped.","type":"content","url":"/optimization#necessary-first-order-condition-f-o-c","position":7},{"hierarchy":{"lvl1":"Optimization","lvl2":"Sufficient Second-Order Condition (S.O.C) for Maximum/Minimum"},"type":"lvl2","url":"/optimization#sufficient-second-order-condition-s-o-c-for-maximum-minimum","position":8},{"hierarchy":{"lvl1":"Optimization","lvl2":"Sufficient Second-Order Condition (S.O.C) for Maximum/Minimum"},"content":"Following the two examples above, we can characterize a stationary point as a maximum or minimum by taking the second derivative.\n\nIf f''(x^*) < 0, the stationary point represents a maximum.\n\nIf f''(x^*) > 0, the stationary point represents a minimum.\n\nExample 1h''(x) = -2.\n\nSince this is negative, the stationary point (4,1) is a maximum.\n\nExample 2z''(x) = 2.\n\nSince this is positive, the stationary point (4,1) is a minimum.\n\nExample 3\n\nLetp(x) = \\frac{1}{3}x^3 - \\frac{1}{2}x^2 - 2x + 20.\n\nThe F.O.C givesp'(x) = x^2 - x - 2 = 0,\n\nwhich implies x^* = -1 and x^* = 2.\n\nThe S.O.C givesp''(x) = 2x - 1.\n\nAt x^* = -1, p''(-1) = -3 < 0 → maximum\n\nAt x^* = 2, p''(2) = 3 > 0 → minimum","type":"content","url":"/optimization#sufficient-second-order-condition-s-o-c-for-maximum-minimum","position":9},{"hierarchy":{"lvl1":"Optimization","lvl3":"S.O.C Is Sufficient but Not Necessary","lvl2":"Sufficient Second-Order Condition (S.O.C) for Maximum/Minimum"},"type":"lvl3","url":"/optimization#s-o-c-is-sufficient-but-not-necessary","position":10},{"hierarchy":{"lvl1":"Optimization","lvl3":"S.O.C Is Sufficient but Not Necessary","lvl2":"Sufficient Second-Order Condition (S.O.C) for Maximum/Minimum"},"content":"Considery = x^4.\n\nThe F.O.C gives x^* = 0.\n\nThe S.O.C also gives 0, which is neither positive nor negative.\nYet, x^* = 0 is clearly a minimum.","type":"content","url":"/optimization#s-o-c-is-sufficient-but-not-necessary","position":11},{"hierarchy":{"lvl1":"Optimization","lvl2":"Global and Local Minimum/Maximum"},"type":"lvl2","url":"/optimization#global-and-local-minimum-maximum","position":12},{"hierarchy":{"lvl1":"Optimization","lvl2":"Global and Local Minimum/Maximum"},"content":"We must distinguish between global and local extrema.","type":"content","url":"/optimization#global-and-local-minimum-maximum","position":13},{"hierarchy":{"lvl1":"Optimization","lvl3":"Global Maximum","lvl2":"Global and Local Minimum/Maximum"},"type":"lvl3","url":"/optimization#global-maximum","position":14},{"hierarchy":{"lvl1":"Optimization","lvl3":"Global Maximum","lvl2":"Global and Local Minimum/Maximum"},"content":"If f(x) is everywhere differentiable and has stationary point x^*, then x^* is a global maximum iff'(x) \\ge 0 \\text{ for all } x \\le x^*,\n\\quad \\text{and} \\quad\nf'(x) \\le 0 \\text{ for all } x \\ge x^*.\n\nThat is, the function increases up to x^* and decreases afterward.","type":"content","url":"/optimization#global-maximum","position":15},{"hierarchy":{"lvl1":"Optimization","lvl3":"Global Minimum","lvl2":"Global and Local Minimum/Maximum"},"type":"lvl3","url":"/optimization#global-minimum","position":16},{"hierarchy":{"lvl1":"Optimization","lvl3":"Global Minimum","lvl2":"Global and Local Minimum/Maximum"},"content":"Similarly, x^* is a global minimum iff'(x) \\le 0 \\text{ for all } x \\le x^*,\n\\quad \\text{and} \\quad\nf'(x) \\ge 0 \\text{ for all } x \\ge x^*.","type":"content","url":"/optimization#global-minimum","position":17},{"hierarchy":{"lvl1":"Optimization","lvl3":"Local Maximum / Minimum","lvl2":"Global and Local Minimum/Maximum"},"type":"lvl3","url":"/optimization#local-maximum-minimum","position":18},{"hierarchy":{"lvl1":"Optimization","lvl3":"Local Maximum / Minimum","lvl2":"Global and Local Minimum/Maximum"},"content":"We speak of a local maximum or minimum if x^* is a stationary maximum or minimum only in a neighborhood of x^*, not over the entire domain.","type":"content","url":"/optimization#local-maximum-minimum","position":19},{"hierarchy":{"lvl1":"Optimization","lvl2":"Concavity and Convexity"},"type":"lvl2","url":"/optimization#concavity-and-convexity","position":20},{"hierarchy":{"lvl1":"Optimization","lvl2":"Concavity and Convexity"},"content":"If f(x) is strictly concave on (m,n) and has a stationary point x^* with m < x^* < n, then x^* is a local maximum.\n\nIf f(x) is strictly concave everywhere, it has at most one stationary point, which is a global maximum.\n\nConversely,\n\nIf f(x) is strictly convex on (m,n) and has a stationary point x^*, then x^* is a local minimum.\n\nIf f(x) is strictly convex everywhere, that stationary point is a global minimum.","type":"content","url":"/optimization#concavity-and-convexity","position":21},{"hierarchy":{"lvl1":"Optimization","lvl2":"Inflection Points"},"type":"lvl2","url":"/optimization#inflection-points","position":22},{"hierarchy":{"lvl1":"Optimization","lvl2":"Inflection Points"},"content":"Consider the functionk(x) = 1 + (x - 4)^3.\n\nThe F.O.C gives a stationary point (4,1).\n\nThe S.O.C evaluated at this point is 0, so it is inconclusive.\n\nWe must then take higher-order derivatives.\n\nIf the first nonzero higher-order derivative evaluated at the stationary point is:\n\nOdd-order → inflection point\n\nEven-order → maximum or minimum (depending on sign)\n\nIf the first non-zero derivative at the stationary point c is of even order (n = 2, 4, 6...):\n\nDerivative Sign\n\nResult\n\nVisual Intuition\n\nf^{(n)}(c) > 0\n\nLocal Minimum\n\nThe function “curves up” away from the point in all directions.\n\nf^{(n)}(c) < 0\n\nLocal Maximum\n\nThe function “curves down” away from the point in all directions.\n\nFor this example,k'''(4) \\ne 0,\n\nwhich is the third (odd) derivative.\nHence, (4,1) is an inflection point.\n\nTry these ^^\n\nStationary Points and Higher-Order Derivative Test\n\nFind the stationary points of the following functions and determine whether you have a minimum, maximum, or inflection point by determining whether at the stationary point the function is convex or concave (or neither).\n\n(i) f(x) = -x^3 + 6x^2 + 15x - 32\n\n(ii) f(x) = (2x - 7)^3\n\n(iii) f(x) = (x + 2)^4\n\n(iv) f(x) = -2(x - 6)^6\n\n(v) f(x) = x^4\n\nAnswers\n\n(i) f'(x) = -3x^2 + 12x + 15\n\nSetting f'(x)=0:x = -1 \\quad \\text{and} \\quad x = 5\n\nSecond derivative:f''(x) = -6x + 12f''(-1) = 18 > 0 \\Rightarrow \\text{minimum}f''(5) = -18 < 0 \\Rightarrow \\text{maximum}\n\nInflection point:f''(x)=0 \\Rightarrow x=2\n\n(ii) f'(x) = 6(2x - 7)^2\\text{Hence, }x = 3.5f''(x) = 24(2x - 7)f''(3.5) = 0f'''(x) = 48\n\nSince the first nonzero derivative at the critical point is of odd order, we have an inflection point at x=3.5.\n\n(iii) f'(x) = 4(x+2)^3x = -2f''(-2)=0, \\quad f'''(-2)=0, \\quad f^{(4)}(-2)=24>0\n\nThe first nonzero derivative is even and positive, hence we have a minimum.\n\n(iv) f'(x) = -12(x-6)^5x=6\n\nAll derivatives up to order 5 vanish.f^{(6)}(6) = -1440 < 0\n\nThe first nonzero derivative is even and negative, hence we have a maximum.\n\n(v) f'(x) = 4x^3x=0\n\nAll derivatives up to order 3 vanish.f^{(0)}(4) = 24 > 0\n\nThe first nonzero derivative is of even order and it’s value is positive, hence we have an local minimum.","type":"content","url":"/optimization#inflection-points","position":23},{"hierarchy":{"lvl1":"Optimization","lvl2":"Economic Applications"},"type":"lvl2","url":"/optimization#economic-applications","position":24},{"hierarchy":{"lvl1":"Optimization","lvl2":"Economic Applications"},"content":"A Monopolist’s Optimal Pricing Scheme\n\nStrategic Behavior of Duopolists\n\nRules versus Discretion in Monetary Policy\n\nThe Inflation Tax and Seigniorage\n\nThe Golden Rule","type":"content","url":"/optimization#economic-applications","position":25},{"hierarchy":{"lvl1":"Optimization","lvl2":"Multivariate Optimization"},"type":"lvl2","url":"/optimization#multivariate-optimization","position":26},{"hierarchy":{"lvl1":"Optimization","lvl2":"Multivariate Optimization"},"content":"We now generalize the univariate techniques to multivariate optimization.","type":"content","url":"/optimization#multivariate-optimization","position":27},{"hierarchy":{"lvl1":"Optimization","lvl2":"Multivariate First-Order Condition"},"type":"lvl2","url":"/optimization#multivariate-first-order-condition","position":28},{"hierarchy":{"lvl1":"Optimization","lvl2":"Multivariate First-Order Condition"},"content":"If we have a functiony = f(x_1, x_2, \\ldots, x_n)\n\nthat is differentiable with respect to each of its arguments and has a stationary point at\n(x_1^*, x_2^*, \\ldots, x_n^*), then each of the partial derivatives at that point equals zero.\n\nThat is,f_1(x_1^*, x_2^*, \\ldots, x_n^*) = 0 \\\\\nf_2(x_1^*, x_2^*, \\ldots, x_n^*) = 0 \\\\\n\\vdots \\\\\nf_n(x_1^*, x_2^*, \\ldots, x_n^*) = 0\n\nExample 1\n\nConsider the bivariate functiong(x_1, x_2) = 6x_1 - x_1^2 + 16x_2 - 4x_2^2\n\nThe first-order conditions areg_1(x_1, x_2) = 6 - 2x_1 = 0 \\\\\ng_2(x_1, x_2) = 16 - 8x_2 = 0\n\nThe single stationary point is thereforex_1^* = 3, \\quad x_2^* = 2\n\nand the value of the function at this point isg(3,2) = 25.\n\nWe will show later using the second-order condition that this stationary point represents a maximum.\n\nLet’s visualize the equation and its stationary point.\n\nIf we take a slice of the function g(x_1, x_2) at x_2 = 2, the stationary point is achieved at x_1 = 3.\nSimilarly, taking a slice at x_1 = 3 shows a stationary point at x_2 = 2. Visually, we have\n\nExample 2\n\nConsider the functionh(x_1, x_2) = x_1^2 + 4x_2^2 - 2x_1 - 16x_2 + x_1 x_2\n\nThe first-order conditions giveh_1(x_1, x_2) = 2x_1 - 2 + x_2 = 0 \\\\\nh_2(x_1, x_2) = 8x_2 - 16 + x_1 = 0\n\nHence the single stationary point isx_1^* = 0, \\quad x_2^* = 2\n\nand the value of the function at this point ish(0,2) = -16.\n\nBelow is a visualization of this function with the plane tangent and stationary point.","type":"content","url":"/optimization#multivariate-first-order-condition","position":29},{"hierarchy":{"lvl1":"Optimization","lvl2":"Second-Order Condition in the Bivariate Case"},"type":"lvl2","url":"/optimization#second-order-condition-in-the-bivariate-case","position":30},{"hierarchy":{"lvl1":"Optimization","lvl2":"Second-Order Condition in the Bivariate Case"},"content":"For the univariate case, the second differential of a function can be considered as the differential of the first differential and denoted asd(dy) = d^2 y.\n\nFor y = f(x), the second differential isd^2 y = f''(x)(dx)^2,\n\nwhich is nonnegative for any dx.","type":"content","url":"/optimization#second-order-condition-in-the-bivariate-case","position":31},{"hierarchy":{"lvl1":"Optimization","lvl3":"Second Differential in the Bivariate Case","lvl2":"Second-Order Condition in the Bivariate Case"},"type":"lvl3","url":"/optimization#second-differential-in-the-bivariate-case","position":32},{"hierarchy":{"lvl1":"Optimization","lvl3":"Second Differential in the Bivariate Case","lvl2":"Second-Order Condition in the Bivariate Case"},"content":"For a bivariate function y = f(x_1, x_2), the total differential isdy = f_1(x_1, x_2),dx_1 + f_2(x_1, x_2),dx_2.\n\nTaking the total derivative of this expression yields the second total differential:d^2 y\n= f_{11}(dx_1)^2 + f_{22}(dx_2)^2 + 2f_{12}dx_1 dx_2.","type":"content","url":"/optimization#second-differential-in-the-bivariate-case","position":33},{"hierarchy":{"lvl1":"Optimization","lvl2":"Sufficient Conditions for Local Maxima and Minima"},"type":"lvl2","url":"/optimization#sufficient-conditions-for-local-maxima-and-minima","position":34},{"hierarchy":{"lvl1":"Optimization","lvl2":"Sufficient Conditions for Local Maxima and Minima"},"content":"If d^2 y < 0 for all (dx_1, dx_2), the stationary point is a local maximum.\n\nIf d^2 y > 0 for all (dx_1, dx_2), the stationary point is a local minimum.\n\nA necessary condition for a minimum isf_{11} > 0 \\quad \\text{and} \\quad f_{22} > 0,\n\nand for a maximum,f_{11} < 0 \\quad \\text{and} \\quad f_{22} < 0.\n\nHowever, the cross-partial derivative f_{12} must also be considered.","type":"content","url":"/optimization#sufficient-conditions-for-local-maxima-and-minima","position":35},{"hierarchy":{"lvl1":"Optimization","lvl3":"Completing the Square","lvl2":"Sufficient Conditions for Local Maxima and Minima"},"type":"lvl3","url":"/optimization#completing-the-square","position":36},{"hierarchy":{"lvl1":"Optimization","lvl3":"Completing the Square","lvl2":"Sufficient Conditions for Local Maxima and Minima"},"content":"By completing the square, the second differential can be rewritten, leading to the condition:f_{11} f_{22} > (f_{12})^2.","type":"content","url":"/optimization#completing-the-square","position":37},{"hierarchy":{"lvl1":"Optimization","lvl3":"Second-Order Condition for a Maximum","lvl2":"Sufficient Conditions for Local Maxima and Minima"},"type":"lvl3","url":"/optimization#second-order-condition-for-a-maximum","position":38},{"hierarchy":{"lvl1":"Optimization","lvl3":"Second-Order Condition for a Maximum","lvl2":"Sufficient Conditions for Local Maxima and Minima"},"content":"If y = f(x_1, x_2) has a stationary point (x_1^*, x_2^*) andf_{11}(x_1^*, x_2^*) < 0\n\\quad \\text{and} \\quad\nf_{11} f_{22} > (f_{12})^2,\n\nthen the function reaches a maximum at that point.","type":"content","url":"/optimization#second-order-condition-for-a-maximum","position":39},{"hierarchy":{"lvl1":"Optimization","lvl3":"Second-Order Condition for a Minimum","lvl2":"Sufficient Conditions for Local Maxima and Minima"},"type":"lvl3","url":"/optimization#second-order-condition-for-a-minimum","position":40},{"hierarchy":{"lvl1":"Optimization","lvl3":"Second-Order Condition for a Minimum","lvl2":"Sufficient Conditions for Local Maxima and Minima"},"content":"Iff_{11}(x_1^*, x_2^*) > 0\n\\quad \\text{and} \\quad\nf_{11} f_{22} > (f_{12})^2,\n\nthen the function reaches a minimum.\n\nSpecial Cases\n\nIf f_{11} f_{22} < (f_{12})^2, the stationary point is a saddle point.\n\nIf f_{11} f_{22} = (f_{12})^2, the test is inconclusive.\n\nLet’s continue with the example above.\n\nThe second partial derivatives ofh(x_1,x_2) = x_1^2 + 4x_2^2 - 2x_1 - 16x_2 + x_1x_2\n\nareh_{11}(x_1,x_2) = 2 \\quad \\text{and} \\quad h_{22}(x_1,x_2) = 8\n\nBoth are positive. The cross-partial derivative ish_{12}(x_1,x_2) = 1.\n\nSinceh_{11} h_{22} > (h_{12})^2,\n\nthat is,16 > 1,\n\nthe stationary point (0,2) is a minimum.\n\nAs another example, considerg(x_1,x_2) = 6x_1 - x_1^2 + 16x_2 - 4x_2^2.\n\nThe second partial derivatives areg_{11}(x_1,x_2) = -2 \\quad \\text{and} \\quad g_{22}(x_1,x_2) = -8,\n\nand the cross-partial derivative isg_{12}(x_1,x_2) = 0.\n\nSince the second partial derivatives are both negative andg_{11} g_{22} > (g_{12})^2,\n\nthat is,16 > 0,\n\nwe have the conditions for a maximum.","type":"content","url":"/optimization#second-order-condition-for-a-minimum","position":41},{"hierarchy":{"lvl1":"Optimization","lvl2":"Second-Order Condition in the General Multivariate Case"},"type":"lvl2","url":"/optimization#second-order-condition-in-the-general-multivariate-case","position":42},{"hierarchy":{"lvl1":"Optimization","lvl2":"Second-Order Condition in the General Multivariate Case"},"content":"Let us use the tools of matrix algebra to develop a set of conditions that enables us to find the sign of the second total differential of a multivariate function.\n\nFirst, assume a bivariate case for which the second total differential is given byd^2 y\n=\nf_{11}(dx_1)^2\n+\nf_{22}(dx_2)^2\n+\n2 f_{12}(dx_1)(dx_2).\n\nThis expression can be written in matrix form as the quadratic form of the two variables dx_1 and dx_2 as follows:d^2 y\n=\n\\begin{bmatrix}\ndx_1 & dx_2\n\\end{bmatrix}\n\\begin{bmatrix}\nf_{11} & f_{12} \\\\\nf_{21} & f_{22}\n\\end{bmatrix}\n\\begin{bmatrix}\ndx_1 \\\\\ndx_2\n\\end{bmatrix}.\n\nIn other words, the second total differential (or second total derivative) for a multivariate function can be written more generally asd^2 y\n=\ndx' H dx\n=\n\\begin{bmatrix}\ndx_1 & dx_2 & \\cdots & dx_n\n\\end{bmatrix}\n\\begin{bmatrix}\nf_{11} & f_{12} & \\cdots & f_{1n} \\\\\nf_{21} & f_{22} & \\cdots & f_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nf_{n1} & f_{n2} & \\cdots & f_{nn}\n\\end{bmatrix}\n\\begin{bmatrix}\ndx_1 \\\\\ndx_2 \\\\\n\\vdots \\\\\ndx_n\n\\end{bmatrix}.\n\nHere, H is the Hessian matrix, and dx is the column vector of differentials.\n\nAll that remains is to determine the sign definiteness of the quadratic form by determining the sign definiteness of the Hessian.","type":"content","url":"/optimization#second-order-condition-in-the-general-multivariate-case","position":43},{"hierarchy":{"lvl1":"Optimization","lvl3":"Interpreting the Second-Order Condition","lvl2":"Second-Order Condition in the General Multivariate Case"},"type":"lvl3","url":"/optimization#interpreting-the-second-order-condition","position":44},{"hierarchy":{"lvl1":"Optimization","lvl3":"Interpreting the Second-Order Condition","lvl2":"Second-Order Condition in the General Multivariate Case"},"content":"The sign of the second total differential d^2 y determines the local curvature of the function and therefore whether a critical point is a local maximum, minimum, or neither.\n\nBecaused^2 y = dx' H dx,\n\nthe problem reduces to determining the sign definiteness of the Hessian matrix H.","type":"content","url":"/optimization#interpreting-the-second-order-condition","position":45},{"hierarchy":{"lvl1":"Optimization","lvl3":"Positive and Negative Definiteness","lvl2":"Second-Order Condition in the General Multivariate Case"},"type":"lvl3","url":"/optimization#positive-and-negative-definiteness","position":46},{"hierarchy":{"lvl1":"Optimization","lvl3":"Positive and Negative Definiteness","lvl2":"Second-Order Condition in the General Multivariate Case"},"content":"Definition\n\nLet H be a symmetric matrix.\n\nH is positive definite if\ndx' H dx > 0 for all nonzero dx.\n\nH is negative definite if\ndx' H dx < 0 for all nonzero dx.\n\nH is indefinite if the quadratic form takes both positive and negative values.\n\nThese cases correspond to the curvature of the function at a critical point.","type":"content","url":"/optimization#positive-and-negative-definiteness","position":47},{"hierarchy":{"lvl1":"Optimization","lvl3":"Second-Order Conditions for Optimization","lvl2":"Second-Order Condition in the General Multivariate Case"},"type":"lvl3","url":"/optimization#second-order-conditions-for-optimization","position":48},{"hierarchy":{"lvl1":"Optimization","lvl3":"Second-Order Conditions for Optimization","lvl2":"Second-Order Condition in the General Multivariate Case"},"content":"Suppose y = f(x_1, \\ldots, x_n) and \\nabla f = 0 at a point x^*.\n\nSecond-Order Test\n\nIf H(x^*) is negative definite, then x^* is a local maximum.\n\nIf H(x^*) is positive definite, then x^* is a local minimum.\n\nIf H(x^*) is indefinite, then x^* is a saddle point.\n\nKey Distinction\n\n• In one variable, if the first nonzero derivative at a stationary point is of odd order, the point is an inflection point.\n\n• In multiple variables, if the Hessian is indefinite (determinant < 0), the stationary point is a saddle point.\n\nAn inflection point concerns curvature along a single line. A saddle point concerns curvature across different directions.","type":"content","url":"/optimization#second-order-conditions-for-optimization","position":49},{"hierarchy":{"lvl1":"Optimization","lvl2":"Sylvester’s Criterion (Practical Test)"},"type":"lvl2","url":"/optimization#sylvesters-criterion-practical-test","position":50},{"hierarchy":{"lvl1":"Optimization","lvl2":"Sylvester’s Criterion (Practical Test)"},"content":"In practice, definiteness is checked using principal minors of the Hessian.","type":"content","url":"/optimization#sylvesters-criterion-practical-test","position":51},{"hierarchy":{"lvl1":"Optimization","lvl3":"Bivariate Case (n = 2)","lvl2":"Sylvester’s Criterion (Practical Test)"},"type":"lvl3","url":"/optimization#bivariate-case-n-2","position":52},{"hierarchy":{"lvl1":"Optimization","lvl3":"Bivariate Case (n = 2)","lvl2":"Sylvester’s Criterion (Practical Test)"},"content":"LetH =\n\\begin{bmatrix}\nf_{11} & f_{12} \\\\\nf_{21} & f_{22}\n\\end{bmatrix}.\n\nThen:\n\nSylvester’s Criterion (2 variables)\n\nH is positive definite if\nf_{11} > 0 and \\det(H) > 0.\n\nH is negative definite if\nf_{11} < 0 and \\det(H) > 0.\n\nH is indefinite if\n\\det(H) < 0.\n\nThis criterion is widely used in economics because it avoids computing the quadratic form directly.\n\nExample\n\nConsider the functiony = -x_1^2 - 2x_2^2 + 4x_1 x_2.\n\nThe Hessian matrix isH =\n\\begin{bmatrix}\n-2 & 4 \\\\\n4 & -4\n\\end{bmatrix}.\n\nCompute the determinant:\\det(H) = (-2)(-4) - 16 = -8 < 0.\n\nSince the determinant is negative, the Hessian is indefinite, and the critical point is a saddle point.","type":"content","url":"/optimization#bivariate-case-n-2","position":53},{"hierarchy":{"lvl1":"Optimization","lvl2":"Economic Interpretation"},"type":"lvl2","url":"/optimization#economic-interpretation","position":54},{"hierarchy":{"lvl1":"Optimization","lvl2":"Economic Interpretation"},"content":"Concavity (negative definite Hessian) corresponds to diminishing marginal returns and guarantees interior maxima in optimization problems.\n\nConvexity (positive definite Hessian) corresponds to cost minimization problems.\n\nIndefiniteness indicates instability or saddle behavior, common in strategic or general equilibrium settings.\n\nKey takeaway\n\nThe second-order condition in multivariate optimization reduces to checking the sign definiteness of the Hessian matrix.\nMatrix algebra provides a compact and powerful way to characterize curvature, stability, and optimality in economic models.\n\nTry these ^^","type":"content","url":"/optimization#economic-interpretation","position":55},{"hierarchy":{"lvl1":"Optimization","lvl3":"Optimizing Multivariate Functions","lvl2":"Economic Interpretation"},"type":"lvl3","url":"/optimization#optimizing-multivariate-functions","position":56},{"hierarchy":{"lvl1":"Optimization","lvl3":"Optimizing Multivariate Functions","lvl2":"Economic Interpretation"},"content":"\n\nThe Hessian Test Procedure\n\nTo classify a stationary point (x, y) for a function f(x, y):\n\nFind Stationary Points: Set first partial derivatives to zero: f_x = 0 and f_y = 0.\n\nCalculate the Hessian Matrix (H):\nH = \\begin{bmatrix} f_{xx} & f_{xy} \\\\ f_{yx} & f_{yy} \\end{bmatrix}\n\nEvaluate the Determinant (D): D = f_{xx} f_{yy} - (f_{xy})^2\n\nClassification Rules:\n\nIf D > 0 and f_{xx} > 0: Local Minimum\n\nIf D > 0 and f_{xx} < 0: Local Maximum\n\nIf D < 0: Saddle Point\n\nIf D = 0: Inconclusive\n\nWorked Solutions","type":"content","url":"/optimization#optimizing-multivariate-functions","position":57},{"hierarchy":{"lvl1":"Optimization","lvl4":"i. f(x,y) = 3x^2 - xy + 2y^2 - 4x - 7y + 12","lvl3":"Optimizing Multivariate Functions","lvl2":"Economic Interpretation"},"type":"lvl4","url":"/optimization#i-f-x-y-3x-2-xy-2y-2-4x-7y-12","position":58},{"hierarchy":{"lvl1":"Optimization","lvl4":"i. f(x,y) = 3x^2 - xy + 2y^2 - 4x - 7y + 12","lvl3":"Optimizing Multivariate Functions","lvl2":"Economic Interpretation"},"content":"a. Find Stationary Points\n\nf_x = 6x - y - 4 = 0\n\nf_y = -x + 4y - 7 = 0\n\nSolving the system:\nFrom f_y, x = 4y - 7. Substitute into f_x:\n6(4y - 7) - y - 4 = 0\n24y - 42 - y - 4 = 0 \\implies 23y = 46 \\implies \\mathbf{y = 2}\nSubstituting back: x = 4(2) - 7 \\implies \\mathbf{x = 1}\n\nNote that you can actually use x=A^{-1}b, which is recommended for more complex system of equations.\n\nb. Hessian Classification\n\nf_{xx} = 6\n\nf_{yy} = 4\n\nf_{xy} = -1\n\nD = (6)(4) - (-1)^2 = 23\n\nConclusion: Since D > 0 and f_{xx} > 0, the point (1, 2) is a minimum.","type":"content","url":"/optimization#i-f-x-y-3x-2-xy-2y-2-4x-7y-12","position":59},{"hierarchy":{"lvl1":"Optimization","lvl4":"ii. f(x, y) = 60x + 34y - 4xy - 6x^2 - 3y^2 + 5","lvl3":"Optimizing Multivariate Functions","lvl2":"Economic Interpretation"},"type":"lvl4","url":"/optimization#ii-f-x-y-60x-34y-4xy-6x-2-3y-2-5","position":60},{"hierarchy":{"lvl1":"Optimization","lvl4":"ii. f(x, y) = 60x + 34y - 4xy - 6x^2 - 3y^2 + 5","lvl3":"Optimizing Multivariate Functions","lvl2":"Economic Interpretation"},"content":"a. Find Stationary Points\n\nf_x = 60 - 4y - 12x = 0 \\implies 3x + y = 15\n\nf_y = 34 - 4x - 6y = 0 \\implies 2x + 3y = 17\n\nSolving the system:\nMultiply first equation by 3: 9x + 3y = 45.\nSubtract second equation: (9x - 2x) = 45 - 17 \\implies 7x = 28 \\implies \\mathbf{x = 4}.\nThen 3(4) + y = 15 \\implies \\mathbf{y = 3}.\n\nb. Hessian Classification\n\nf_{xx} = -12\n\nf_{yy} = -6\n\nf_{xy} = -4\n\nD = (-12)(-6) - (-4)^2 = 72 - 16 = 56\n\nConclusion: Since D > 0 and f_{xx} < 0, the point (4, 3) is a maximum.","type":"content","url":"/optimization#ii-f-x-y-60x-34y-4xy-6x-2-3y-2-5","position":61},{"hierarchy":{"lvl1":"Optimization","lvl4":"iii. f(x,y) = 48y - 3x^2 - 6xy - 2y^2 + 72x","lvl3":"Optimizing Multivariate Functions","lvl2":"Economic Interpretation"},"type":"lvl4","url":"/optimization#iii-f-x-y-48y-3x-2-6xy-2y-2-72x","position":62},{"hierarchy":{"lvl1":"Optimization","lvl4":"iii. f(x,y) = 48y - 3x^2 - 6xy - 2y^2 + 72x","lvl3":"Optimizing Multivariate Functions","lvl2":"Economic Interpretation"},"content":"a. Find Stationary Points\n\nf_x = -6x - 6y + 72 = 0 \\implies x + y = 12\n\nf_y = 48 - 6x - 4y = 0 \\implies 3x + 2y = 24\n\nSolving the system:\nFrom first eq, x = 12 - y. Substitute into second:\n3(12 - y) + 2y = 24 \\implies 36 - 3y + 2y = 24 \\implies -y = -12 \\implies \\mathbf{y = 12}.\nThen \\mathbf{x = 0}.\n\nb. Hessian Classification\n\nf_{xx} = -6\n\nf_{yy} = -4\n\nf_{xy} = -6\n\nD = (-6)(-4) - (-6)^2 = 24 - 36 = -12\n\nConclusion: Since D < 0, the point (0, 12) is a saddle point.","type":"content","url":"/optimization#iii-f-x-y-48y-3x-2-6xy-2y-2-72x","position":63},{"hierarchy":{"lvl1":"Optimization","lvl4":"iv. f(x, y) = 5x^2 - 3y^2 - 30x + 7y + 4xy","lvl3":"Optimizing Multivariate Functions","lvl2":"Economic Interpretation"},"type":"lvl4","url":"/optimization#iv-f-x-y-5x-2-3y-2-30x-7y-4xy","position":64},{"hierarchy":{"lvl1":"Optimization","lvl4":"iv. f(x, y) = 5x^2 - 3y^2 - 30x + 7y + 4xy","lvl3":"Optimizing Multivariate Functions","lvl2":"Economic Interpretation"},"content":"a. Find Stationary Points\n\nf_x = 10x - 30 + 4y = 0\n\nf_y = -6y + 7 + 4x = 0\n\nHence, x = 2, y = 2.5\n\nb. Hessian Classification\n\nf_{xx} = 10\n\nf_{yy} = -6\n\nf_{xy} = 4\n\nD = (10)(-6) - (4)^2 = -60 - 16 = -76\n\nConclusion: Since D < 0, this function results in a saddle point.","type":"content","url":"/optimization#iv-f-x-y-5x-2-3y-2-30x-7y-4xy","position":65},{"hierarchy":{"lvl1":"Optimization","lvl4":"v. f(x,y) = x^3 - 3x + y^2 - 4y","lvl3":"Optimizing Multivariate Functions","lvl2":"Economic Interpretation"},"type":"lvl4","url":"/optimization#v-f-x-y-x-3-3x-y-2-4y","position":66},{"hierarchy":{"lvl1":"Optimization","lvl4":"v. f(x,y) = x^3 - 3x + y^2 - 4y","lvl3":"Optimizing Multivariate Functions","lvl2":"Economic Interpretation"},"content":"a. Find Stationary Points\n\nCompute the partial derivatives.\n\nf_{x} = 3x^2 - 3\n\nf_{y} = 2y - 4\n\nSet both equal to zero.\n\nFrom f_{x}=0:3x^2 - 3 = 0\n\\quad\\Rightarrow\\quad\nx^2 = 1\n\\quad\\Rightarrow\\quad\nx = \\pm 1.\n\nFrom f_{x_2}=0:2y - 4 = 0\n\\quad\\Rightarrow\\quad\ny = 2.\n\nWe therefore have two critical points:(1,2)\n\\quad\\text{and}\\quad\n(-1,2).\n\nBoth are integers — nice and clean.\n\nb. Hessian Classification\n\nCompute the second partial derivatives.\n\nf_{xx} = 6x\n\nf_{yy} = 2\n\nf_{xy} = f_{yx} = 0\n\nThe Hessian matrix isH =\n\\begin{bmatrix}\n6x & 0 \\\\\n0 & 2\n\\end{bmatrix}.\n\nNote that the determinant is|H| = (6x)(2) - 0 = 12x.\n\nClassification\n\nAt (1,2)|H_1| = 6 > 0 //\n|H_2| = |H| = 12(1) = 12 > 0\n\nSo the Hessian is positive definite.\n\n\n\\Rightarrow\n\\textbf{Local minimum at } (1,2).\n\nAt (-1,2)|H_2| = 12(-1) = -12 < 0.\n\nSince the determinant is negative, the Hessian is sign indefinite.\n\n\n\\Rightarrow\n\\textbf{Saddle point at } (-1,2).\n\nFinal Answer\n\n(1,2) → local minimum\n\n(-1,2) → saddle point","type":"content","url":"/optimization#v-f-x-y-x-3-3x-y-2-4y","position":67}]}